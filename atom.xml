<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>KK&#39;s Note</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jkknotes.com/"/>
  <updated>2020-03-28T11:12:39.577Z</updated>
  <id>https://jkknotes.com/</id>
  
  <author>
    <name>KK.J</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>特征选择方法详解Part3-SelectFromModel- RFE、L1、Tree、Permutation importance</title>
    <link href="https://jkknotes.com/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3Part3-SelectFromModel-%20RFE%E3%80%81L1%E3%80%81Tree%E3%80%81Permutation%20importance/"/>
    <id>https://jkknotes.com/特征选择方法详解Part3-SelectFromModel- RFE、L1、Tree、Permutation importance/</id>
    <published>2020-03-28T11:12:00.000Z</published>
    <updated>2020-03-28T11:12:39.577Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>到目前为止，已经在《特征选择方法详解Part1-方差分析、Pearson、Spearman》和《特征选择方法详解Part2-卡方检验、互信息(Mutual Information)》2篇博文中，详细总结了特征选择的基本方法专家推荐、方差分析和单变量相关性分析方法Pearson、Spearman、卡方检验、互信息方法。本文将最后介绍一下基于模型的特征选择方法，主要包括sklearn中的RFE、基于L1正则化的方法、基于树模型的方法还有python库eli5提供的方法Permutation importance，本文偏向于应用，理论比较少。<a id="more"></a></p><h1 id="基于模型的特征选择方法"><a href="#基于模型的特征选择方法" class="headerlink" title="基于模型的特征选择方法"></a>基于模型的特征选择方法</h1><h2 id="RFE-Recursive-feature-elimination"><a href="#RFE-Recursive-feature-elimination" class="headerlink" title="RFE(Recursive feature elimination)"></a>RFE(Recursive feature elimination)</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>这个方法其实比较简单，就是使用机器学习模型不断的去训练模型，每训练一个模型，就去掉一个最不重要的特征，直到特征达到指定的数量。从上面的描述可知，所使用的模型能表示出特征重要性排序。在sklearn中，带有<strong>coef_</strong> 和 <strong>‌feature_importances_</strong> 的模型都可以，这样的模型比较多，基于树的模型、线性的一系列模型大都满足要求，具体选择哪个，可以多尝试。</p><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>sklearn.feature_selection.RFE方法和sklearn.feature_selection.RFECV方法实现了这种方法，两者的区别是，后者使用了交叉验证。这里直接给出sklearn文档中的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_friedman1</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = make_friedman1(n_samples=<span class="number">50</span>, n_features=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>estimator = SVR(kernel=<span class="string">"linear"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>selector = RFECV(estimator, step=<span class="number">1</span>, cv=<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>selector = selector.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>selector.support_  <span class="comment"># 值为True的列表示被选择的特征</span></span><br><span class="line">array([ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br></pre></td></tr></table></figure><h2 id="基于L1正则化的方法"><a href="#基于L1正则化的方法" class="headerlink" title="基于L1正则化的方法"></a>基于L1正则化的方法</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>要理解为什么L1范数能进行特征选择，那就要从正则化说起，正则化是一种防止机器学习模型过拟合的方法，它的具体实现是在机器学习模型的损失函数加上一个惩罚项，即正则化项，其一般形式如下：</p><p>$$Loss function = \sum_{i=1}^NL(f(x_i), y_i)+\lambda J(w_i)$$</p><p>上式中的 $J(w_i)$ 函数即是正则化项，上式中的第一项称为期望损失。为什么能正则化项能起到防止过拟合的作用呢？模型过拟合，表现在其在训练集上表现的比较好，但泛化能力差。如果每个样本都是一个点，那过拟合的模型会剧烈的上下抖动尽力穿过每一个点，这样模型参数 $w_i$ 绝对值就比较大。在训练模型时，实际上不断的迭代模型参数 $w_i$ 使损失函数最小，为了达到这个目的，损失函数就希望期望损失和正则化项都较小，而实际上期望损失变小则正则化项就变大，两者不断博弈，最后达到一个最优的状态。这时候实际上对于无用的特征，其 $w_i$ 就会变的很小，这也是上节中RFE方法的基础所在。</p><p>回到L1正则化，其公式如下：</p><p>$$J(w_i)=|W|<em>1=\sum</em>{i=1}^N|w_i|$$</p><p>L1正则化有一个不一样的特点，当使用带正则化项的损失函数，当模型达到最优，无用特征前面的系数 $w_i$ 就会变成0，也就是起到了特征选择的作用。注意到上面公式正则化项前都有一个参数 $\lambda$ ，其是控制惩罚程度的参数，其越大越不容易过拟合。在特征选择时，其越大，则系数为0的特征越多。</p><p>在sklearn文档中，官方建议：如果是回归任务， 使用<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html" target="_blank" rel="noopener">linear_model.Lasso</a>做特征选择，其实L1又叫Lasso，Lasso模型损失函数的公式如下：</p><p>$$\frac{1}{2n}\sum_{i=1}^N||y_i-f(x_i, w_i)||^2 + \lambda||W||_1$$</p><p>如果是分类任务，使用逻辑回归<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank" rel="noopener">linear_model.LogisticRegression</a>和核为线性核的SVM<a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" target="_blank" rel="noopener">svm.LinearSVC</a>。</p><h3 id="使用示例-1"><a href="#使用示例-1" class="headerlink" title="使用示例"></a>使用示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.shape</span><br><span class="line">(<span class="number">150</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lsvc = LinearSVC(C=<span class="number">0.01</span>, penalty=<span class="string">"l1"</span>, dual=<span class="literal">False</span>).fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model = SelectFromModel(lsvc, prefit=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_new = model.transform(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_new.shape</span><br><span class="line">(<span class="number">150</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><h2 id="基于树模型的方法"><a href="#基于树模型的方法" class="headerlink" title="基于树模型的方法"></a>基于树模型的方法</h2><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><p>在《特征选择方法详解Part2-卡方检验、互信息(Mutual Information)》一文中，详细介绍了互信息方法。实际上经典决策树模型ID3，就是通过互信息，即信息增益实现建模的。后来的决策树模型C4.5使用信息增益率进行建模，无论哪种方法，都是先从“对数据集纯度影响大的特征”开始分支的，实际上建树的过程，就是特征选择的过程，也是特征重要性排序的过程。</p><p>这里附上一张我原来输出过的决策树的图，感性的看一下决策树长什么样（可点击看高清图），越靠近根部的特征越重要：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/tree.png" alt=""></p><h3 id="使用示例-2"><a href="#使用示例-2" class="headerlink" title="使用示例"></a>使用示例</h3><p>在sklearn中基于树的模型都可以做特征选择，主要包括树模型库sklearn.tree下面的树模型和集成学习方法库sklearn.ensemble下面的基于树的模型。同样，给一个官方文档中的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.shape</span><br><span class="line">(<span class="number">150</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf = ExtraTreesClassifier(n_estimators=<span class="number">50</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf = clf.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf.feature_importances_  </span><br><span class="line">array([ <span class="number">0.04</span>...,  <span class="number">0.05</span>...,  <span class="number">0.4</span>...,  <span class="number">0.4</span>...])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model = SelectFromModel(clf, prefit=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_new = model.transform(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_new.shape               </span><br><span class="line">(<span class="number">150</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="Permutation-importance"><a href="#Permutation-importance" class="headerlink" title="Permutation importance"></a>Permutation importance</h2><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>这个原理真的很简单：依次打乱数据集中每一个特征数值的顺序，其实就是做shuffle，然后观察模型的效果，下降的多的说明这个特征对模型比较重要。没了。</p><h3 id="使用示例-3"><a href="#使用示例-3" class="headerlink" title="使用示例"></a>使用示例</h3><p>下面示例中，参数model表示已经训练好的模型（支持sklearn中全部带有<strong>coef_</strong> 和 <strong>‌feature_importances_</strong> 的模型，部分pytorch和keras训练的深度学习模型）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> eli5</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> eli5.sklearn <span class="keyword">import</span> PermutationImportance</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>perm = PermutationImportance(model, random_state=<span class="number">20</span>).fit(test_x, test_y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>eli5.show_weights(perm, feature_names=test_x.columns.tolist())</span><br></pre></td></tr></table></figure><p>上面代码的输出见下图：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/feature_w.png" alt=""></p><h1 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h1><p>到这里，特征选择的所有方法就结束了。接下来一段时间可能专注于写深度学习领域的一些东西了。欢迎一起学习交流。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;到目前为止，已经在《特征选择方法详解Part1-方差分析、Pearson、Spearman》和《特征选择方法详解Part2-卡方检验、互信息(Mutual Information)》2篇博文中，详细总结了特征选择的基本方法专家推荐、方差分析和单变量相关性分析方法Pearson、Spearman、卡方检验、互信息方法。本文将最后介绍一下基于模型的特征选择方法，主要包括sklearn中的RFE、基于L1正则化的方法、基于树模型的方法还有python库eli5提供的方法Permutation importance，本文偏向于应用，理论比较少。
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jkknotes.com/categories/Machine-Learning/"/>
    
      <category term="特征工程" scheme="https://jkknotes.com/categories/Machine-Learning/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="特征工程" scheme="https://jkknotes.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="特征选择" scheme="https://jkknotes.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="RFE" scheme="https://jkknotes.com/tags/RFE/"/>
    
      <category term="L1" scheme="https://jkknotes.com/tags/L1/"/>
    
      <category term="Permutation importance" scheme="https://jkknotes.com/tags/Permutation-importance/"/>
    
  </entry>
  
  <entry>
    <title>特征选择方法详解Part2-卡方检验、互信息(Mutual Information)</title>
    <link href="https://jkknotes.com/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3Part2-%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C%E3%80%81%E4%BA%92%E4%BF%A1%E6%81%AF(Mutual%20Information)/"/>
    <id>https://jkknotes.com/特征选择方法详解Part2-卡方检验、互信息(Mutual Information)/</id>
    <published>2020-03-22T09:36:00.000Z</published>
    <updated>2020-03-23T12:33:40.145Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>在前文《特征选择方法详解Part1-方差分析、Pearson、Spearman》中，详细总结了特征选择的基本方法专家推荐、方差分析和单变量相关性分析方法Pearson、Spearman方法。在本文中将延续Part1的行文结构，介绍单变量相关性分析方法中的卡方检验和互信息方法。<a id="more"></a></p><h1 id="单变量分析"><a href="#单变量分析" class="headerlink" title="单变量分析"></a>单变量分析</h1><h2 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>卡方检验，又称 $\chi^2$ 检验，其用来衡量样本实际观测值与理论推断值之间的偏离程度。在特征工程计算相关性时，理论推断值可以理解为：假设此特征与目标变量（即label）无关，此特征的取值应该服从的分布。（不理解的话直接看下文中例子就明白了）。</p><p>先上公式（同样，不想看公式的话直接看下文中例子就明白了）：</p><p>$$\chi^2 = \sum_{i=1}^n\frac{(A-T)^2}{T}$$</p><p>其中A为实际观测值，T为理论推断值。从上式可以很清晰地看到，计算得到的 $\chi^2$ 值越大，相关性越高。</p><p>举个例子就很好明白了：假如现在有一批肿瘤内科患者的数据，想判断患者的收入水平（简单以穷人和富人代替）是否与患者患肠癌有关。简单制作下表表示真实观测值的分布：</p><table><thead><tr><th align="left"></th><th align="left">肠癌患者</th><th align="left">非肠癌患者</th><th align="left">总计</th></tr></thead><tbody><tr><td align="left">穷人</td><td align="left">67</td><td align="left">83</td><td align="left">150</td></tr><tr><td align="left">富人</td><td align="left">102</td><td align="left">64</td><td align="left">166</td></tr><tr><td align="left">总计</td><td align="left">169</td><td align="left">147</td><td align="left">316</td></tr></tbody></table><p>在上表中四个频数就是卡方检验公式里的A，即A=[67, 83, 102, 64]。</p><p>在这个数据中，肠癌患者约占总患者人数的53.48%。如果我们假设患者的收入水平与其患肠癌无关，那理论上，表格应该如下：</p><table><thead><tr><th align="left"></th><th align="left">肠癌患者</th><th align="left">非肠癌患者</th><th align="left">总计</th></tr></thead><tbody><tr><td align="left">穷人</td><td align="left">150x53.48% $\approx$ 80</td><td align="left">150x46.52% $\approx$ 70</td><td align="left">150</td></tr><tr><td align="left">富人</td><td align="left">166x53.48% $\approx$ 89</td><td align="left">166x46.52% $\approx$ 77</td><td align="left">166</td></tr><tr><td align="left">总计</td><td align="left">169</td><td align="left">147</td><td align="left">316</td></tr></tbody></table><p>那么这时候，理论推断值T=[80, 70, 89, 77]。</p><p>这下就很明白了，带入卡方检验公式得：</p><p>$$\chi^2 = \frac{(67-80)^2}{80} + \frac{(83-70)^2}{70} + \frac{(102-89)^2}{89} + \frac{(64-77)^2}{77} \approx 8.62$$</p><p>计算得到了 $\chi^2$ 值，如何判断“患者的收入水平与其患肠癌无关”的假设是否成立呢？聪明的人早已整理了卡方分布临界值表，根据 $\chi^2$ 值和其对应的自由度（自由度=(行数 - 1) * (列数 - 1)，如上表，行数和列数不计入“总计”列和行。）去表里查一下就可以了（见下图），这个例子中，8.62 &gt; 6.64, 就认为有1%的概率认为“患者的收入水平与其患肠癌无关”的假设成立，即有99%的置信度认为“患者的收入水平与其患肠癌有关”。</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200317/p3.png" alt=""></p><p>上面例子中，使用的变量如收入水平，使用的是离散值，而且从公式可以看到，需要统计频数信息，显然连续变量不适合统计频数。那么问题来了，<strong>连续值如何使用卡方检验方法呢？</strong> 其实很简单，只需要将连续变量离散化就可以了，想想决策树算法在连续变量上怎么使用的，就是将连续变量的取值范围划分区间，这样每个连续变量的值都有一个区间，就变成离散变量了。使用卡方检验也是一样的道理。</p><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>sklearn.feature_selection.chi2(X, y）可以计算 $\chi^2$ 值和对应的P-value，但是进行特征选择时，当然不是一个一个计算，而是批量计算。</p><p>sklearn.feature_selection提供了 SelectKBest和SelectPercentile方法，看名字就知道，SelectKBest选择前K个最好的特征，SelectPercentile则是按百分比选择特征。这2个方法可选择计算相关性的方法，具体可查看官方文档<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" target="_blank" rel="noopener">SelectKBest</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile" target="_blank" rel="noopener">SelectPercentile</a></p><p>这里给一个官方示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.shape</span><br><span class="line">(<span class="number">150</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_new = SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_new.shape</span><br><span class="line">(<span class="number">150</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="互信息（Mutual-Information）"><a href="#互信息（Mutual-Information）" class="headerlink" title="互信息（Mutual Information）"></a>互信息（Mutual Information）</h2><p>互信息这个方法，真正总结起来，远远比我想象的要复杂。</p><p>我仔细阅读了sklearn.feature_select中方法mutual_info_regression、mutual_info_classif的<a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_selection/_mutual_info.py" target="_blank" rel="noopener">源码</a>、sklearn.metrics中方法mutual_info_score、normalized_mutual_info_score、adjusted_mutual_info_score的<a href="https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/metrics/cluster/_supervised.py" target="_blank" rel="noopener">源码</a>和互信息的经典论文【A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E69, 2004.】，然后才弄清楚以下几点：</p><ul><li>mutual_info_regression、mutual_info_classif两种方法实现时的区别</li><li>sklearn.feature_select中互信息方法和sklearn.metrics中互信息方法的区别</li><li>当变量是连续变量的时候，如何计算的（我原先以为是将连续变量划分区间从而实现离散化，实际和我想象的远不一样）</li></ul><p>这一小节，我会对上面三个点进行总结性的介绍，如果有时间，我想我会专门写一篇文章详细介绍。</p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><h4 id="互信息（Mutual-Information）-1"><a href="#互信息（Mutual-Information）-1" class="headerlink" title="互信息（Mutual Information）"></a>互信息（Mutual Information）</h4><h5 id="定义一"><a href="#定义一" class="headerlink" title="定义一"></a>定义一</h5><p>了解决策树的生成过程的话，互信息的理解其实很简单。首先介绍一个概念：信息熵（information entropy）,按西瓜书的说法，其是度量样本集合纯度最常用的一种指标。看到“熵”（其物理含义是度量体系混乱程度的指标，值越大越混乱），再看到修饰词“信息”，我想它的含义已经很明确了。这里同样给出其在西瓜书上的公式：</p><p>$$Ent(D) = -\sum_{k=1}^{|Y|}p_k\log_2p_k$$</p><p>这里的D可以理解为一个变量（机器学习中可认为其是label，feature都可以定义为D）， $p_k$ 表示向量D中第k个取值所占的比例。Ent(D)值越小，则D纯度越高。</p><p>西瓜书中的信息增益（information gain）即是互信息。这里给出互信息的第一个公式(来自西瓜书，变量名修改了下)：</p><p>$$Gain(Y, X) = Ent(Y) - \sum_{v-1}^V\frac{|Y^v|}{|Y|}Ent(Y^v)$$</p><p>式中定义了变量Y和X之间的互信息，这里的 $v$ 表示X所有可能取值的编号， $Y_v$ 表示当 $x=x_v$ 时，其所对应的Y的值的集合， $|Y^v|$ 即表示此集合中的样本数量。举个例子吧，假如 $Y=[1,1,0,1], X=[1,2,1,1]$ ， 这时X的取值有2个：1和2，即 $v=[0, 1]$ , 当v=0, 即 $x=x_0=1$ 时， $|Y^v|=[1,0,1]$ , $|Y^v|=3$ .Gain(Y, X) 值越大，说明X和Y相关性越高。</p><p>从上面的公式和举例来看，明显X和Y是离散值。如果是连续值的话，可以将X和Y通过划分区间的方式离散化，具体可见西瓜书4.4节。</p><h5 id="定义二"><a href="#定义二" class="headerlink" title="定义二"></a>定义二</h5><p>直接上公式, 如果X和Y是离散变量：</p><p>$$MI(X,Y)=\sum_{i=1}^{|X|} \sum_{j=1}^{|Y|} \hat P(i,j)<br>        \log\frac{\hat P(i,j)}{P(i)P(j)}=\sum_{i=1}^{|X|} \sum_{j=1}^{|Y|} \frac{|X_i\cap Y_j|}{N}<br>        \log\frac{|X_i \cap Y_j|/N}{|X_i|/N \cdot |Y_j|/N}$$</p><p>如果X和Y是连续变量：</p><p>$$MI(X,Y)=\iint f(x,y) \log\frac{f(x,y)}{f(x)f(y)}$$</p><p>上式中 $f(x,y)$ 表示变量X和Y的联合概率密度函数， $f(x)$ 和 $f(y)$ 同理。话说这个式子，完全不知道怎么使用。</p><h5 id="定义三"><a href="#定义三" class="headerlink" title="定义三"></a>定义三</h5><p>直接上公式（来自于我看网上没人写过这个公式，因为很少有人懂吧）：</p><p>$$MI(X,Y)=\Psi(k)-&lt;\Psi(n_x+1)+\Psi(n_y+1)&gt;+\Psi(N)$$</p><p>上式 $\Psi$ 函数是 Gamma函数的导数，scipy.special.digamma可实现其计算；另外 $\lt X\gt=N^{-1}\sum_{i=1}^Nx_i$ ，上式中k是k近邻算法的参数， $n_x$ 表示使用X拟合一个k近邻算法后，给定中心点和半径，所有在半径内对数据的个数（我看sklearn上源码是这样算的，论文中也说了，但是没说实现细节）， $n_x$ 同理，N表示样本数。</p><p>这个公式引用自互信息的经典论文【A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E69, 2004.】。其从公式二推导而来（推导挺复杂，没看懂）。</p><p>为什么要写这个公式呢，sklearn中mutual_info_regression、mutual_info_classif方法在实现时，如果X和Y其中出现连续变量，则使用上式计算的互信息。</p><h4 id="Normalized-Mutual-Information"><a href="#Normalized-Mutual-Information" class="headerlink" title="Normalized Mutual Information"></a>Normalized Mutual Information</h4><p>Normalized Mutual Information将互信息归一化到0和1之间，0表示不相关，1表示很完美的相关，这样就更方便比较不同的feature和label的相关程度大小。如下为其公式：</p><p>$$NMI(X,Y)=\frac{MI(X,Y)}{F(H(X),H(Y))}, H(X)=-Ent(X)$$</p><p>上式中 $F(H(X),H(Y))$ F表示函数，其可以是均值(mean)、最大值(max)、最小值(min)、均方根函数(sqrt)</p><h4 id="Adjusted-Mutual-Information"><a href="#Adjusted-Mutual-Information" class="headerlink" title="Adjusted Mutual Information"></a>Adjusted Mutual Information</h4><p><strong>不管是NMutual Information还是Normalized Mutual Information都有一个缺点：对于离散变量X和Y，如果X和Y的取值越多，那么MI（X,Y）和NMI（X,Y）都倾向于变得更大，而这并不代表X和Y的相关性变得更强</strong>。于是，Adjusted Mutual Information应运而生，其公式如下：</p><p>$$AMI(X, Y)=\frac{MI(X, Y) - E(MI(X, Y))}{F(H(X), H(Y)) - E(MI(X, Y))}$$</p><p>至于上式中的 $E(MI(X, Y))$ 的定义，由于太长且很复杂，这里就不写了（主要自己没理解[捂脸]），给个维基百科的链接吧，有兴趣的朋友可以fan&amp;@¥墙看一下<a href="https://en.wikipedia.org/wiki/Adjusted_mutual_information" target="_blank" rel="noopener">Adjusted Mutual Information</a>）。</p><h3 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h3><ul><li><p>不管是NMutual Information还是Normalized Mutual Information都有一个缺点：对于离散变量X和Y，如果X和Y的取值越多，那么MI（X,Y）和NMI（X,Y）都倾向于变得更大，而这并不代表X和Y的相关性变得更强。所以现在使用更多的是Adjusted Mutual Information；</p></li><li><p>sklearn中mutual_info_regression方法在计算互信息时使用的是定义三；</p></li><li><p>sklearn中mutual_info_classif方法在计算互信息时，如果X和Y其中出现连续变量，使用定义三计算互信息，如果都是离散变量，则直接调用sklearn.metrics.mutual_info_score方法，即定义二中的第一个公式；</p></li><li><p>sklearn.metrics中方法mutual_info_score、normalized_mutual_info_score、adjusted_mutual_info_score本意是评价聚类算法效果的，所以这三种算法只支持离散变量。即使输入连续变量也会按照离散变量去计算，导致不正确的结果。</p></li></ul><h3 id="使用示例-1"><a href="#使用示例-1" class="headerlink" title="使用示例"></a>使用示例</h3><p>sklearn.feature_select中方法mutual_info_regression、mutual_info_classif的使用可见上节卡方检验的使用示例（只需要将方法chi2改成mutual_info_regressionu或mutual_info_classif即可）。</p><p>这里给一下sklearn.metrics中方法mutual_info_score、normalized_mutual_info_score、adjusted_mutual_info_score的示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mutual_info_score, normalized_mutual_info_score, adjusted_mutual_info_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rng = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y1 = rng.randint(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y2 = rng.randint(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y2[:<span class="number">20</span>] = y1[:<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"MI"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(mutual_info_score(y1, y2), mutual_info_score(y1 % <span class="number">3</span>, y2 % <span class="number">3</span>))</span><br><span class="line"><span class="number">0.5305136007637552</span>, <span class="number">0.026551556693229707</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"NMI"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(normalized_mutual_info_score(y1, y2), normalized_mutual_info_score(y1 % <span class="number">3</span>, y2 % <span class="number">3</span>))</span><br><span class="line"><span class="number">0.2381306291604139</span>, <span class="number">0.0252301499426718</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"AMI"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(adjusted_mutual_info_score(y1, y2), adjusted_mutual_info_score(y1 % <span class="number">3</span>, y2 % <span class="number">3</span>))</span><br><span class="line"><span class="number">0.03389853740628566</span>, <span class="number">0.0054476031181075295</span></span><br></pre></td></tr></table></figure><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>一下午码了这么多，肯定有一些错字或者遗漏，以后发现再改吧。</p><p>写到这里，单变量相关性分析方法就介绍完了。基于模型的特征选择方法可继续看Part3。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前文《特征选择方法详解Part1-方差分析、Pearson、Spearman》中，详细总结了特征选择的基本方法专家推荐、方差分析和单变量相关性分析方法Pearson、Spearman方法。在本文中将延续Part1的行文结构，介绍单变量相关性分析方法中的卡方检验和互信息方法。
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jkknotes.com/categories/Machine-Learning/"/>
    
      <category term="特征工程" scheme="https://jkknotes.com/categories/Machine-Learning/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="特征工程" scheme="https://jkknotes.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="特征选择" scheme="https://jkknotes.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="卡方检验" scheme="https://jkknotes.com/tags/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/"/>
    
      <category term="互信息(Mutual Information)" scheme="https://jkknotes.com/tags/%E4%BA%92%E4%BF%A1%E6%81%AF-Mutual-Information/"/>
    
  </entry>
  
  <entry>
    <title>特征选择方法详解Part1-方差分析、Pearson、Spearman</title>
    <link href="https://jkknotes.com/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3Part1-%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90%E3%80%81Pearson%E3%80%81Spearman/"/>
    <id>https://jkknotes.com/特征选择方法详解Part1-方差分析、Pearson、Spearman/</id>
    <published>2020-03-18T12:56:49.000Z</published>
    <updated>2020-03-18T13:31:11.667Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>正常的机器学习项目，除了数据清洗，特征工程是最耗时和耗精力的步骤，也是对整个项目效果好坏起决定性作用的步骤之一。特征工程更多的是经验性的东西，当然所有的经验都建立在对基本方法的了解之上。本文将对特征工程中<strong>特征选择</strong>的常用方法方差分析、Pearson、Spearman进行介绍。<a id="more"></a></p><h1 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h1><h2 id="专家推荐和业务理解"><a href="#专家推荐和业务理解" class="headerlink" title="专家推荐和业务理解"></a>专家推荐和业务理解</h2><p>此种方法是最基本、最简单也是最重要的一步。</p><p>当数据准备好，几百个特征摆在眼前，如果我们对业务很熟悉，有比较深刻的理解，我们自然能知道哪些特征很重要，而哪些特征没什么用，这样就自然而然的去掉了一些特征，再继续使用其他方法进行特征选择。</p><p>而当我们对业务不太熟悉，或者没办法短时间内有很深刻的理解。比如说一个纯技术的同学，突然接到一个关于医疗的项目，我想最聪明的办法，去找医生聊一聊，让他们给一些建议。</p><h2 id="方差分析"><a href="#方差分析" class="headerlink" title="方差分析"></a>方差分析</h2><p>方差分析是一个简单、基本的特征选择方法。简单来说，方差越大的特征，认为其越有用，而方差越小的特征，因为每个样本在此特征的值比较接近或者相同，就认为其对模型训练没有作用。sklearn中的方法VarianceThreshold可以很方便的完成此项工作。这里直接给出官方文档中的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], </span><br><span class="line">         [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], </span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sel = VarianceThreshold(threshold=(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sel.fit_transform(X)</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><h1 id="单变量分析"><a href="#单变量分析" class="headerlink" title="单变量分析"></a>单变量分析</h1><h2 id="Pearson-Correlation-Coefficient"><a href="#Pearson-Correlation-Coefficient" class="headerlink" title="Pearson Correlation Coefficient"></a>Pearson Correlation Coefficient</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>Pearson Correlation Coefficient（皮尔逊相关系数）表示2个向量的线性相关程度。</p><p>先上公式：</p><p>$$\rho(X, Y) = \frac{\sum_{i=1}^n(X-\mu_x)(Y-\mu_y)}{\sqrt{\sum_{i=1}^n(X_i - \mu_x)^2}\sqrt{\sum_{i=1}^n(Y_i - \mu_y)^2}}$$</p><p>可以看到分子是向量X和Y（也就是特征X和label Y）的协方差（Covariance），分母是X和Y方差的积。通俗的说，协方差描述2个向量线性相关的程度，如果一个变量跟随另一个变量同时变大或者变小，那么这两个变量的协方差就是正值，反之类似。但是X和Y的协方差的值比Z和Y的协方差值大，并不能代表X和Y的相关性比Z和Y的相关性强。为了消除这种类似于“量纲”不同带来的影响，于是将协方差除以2个向量的方差，即得到值在区间[-1, 1]的Pearson Correlation Coefficient（皮尔逊相关系数），-1表示2个变量完全负相关，1表示2个变量完全正相关。</p><p><strong>另一个我认为更好的解释是：</strong>仔细观察上式，其其实是向量 $X-\mu_x$ 和 $Y-\mu_y$ 夹角的余弦值，我们知道，当余弦值为1，2个向量完全重合，即完全正相关，当余弦值为-1， 2个向量方向相反，即完全负相关。那为什么不直接计算2个向量的余弦值而是还需要减去各自的均值，回忆高中数学，向量都是以原点为起点，以终点坐标表示该向量，即所有向量的数字表示都在同一坐标系下，这里减均值起到相同的效果。</p><p>其实上面的解释，如果将 $X-\mu_x$ 和 $Y-\mu_y$ 看成一个整体，说的也就是<strong>余弦相似度</strong>，在NLP任务中，其常被用来度量2个词向量的相似度。</p><p>同时在运用Pearson方法时，有一个很重要的前提：要对其进行显著性检验。我们的数据是从真实世界采集的，不一定代表真实世界数据的分布，即存在抽样误差，这就导致我们计算的皮尔逊相关性系数不一定有意义。至于如何证明二者有意义，这篇文章就不详细说了。聪明的数学家想到使用T检验计算一个值P-value，如果P-value小于0.05（大部分研究使用），就表明有95%的置信度相信真实世界的数据是有相关性的（可网上搜索详细原理）。</p><p>这里附一张图，可视化皮尔逊相关系数取不同值时，数据分布大致长什么样：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200317/p1.png" alt=""></p><h3 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h3><ul><li>使用pearson方法的数据应满足正态分布。这是显著性检验方法T检验的前提。</li><li>皮尔逊相关系数是用于衡量线性相关关系的，在不清楚数据分布的情况下，不能直接比较皮尔逊相关系数。比较简单的方法是画散点图看一下大概趋势。</li><li>异常值会对结果产生较大影响。</li><li>当皮尔逊相关系数为0，不能说明其完全不相关，可能是非线性相关。</li></ul><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = np.arange(<span class="number">7</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stats.pearsonr(a, b)</span><br><span class="line"><span class="comment"># 第一项Pearson相关系数，第二项P-value</span></span><br><span class="line">(<span class="number">0.8660254037844386</span>, <span class="number">0.011724811003954649</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stats.pearsonr([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">10</span>, <span class="number">9</span>, <span class="number">2.5</span>, <span class="number">6</span>, <span class="number">4</span>])</span><br><span class="line">(<span class="number">-0.7426106572325057</span>, <span class="number">0.1505558088534455</span>)</span><br></pre></td></tr></table></figure><p>另外，pandas的corr()方法，也可批量计算Pearson相关性系数。</p><p>看到这里，来一个玩相关性的小游戏 <a href="http://guessthecorrelation.com/" target="_blank" rel="noopener"><strong>Guess the Correlation</strong></a></p><h2 id="Spearman-Correlation-Coefficient"><a href="#Spearman-Correlation-Coefficient" class="headerlink" title="Spearman Correlation Coefficient"></a>Spearman Correlation Coefficient</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>Spearman Correlation Coefficient（斯皮尔曼相关系数）表示2个向量的<strong>单调</strong>相关程度，即变量Y随X增大而增大或者减小的程度，其取值范围为[-1,1]。</p><p>先上公式：</p><p>$$\rho_s = 1 - \frac{6\sum{d_i^2}}{n(n^2 - 1)}$$</p><p>对于已知的向量X=[4,5,2]和Y=[7,4,6]，首先将X和Y按大小进行排序，并按排序前X元素的位置，记下该元素排序后的次序。比如X从大往小排序后，4排第2位，5排第1位，2排第3位，排序后位置X’=[2,1,3], 同理得到Y’=[1,3,2].上式中的 $\sum{d_i^2}=(2-1)^2+(1-3)^2+(3-2)^2$ ，n就是向量中数据的个数，代入上式可求的斯皮尔曼相关系数为0.25.</p><p>同样，斯皮尔曼方法也需要进行显著性检验，P-value的含义如上文介绍，不再赘述。</p><h3 id="Notice-1"><a href="#Notice-1" class="headerlink" title="Notice"></a>Notice</h3><ul><li>使用斯皮尔曼方法，没有对数据分布的要求</li><li>斯皮尔曼相关系数表示的是2个向量的单调相关程度，即变量Y随X增大而增大或者减小的程度，即使其值为0，也不能说明完全不相关，因为可能是其他非单调相关（可看下面代码示例）。</li></ul><h3 id="使用示例-1"><a href="#使用示例-1" class="headerlink" title="使用示例"></a>使用示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = range(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = [x ** <span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> a] <span class="comment"># 非单调</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = [x ** <span class="number">3</span> <span class="keyword">for</span> x <span class="keyword">in</span> a] <span class="comment"># 单调递增</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stats.spearmanr(a, b)</span><br><span class="line">SpearmanrResult(correlation=<span class="number">-0.27609440361293197</span>, pvalue=<span class="number">0.44001452222946114</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stats.spearmanr(a, c)</span><br><span class="line">SpearmanrResult(correlation=<span class="number">0.9999999999999999</span>, pvalue=<span class="number">6.646897422032013e-64</span>)</span><br></pre></td></tr></table></figure><p>另外，pandas的corr()方法，也可批量计算Spearman相关性系数</p><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>Part 1就写到这里，卡方检验、互信息和基于模型的特征选择方法可继续看part2、part3</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正常的机器学习项目，除了数据清洗，特征工程是最耗时和耗精力的步骤，也是对整个项目效果好坏起决定性作用的步骤之一。特征工程更多的是经验性的东西，当然所有的经验都建立在对基本方法的了解之上。本文将对特征工程中&lt;strong&gt;特征选择&lt;/strong&gt;的常用方法方差分析、Pearson、Spearman进行介绍。
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jkknotes.com/categories/Machine-Learning/"/>
    
      <category term="特征工程" scheme="https://jkknotes.com/categories/Machine-Learning/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="特征工程" scheme="https://jkknotes.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="特征选择" scheme="https://jkknotes.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="Pearson" scheme="https://jkknotes.com/tags/Pearson/"/>
    
      <category term="Spearman" scheme="https://jkknotes.com/tags/Spearman/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle《SIIM-ACR Pneumothorax Segmentation》第一名方案详解</title>
    <link href="https://jkknotes.com/kaggle%E3%80%8ASIIM-ACR%20Pneumothorax%20Segmentation%E3%80%8B%E7%AC%AC%E4%B8%80%E5%90%8D%E6%96%B9%E6%A1%88%E8%AF%A6%E8%A7%A3/"/>
    <id>https://jkknotes.com/kaggle《SIIM-ACR Pneumothorax Segmentation》第一名方案详解/</id>
    <published>2020-01-16T11:30:29.000Z</published>
    <updated>2020-01-16T15:36:09.955Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p>当初刷微博无意中看到这个比赛，内容和自己所在团队的工作内容很契合，就花了点时间学习了下第一名的方案。在这里把自己学习的成果记录下来，也希望和大家一起讨论学习。<a id="more"></a></p></blockquote><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>引用一段比赛页面对这个比赛的介绍：</p><blockquote><p>“Pneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying—it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event.</p></blockquote><blockquote><p>Pneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more confident diagnosis for non-radiologists.”</p></blockquote><p>比赛地址：<a href="https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/overview/description" target="_blank" rel="noopener">SIIM-ACR Pneumothorax Segmentation</a></p><p>Data: </p><ul><li><p>10679份dicom文件（站立位胸片），有气胸：无气胸 = 2379:8300</p></li><li><p>有气胸的胸片皆有mask, run-length-encoded (RLE)格式</p></li><li><p>一个胸片如果有多处气胸，会有多个单独的mask</p></li></ul><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg1.png" alt="数据示例"></p><p>Evaluation: Dice coefficient:</p><p>$$\frac {2*|X \bigcap Y|}{|X|+|Y|}$$</p><h1 id="OverView"><a href="#OverView" class="headerlink" title="OverView"></a>OverView</h1><p>用一张图来表示作者的思路：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg2_0.png" alt=""></p><p>作者在训练模型时，采用5折交叉验证，并将模型训练分成了4个阶段（下文中分别表示成part0，part1，part2，part3），每个阶段都是上图中这样一个完整的过程，后一阶段直接在前一阶段的最优模型上fine-tuning.每一个阶段有不同的数据构成和模型参数。</p><h1 id="Input-Data"><a href="#Input-Data" class="headerlink" title="Input Data"></a>Input Data</h1><h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h2><p>这里模型的输入是1024X1024X3的胸片和1024X1024X1的mask。作者直接使用了图像增强库 <a href="https://albumentations.readthedocs.io/en/latest/" target="_blank" rel="noopener">albumentations</a> 对数据进行增强，作者使用了一个较为复杂的方案：以固定顺序对图像进行不同的变换，并且给予每种方法一定的概率，使增强方法的运用随机化。具体如下:</p><ol><li>HorizontalFlip：0.5</li><li>OneOf：0.3<ol><li>RandomContrast：0.5</li><li>RandomGamma：0.5</li><li>RandomBrightness：0.5</li></ol></li><li>OneOf：0.3<ol><li>ElasticTransform：0.5</li><li>GridDistortion：0.5</li><li>OpticalDistortion：0.5</li></ol></li><li>ShiftScaleRotate：0.5</li></ol><p>上面小数表示此增强方法运用的概率，OneOf表示在其子方法中选择一个。</p><p>需要注意的是，这里有2个OneOf，第一个OneOf下面的增强方法主要对图像的亮度、对比度等进行调整，而第二个OneOf下面的增强方法主要对图像的形状进行调整。可以看几个例子。</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg2.png" alt="图像增强示例1"></p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg3.png" alt="图像增强示例2"></p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg4.png" alt="图像增强示例3"></p><p>因为训练模型分成四个阶段，每个阶段使用的数据是一样的，使用一定概率给图像做增强，实际上保证了每个阶段实际参加训练的数据都不完全一样。</p><h2 id="Sliding-Sample"><a href="#Sliding-Sample" class="headerlink" title="Sliding Sample"></a>Sliding Sample</h2><p>对无气胸的样本随机下采样，使有气胸的样本占0.8（part0），0.6（part1），0.4（part2），0.5（part3）</p><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Model-Zoo"><a href="#Model-Zoo" class="headerlink" title="Model Zoo"></a>Model Zoo</h2><ul><li>Albunet: UNet <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">(论文链接)</a> with Resnet34<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">(论文链接)</a> encoder proposed by Alexander Buslaev</li></ul><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg5.png" alt="网络结构"></p><p>UNet简直是图像分割的神器，特别是在医疗图像分割的上，目前其各种变体网络仍是各种比赛的主力。Unet主要有2个特点：<strong>1.U型结构；2.skip connection.</strong></p><p>– U型结构encoder的下采样和decoder的上采样的次数相同，这就保证了模型的输出恢复到原图片的分辨率，实现端到端的预测。</p><p>– skip connection的结构使模型结合不同level的feature map上进行学习，相比于FCN分割边缘更清晰。</p><p>同时Albunet的网络的encoder使用的是ResNet，其由何凯明大佬于2015年提出（Unet也是这一年提出，回过头看，这一年真的是丰收的一年），同样风靡至今。从理论上讲，越高级的特征，应该有越强的表征能力，而VGG网络证明，网络的深度对特征的表达能力至关重要。</p><p>理想情况下，当我们直接对网络进行简单的堆叠到特别长，网络内部的特征在其中某一层已经达到了最佳的情况，这时候剩下层应该不对改特征做任何改变，自动学成恒等映射的形式。也就是说，对一个特别深的深度网络而言，该网络的浅层形式的解空间应该是这个深度网络解空间的子集，但实际上，如果使用简单是网络堆叠，由于网络性能衰减，网络的效果反而越差。为了深度网络后面的层至少实现恒等映射的作用，作者提出了residual模块。</p><p>除了Albunet，作者还尝试了如下2种网络进行实验：</p><ul><li><p>Resnet50: <a href="https://github.com/SpaceNetChallenge/SpaceNet_Off_Nadir_Solutions/tree/master/selim_sef/zoo" target="_blank" rel="noopener">GitHub链接</a></p></li><li><p>SCSEUnet：也就是SENet(<a href="https://arxiv.org/abs/1709.01507v1" target="_blank" rel="noopener">(论文：squeeze-and-excitation network)</a>)</p></li></ul><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>作者使用BCE，Dice和Focal loss加权的方式作为最终的损失函数：</p><p>$$Loss = W_1 * StableBCELoss + W_2 * DiceLoss + W_3 * FocalLoss2d()$$</p><p>这里需要注意的是，损失函数StableBCELoss的输入是没有进行sigmoid计算的模型输出，而其他2个损失函数的输入是经过sigmoid计算的模型输出。作者使用这三个损失函数，是想从三个维度驱动网络学习。</p><p>– Focal loss主要解决正负样本失衡的问题，在医学图像分割的这种像素级分类的任务中，往往都是正样本较少，负样本较多，气胸的分割同样不例外。其公式如下：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg9.png" alt=""></p><p>从如上公式上可以看到，某个样本输出的概率越高，其产生的loss越小，实际上达到了促使网络学习困难样本的目的（在分割任务中，目标像素就是困难样本，因为其数量较少）。</p><p>– Dice loss在很多关于医学图像分割的竞赛、论文和项目中出现的频率很高，此比赛的评价指标也是它。从前文它的公式可以看到，其表示的是预测的轮廓与真实的mask的相似程度。其公式如下：</p><p>$$DiceLoss = 1 - \frac {2*|X \bigcap Y|}{|X|+|Y|}$$</p><p>– StableBCELoss, 这里非常奇怪的是，其输入是没有进行sigmoid计算的模型输出，至今没想通（to do：等搞明白了补上）。这里附上pytorch文档中其公式：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg10.png" alt=""></p><p>最终作者寻优，得到各个损失函数的权重如下：</p><ul><li><p>(3,1,4) for albunet_valid and seunet;</p></li><li><p>(1,1,1) for albunet_public;</p></li><li><p>(2,1,2) for resnet50.</p></li></ul><h2 id="Learn-Rate-Scheduler"><a href="#Learn-Rate-Scheduler" class="headerlink" title="Learn Rate Scheduler"></a>Learn Rate Scheduler</h2><p>作者在不同的训练阶段，采用了不同的学习率调整策略，具体如下：</p><table><thead><tr><th>Phase</th><th>Start Lr</th><th>Scheduler</th></tr></thead><tbody><tr><td>part0</td><td>1e-4</td><td>ReduceLROnPlateau</td></tr><tr><td>part1</td><td>1e-5</td><td>ReduceLROnPlateau or CosineAnnealingWarmRestarts</td></tr><tr><td>part2</td><td>1e-5</td><td>ReduceLROnPlateau or CosineAnnealingWarmRestarts</td></tr><tr><td>part3</td><td>1e-6</td><td>ReduceLROnPlateauor CosineAnnealingWarmRestarts</td></tr></tbody></table><h3 id="torch-optim-lr-scheduler-ReduceLROnPlateau"><a href="#torch-optim-lr-scheduler-ReduceLROnPlateau" class="headerlink" title="torch.optim.lr_scheduler.ReduceLROnPlateau"></a>torch.optim.lr_scheduler.ReduceLROnPlateau</h3><p>其基本思想是：当参考的评价指标停止变优时,降低学习率，挺实用的方法。</p><p>Code Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">scheduler = ReduceLROnPlateau(optimizer, <span class="string">'min'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">     train(...)</span><br><span class="line">     val_loss = validate(...)</span><br><span class="line">     <span class="comment"># Note that step should be called after validate()</span></span><br><span class="line">     scheduler.step(val_loss)</span><br></pre></td></tr></table></figure><h3 id="torch-optim-lr-scheduler-CosineAnnealingLR"><a href="#torch-optim-lr-scheduler-CosineAnnealingLR" class="headerlink" title="torch.optim.lr_scheduler.CosineAnnealingLR"></a>torch.optim.lr_scheduler.CosineAnnealingLR</h3><p>可以翻译为余弦退火调整学习率：根据论文<a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">SGDR: Stochastic Gradient Descent with Warm Restarts</a>而来，只包含论文里余弦退火部分，并不包含restart部分。</p><p>当使用SGD时，模型的Loss应越来约接近全局最小值。当它逐渐接近这个最小值时，学习率应该变得更小来使得模型尽可能接近这一点。</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg11.png" alt="网上随便找了个图"></p><p>从上图可以看出，随着x的增加，余弦值首先缓慢下降，然后加速下降，再次缓慢下降。这种下降模式能和学习率配合，比较有效的训练模型。</p><h3 id="torch-optim-lr-scheduler-CosineAnnealingWarmRestarts"><a href="#torch-optim-lr-scheduler-CosineAnnealingWarmRestarts" class="headerlink" title="torch.optim.lr_scheduler.CosineAnnealingWarmRestarts"></a>torch.optim.lr_scheduler.CosineAnnealingWarmRestarts</h3><p>SGDR：这个方法就是论文<a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">SGDR: Stochastic Gradient Descent with Warm Restarts</a>的完整实现。</p><p>在训练时，梯度下降算法可能陷入局部最小值，而不是全局最小值。做随机梯度下降时可以通过突然提高学习率，来“跳出”局部最小值并找到通向全局最小值的路径。具体可以看下图：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg12.png" alt="网上随便找了个图"></p><h1 id="Post-Proprecess"><a href="#Post-Proprecess" class="headerlink" title="Post Proprecess"></a>Post Proprecess</h1><p>我们训练的分割模型输出每个像素的0-1概率，然后卡一下阈值，我们可以称这样的mask为basic sigmoid mask, 实际上医学图像中我们的分割目标也许并不存在，所以常用双重阈值（top_score_threshold, min_contour_area）的方法计算出mask并同时判断是否有分割目标（在本次比赛中我们分割目标是气胸），这种方法且称为doublet。其具体逻辑为：当大于概率阈值top_score_threshold的pixel数少于<br>min_contour_area，就将mask像素值全部置0，也就是认为此胸片没有气胸。简单画个示意图如下：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg6.png" alt="Doublet E.G1"></p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg7.png" alt="Doublet E.G2"></p><p>而作者在此基础上作了改进，使用了三重阈值（top_score_threshold, min_contour_area, bottom_score_threshold）的方法来达到相同的目标，且称改进后的方法为Triplet.其具体逻辑为：当大于概率阈值top_score_threshold的pixel数少于<br>min_contour_area，就将此mask pixel值全部置0，也就是认为此胸片没有气胸，然后再使用阈值bottom_score_threshold产生真正的mask。简单画个示意图如下：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/p_seg8.png" alt="Triplet"></p><p>最终作者通过搜索，分别获得了在validation和在Public Leaderboard上的最优参数：</p><ul><li>Best triplet on validation: (0.75, 2000, 0.3).</li><li>Best triplet on Public Leaderboard: (0.7, 600, 0.3)</li></ul><p>最后再附上作者的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classification_mask = predicted &gt; top_score_threshold</span><br><span class="line">mask = predicted.copy()</span><br><span class="line">mask[classification_mask.sum(axis=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)) &lt; min_contour_area, :,:,:] = np.zeros_like(predicted[<span class="number">0</span>])</span><br><span class="line">mask = mask &gt; bot_score_threshold</span><br><span class="line"><span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><h1 id="Train-and-Inference"><a href="#Train-and-Inference" class="headerlink" title="Train and Inference"></a>Train and Inference</h1><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>前面就已经提到过，作者将训练过程分为了4个阶段，这里在最前面新加一个预训练阶段。梳理如下：</p><table><thead><tr><th align="left">Phase</th><th align="left">Sample rate</th><th align="left">Start lr</th><th align="left">Scheduler</th><th align="left">Epochs</th><th align="left">Note</th></tr></thead><tbody><tr><td align="left">part pre</td><td align="left"></td><td align="left"></td><td align="left"></td><td align="left"></td><td align="left">The model be pretrained on our dataset with lower resolution (512x512)</td></tr><tr><td align="left">part0</td><td align="left">0.8</td><td align="left">1e-4</td><td align="left">ReduceLROnPlateau</td><td align="left">10-12</td><td align="left">The goal of this part: quickly get a good enough model with validation score about 0.835</td></tr><tr><td align="left">part1</td><td align="left">0.6</td><td align="left">1e-5</td><td align="left">ReduceLROnPlateau or CosineAnnealingWarmRestarts</td><td align="left">Repeat until best convergence</td><td align="left">uptrain the best model from the previous step</td></tr><tr><td align="left">part2</td><td align="left">0.4</td><td align="left">1e-5</td><td align="left">ReduceLROnPlateau or CosineAnnealingWarmRestarts</td><td align="left">Repeat until best convergence</td><td align="left">uptrain the best model from the previous step</td></tr><tr><td align="left">part3</td><td align="left">0.5</td><td align="left">1e-6</td><td align="left">ReduceLROnPlateauor CosineAnnealingWarmRestarts</td><td align="left">Repeat until best convergence</td><td align="left">uptrain the best model from the previous step</td></tr></tbody></table><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>作者采用五折交叉验证训练模型，并选择每一个fold的top3个模型的结果求平均输出最终的mask。这里的求平均每个像素点的概率求平均。模型的最终效果如下表：</p><p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200112/dashboard.png" alt=""></p><p>NOTE：</p><ul><li><p>albunet_public - best model for Public Leaderboard</p></li><li><p>albunet_valid - best resnet34 model on validation</p></li><li><p>seunet - best seresnext50 model on validation</p></li><li><p>resnet50 - best resnet50 model on validation</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;当初刷微博无意中看到这个比赛，内容和自己所在团队的工作内容很契合，就花了点时间学习了下第一名的方案。在这里把自己学习的成果记录下来，也希望和大家一起讨论学习。
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://jkknotes.com/categories/Deep-Learning/"/>
    
      <category term="项目实战" scheme="https://jkknotes.com/categories/Deep-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="AI医疗" scheme="https://jkknotes.com/tags/AI%E5%8C%BB%E7%96%97/"/>
    
      <category term="Pneumothorax Segmentation" scheme="https://jkknotes.com/tags/Pneumothorax-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>awk命令详解</title>
    <link href="https://jkknotes.com/awk%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"/>
    <id>https://jkknotes.com/awk命令详解/</id>
    <published>2019-11-13T13:42:36.000Z</published>
    <updated>2020-01-16T15:33:21.769Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>awk 其名称的由来很简单，就是由其三个创始人Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母组成。很多人都说它是一个效率神器，以前也多多少少用过它一些简单的用法。后来python一上手，pandas一用上，反而忘了它。正所谓技多不压身，这么高大上的程序语言（速度快，常用命令简单， 效率高），一定要学会它。所以这篇文章特将awk的基本语法，常用的命令以及相对应的命令记录下来，以备需要时直接复制使用。<a id="more"></a></p><p>awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。awk脚本通常用来格式化文本文件中的信息。awk是以文件的每一行为处理单位的，即其对所接收文件或者其他文本内容的一行执行相应的命令，来处理文本。</p><h1 id="基本格式"><a href="#基本格式" class="headerlink" title="基本格式"></a>基本格式</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;命令&#125;'</span> file1, file2, ...</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其他命令的输出 ｜ awk <span class="string">'&#123;命令&#125;'</span></span><br></pre></td></tr></table></figure><p>举个例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $1&#125;'</span> demo.txt</span><br></pre></td></tr></table></figure><p>awk的命令一般写在花括号里，但是花括号并不是必须的，某些情况下它可以省略，或者压根不需要。下面看具体讲解和实例。</p><h1 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h1><h2 id="awk内置变量"><a href="#awk内置变量" class="headerlink" title="awk内置变量"></a>awk内置变量</h2><p>前面说了，awk一般以行为单位进行处理，下表中的<strong>记录行</strong>即表示文件的一行，这是默认情况下以换行符作为判断为一行的标志，当然也可以手动修改以其他字符作为换行标志。下表中的字段可以理解为使用分隔符分隔记录行后的每一个元素。<br>以下是awk内置变量及其含义：</p><table><thead><tr><th>变量名</th><th>含义</th></tr></thead><tbody><tr><td>$0</td><td>当前记录行</td></tr><tr><td>$1 ~ $n</td><td>当对当前记录行进行分隔时，表示分隔后第几个字段</td></tr><tr><td>FS</td><td>字段分隔符，默认是空格</td></tr><tr><td>RS</td><td>记录行分隔符，默认为换行符</td></tr><tr><td>NF</td><td>对记录行进行分隔后，字段的个数</td></tr><tr><td>NR</td><td>已经读出对记录数，从1开始，多个文件时继续累加计数</td></tr><tr><td>FNR</td><td>已经读出对记录数，从1开始，多个文件时每个文件单独计数</td></tr><tr><td>ORS</td><td>输出时的记录行分隔符， 默认是换行符</td></tr><tr><td>OFS</td><td>输出时的字段分隔符， 默认是空格</td></tr></tbody></table><h2 id="n、-F、FS、RS"><a href="#n、-F、FS、RS" class="headerlink" title="$n、-F、FS、RS"></a>$n、-F、FS、RS</h2><p>首先看 <strong>$0 ~ $n 和 -F</strong> 的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cat awk_test.txt</span><br><span class="line">a,b,c,d</span><br><span class="line">e,f,g,h</span><br><span class="line"></span><br><span class="line">&gt;&gt; awk <span class="string">'&#123;print $0&#125;'</span> awk_test.txt <span class="comment"># 实例1</span></span><br><span class="line">a,b,c,d</span><br><span class="line">e,f,g,h</span><br><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'&#123;print $1,$2,$3,$4&#125;'</span> awk_test.txt <span class="comment"># 实例2</span></span><br><span class="line">a b c d</span><br><span class="line">e f g h</span><br></pre></td></tr></table></figure><p>这里的 ‘-F’ 命令用于定义按什么进行分隔，功能同python里的split。‘$0’ 表示整个记录行（实例1），‘$1,$2,$3,$4’表示分隔后的每个元素。</p><p>再继续来看<strong>FS和RS</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'&#123;print $1 FS $2,$3,$4&#125;'</span> awk_test.txt <span class="comment"># 实例3</span></span><br><span class="line">a,b c d</span><br><span class="line">e,f g h</span><br><span class="line"></span><br><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'&#123;print $1 RS $2,$3,$4&#125;'</span> awk_test.txt <span class="comment"># 实例4</span></span><br><span class="line">a</span><br><span class="line">b c d</span><br><span class="line">e</span><br><span class="line">f g h</span><br></pre></td></tr></table></figure><p>实例3中，FS的值已经由‘-F’命令赋值成‘，’了；实例4中，RS默认为换行符，所以输出结果进行了换行。</p><p>如果想用多个分隔符分隔记录行，只需要将多个分隔符放在 [分隔符1 隔符2 ..] 中，如果想用一个或者多个相同分隔符进行分隔，可以写成  [分隔符1 隔符2 ..]+ 。继续看例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">echo</span> <span class="string">'I am Poe,my qq is 0000001'</span> | awk -F <span class="string">'[" ",]'</span> <span class="string">'&#123;print $3 " " $7&#125;'</span></span><br><span class="line">Poe 0000001</span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">echo</span> <span class="string">'I am Poe,,my qq is 0000001'</span> | awk -F <span class="string">'[" ",]+'</span> <span class="string">'&#123;print $3 " " $7&#125;'</span></span><br><span class="line">Poe 0000001</span><br></pre></td></tr></table></figure><p>不解释了，已经很清楚了。</p><h2 id="NF、NR、FNR"><a href="#NF、NR、FNR" class="headerlink" title="NF、NR、FNR"></a>NF、NR、FNR</h2><p>先看 <strong>NF和NR</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cat awk_test.txt</span><br><span class="line">a,b,c,d</span><br><span class="line">e,f,g,h</span><br><span class="line"></span><br><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'&#123;print NR") "$0"\t size is "NF&#125;'</span> awk_test.txt</span><br><span class="line">1) a,b,c,dsize is 4</span><br><span class="line">2) e,f,g,hsize is 4</span><br></pre></td></tr></table></figure><p>可以看到，结果中有行号，即为NR， 输出结果中的 4 即为字段数。<br>再看以下<strong>FNR及其和NR的区别</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cat file.txt</span><br><span class="line">aaaaaa</span><br><span class="line">bbbbbb</span><br><span class="line"></span><br><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'&#123;print NR") "$0&#125;'</span> awk_test.txt file.txt</span><br><span class="line">1) a,b,c,d</span><br><span class="line">2) e,f,g,h</span><br><span class="line">3) aaaaaa</span><br><span class="line">4) bbbbbb</span><br><span class="line"></span><br><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'&#123;print FNR") "$0&#125;'</span> awk_test.txt file.txt</span><br><span class="line">1) a,b,c,d</span><br><span class="line">2) e,f,g,h</span><br><span class="line">1) aaaaaa</span><br><span class="line">2) bbbbbb</span><br><span class="line"></span><br><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'&#123;if (NR==FNR) print $0&#125;'</span> awk_test.txt file.txt</span><br><span class="line">a,b,c,d</span><br><span class="line">e,f,g,h</span><br></pre></td></tr></table></figure><p>很清楚了吧</p><h2 id="OFS、ORS"><a href="#OFS、ORS" class="headerlink" title="OFS、ORS"></a>OFS、ORS</h2><p>同样的看个例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">echo</span> <span class="string">'1 2 3'</span> | awk <span class="string">'BEGIN &#123;OFS="|"&#125; &#123;print $1,$2,$3&#125;'</span> <span class="comment"># 实例5</span></span><br><span class="line">1|2|3</span><br><span class="line"></span><br><span class="line">&gt;&gt; cat awk_test.txt | awk -F <span class="string">','</span> <span class="string">'&#123;print $1$2&#125;'</span></span><br><span class="line">ab</span><br><span class="line">ef</span><br><span class="line"></span><br><span class="line">&gt;&gt; cat awk_test.txt | awk -F <span class="string">','</span> <span class="string">'BEGIN &#123;ORS="---"&#125; &#123;print $1$2&#125;'</span> <span class="comment"># 实例6</span></span><br><span class="line">ab---ef---</span><br></pre></td></tr></table></figure><p>这里出现了一个新的语法，BEGIN，其作用是定义在命令执行之前需要执行的操作，在实例5中，在命令执行之前将输出时的字段分隔符赋值成了‘｜’。实例6同理。</p><h2 id="BEGIN、END-命令、运算符"><a href="#BEGIN、END-命令、运算符" class="headerlink" title="BEGIN、END 命令、运算符"></a>BEGIN、END 命令、运算符</h2><h3 id="BEGIN-和-END-命令"><a href="#BEGIN-和-END-命令" class="headerlink" title="BEGIN 和 END 命令"></a>BEGIN 和 END 命令</h3><p>在许多编程情况中，需要在 awk 开始处理输入文件中的文本之前执行初始化操作。这个时候 可以定义一个 BEGIN 块。</p><p>因为 awk 在开始处理输入文件之前会执行 BEGIN 块，因此它是初始化 各种变量（包括内置变量，全局变量）的极佳位置。<br>相对应的，awk 还提供了另一个特殊块，叫作 END 块。 awk 在处理了输入文件中的所有行之后执行这个块。通常， END 块用于执行最终计算或打印应该出现在输出流结尾的摘要信息。</p><p>下面这个例子的作用是统计某个文件夹下的文件占用的字节数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; ll |awk <span class="string">'BEGIN &#123;size=0&#125; &#123;size=size+$5&#125; END&#123;print "[end]size is ",size/2014/1024, "M"&#125;'</span></span><br><span class="line">[end]size is 3.3 M</span><br></pre></td></tr></table></figure><p>这样一个简单的例子信息量还是挺丰富的。首先可以看到所有的命令都在 ‘ ’ 里，BEGIN and END 以及记录行正常的操作部分，都有单独的代码块，用 {} 区分。另外我们可以看到，awk支持自定义变量，还可以进行各种运算。实际上awk也是一个轻量级的编程语言。这里代码“BEGIN {size=0}”可以省略，awk默认size=0.</p><h3 id="awk运算符"><a href="#awk运算符" class="headerlink" title="awk运算符"></a>awk运算符</h3><p>既然awk也算是一个编程语言，那自然支持各种运算。如下表所示：</p><table><thead><tr><th>运算符</th><th>解释</th></tr></thead><tbody><tr><td>=、+、-、+=、-=、<em>=、/=、%=、*</em>=、++、–等</td><td>赋值及数学运算</td></tr><tr><td>||、&amp;&amp;</td><td>逻辑或、逻辑与</td></tr><tr><td>&lt;、&lt;=、&gt;、&gt;=、!=、==</td><td>关系运算符</td></tr><tr><td>?:</td><td>三目运算符</td></tr><tr><td>in</td><td>数组中是否存在某键值</td></tr></tbody></table><p>随便举个例子，其余的看看就好：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; awk <span class="string">'BEGIN&#123;a="b";print a=="b"?"ok":"err"&#125;'</span></span><br><span class="line">ok</span><br><span class="line">&gt;&gt; awk <span class="string">'BEGIN&#123;a="b";print a=="c"?"ok":"err"&#125;'</span></span><br><span class="line">err</span><br></pre></td></tr></table></figure><h3 id="awk-数组和循环"><a href="#awk-数组和循环" class="headerlink" title="awk 数组和循环"></a>awk 数组和循环</h3><p>awk的循环包含while、do…while、for循环。这里不介绍了。具体可看链接：<a href="https://www.cnblogs.com/ginvip/p/6352157.html" target="_blank" rel="noopener">https://www.cnblogs.com/ginvip/p/6352157.html</a></p><p>说说数组。awk的数组像极了python里的dict。有数组的地方，常常也有循环，当然这不是绝对的。<br>这次的实例使用一个真实的log文件，先看看log前2行长什么样（涉及到实际业务，做了一些处理）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cat awk_log.log | head -n 2</span><br><span class="line">2019-10-22 18:11:42,726 feature_name_unify.py[line:99] ERROR: d_type, origin_name, sample_type, exam_name: exam,awk,linux,awk命令简介</span><br><span class="line">2019-10-22 18:11:42,727 feature_name_unify.py[line:99] ERROR: d_type, origin_name, sample_type, exam_name: exam,grep,linux,awk命令简介1</span><br></pre></td></tr></table></figure><p>整个log文件有0.18亿行。现在想统计按逗号分隔后倒数第三个位置会出现哪几种字符，及其频数。为了简单起见，用前20行做统计：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cat awk_log.log | head -n 20 | awk -F <span class="string">','</span> <span class="string">'&#123;a[$6]++&#125; END &#123;for (x in a) print x "\t" a[x]&#125;'</span></span><br><span class="line">awk7</span><br><span class="line">grep6</span><br><span class="line">seed3</span><br><span class="line">ls3</span><br><span class="line"><span class="built_in">pwd</span>1</span><br></pre></td></tr></table></figure><p>这个例子先将这个log文件的前20行输出，以便awk处理；在使用awk处理时，先使用逗号进行分隔；后面对记录行的每一个操作都写在单引号 ‘’ 里，每一个{}是一个代码块；对于代码块 {a[$6]++} ，a表示一个数组，这个数组可以理解为python里的dict，$6是其索引；初始时，a[$6] = 0，后面随着处理的记录行增加，每来一个相同的索引，就加1；END {for (x in a) print x “\t” a[x]}在处理完所有记录行后执行，此时用一个循环，打印出数组a的索引及其值。就如同打印出python字典里的key和value。</p><p>再来一个更复杂的例子，统计一个公司每个部门的人数及具体姓名：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cat company.csv | head -n 3</span><br><span class="line">部门,姓名,性别</span><br><span class="line">技术研发部,张三,男</span><br><span class="line">财务部,韩梅梅,女</span><br><span class="line"></span><br><span class="line">&gt;&gt; cat company.csv | head -n 16 ｜ awk -F <span class="string">','</span> <span class="string">'&#123;if (NR==1) next&#125;&#123;a[$1]++;b[$1]=b[$1]","$2&#125; END &#123;for (x in a) print x "\t" a[x] "\t" b[x&#125;'</span></span><br><span class="line">技术研发部5,张三,李四,王五,李雷,tom</span><br><span class="line">财务部4,张一,李二,王三,Bob</span><br><span class="line">商务部3,张二,李三,王四</span><br><span class="line">战略部门2,张五,王六</span><br><span class="line">总裁办1,三一</span><br></pre></td></tr></table></figure><p>这里新出现了一个语法 ‘next’， 它的作用等同于 continue。这个命令出现，就不会执行后面的命令了。这里它的作用是跳过第一行，因为第一行是csv文件的header。这个例子看起来毫无意义，但是在实际处理中，特别是一些log文件，数据量几千万到亿级，用python处理代码写起来也很简单，但是论速度，肯定远远不及awk。awk处理起来很快，比常用的编程语言都要快。</p><h1 id="3-awk的正则表达式"><a href="#3-awk的正则表达式" class="headerlink" title="3. awk的正则表达式"></a>3. awk的正则表达式</h1><p>正则表达式，不管什么语言，语法都是基本一样的。在实际工作中也用过不少，却始终不够熟练，甚至有点蒙。awk我也是如此。这一部分 不详细讲解了（觉得自己讲不明白），但是写2个简单例子，也算对得起这一节。<br>下图是awk正则表达式：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1911/awk_re.jpg" alt><br>awk的正则表达式写在2个 / 之间：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; ls -l | awk <span class="string">'/^d/&#123;print $9&#125;'</span></span><br></pre></td></tr></table></figure><p>这个例子是输出当前目录下所有的文件夹的名称（不写结果了，可自行尝试看结果）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; awk -F <span class="string">':'</span> <span class="string">'$5~/root/&#123;print $0&#125;'</span> /etc/passwd</span><br></pre></td></tr></table></figure><p>这个例子是以分号作为分隔符，匹配第五个字段包含‘root’的行</p><h1 id="多文件操作及awk运行shell命令"><a href="#多文件操作及awk运行shell命令" class="headerlink" title="多文件操作及awk运行shell命令"></a>多文件操作及awk运行shell命令</h1><h2 id="awk-多文件操作"><a href="#awk-多文件操作" class="headerlink" title="awk 多文件操作"></a>awk 多文件操作</h2><p>文章开始的时候就说过，awk的基本像是如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;命令&#125;'</span> file1, file2, ...</span><br></pre></td></tr></table></figure><p>实际上awk是按文件循序逐行读取的，读完file1， 接着读取file2，以此类推.<br>多文件处理的一个常用操作就是多文件特定内容的merge。下面用2个例子演示一下，横向合并2个文件和根据某一列或者多列合并2个文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cat awk_test.txt</span><br><span class="line">a,b,c,d</span><br><span class="line">e,f,g,h</span><br><span class="line"></span><br><span class="line">&gt;&gt; cat awk_test1.txt</span><br><span class="line">2,3,4,a</span><br><span class="line">1,2,3,e</span><br><span class="line"></span><br><span class="line"><span class="comment"># 横向合并2个文件</span></span><br><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'NR==FNR&#123;a[NR]=$0;next&#125;&#123;if (a[FNR]) print a[FNR] FS $0&#125;'</span> awk_test.txt awk_test1.txt <span class="comment"># 实例7</span></span><br><span class="line">a,b,c,d,2,3,4,a</span><br><span class="line">e,f,g,h,1,2,3,e</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据某一列合并2个文件</span></span><br><span class="line">&gt;&gt; awk -F <span class="string">','</span> <span class="string">'NR==FNR&#123;a[$1]=$0;next&#125;&#123;if (a[$4]) print a[$4] FS $1 FS $2 FS $3&#125;'</span> awk_test.txt awk_test1.txt <span class="comment"># 实例8</span></span><br><span class="line">a,b,c,d,2,3,4</span><br><span class="line">e,f,g,h,1,2,3</span><br></pre></td></tr></table></figure><p>这2个例子中的简单命令直接实现了Pandas的merge、concat和join操作，关键是你可以不用安装任何包，配置任何环境。更可贵的是，速度非常快。<br><strong>通常，使用 NR==FNR 条件判断是否正在读取的在第一个文件。</strong>在实例7中，记录行数为索引，将awk_test.txt文件的数据存在数组里，当读到第二个文件，对于每一个记录行，先打印数组里存的第一个文件的数据，再打印第二个文件的内容，就实现了横向拼接。实例8原理基本相同。</p><h2 id="awk-运行shell命令"><a href="#awk-运行shell命令" class="headerlink" title="awk 运行shell命令"></a>awk 运行shell命令</h2><p>使用awk运行shell命令的基本格式是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;system("shell 命令")&#125;'</span></span><br></pre></td></tr></table></figure><p>需要注意一点的是system里的shell命令需要写成字符串，即写在双引号里。<br>来一个最简单的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; awk <span class="string">'&#123;system("echo hello world")&#125;'</span> awk_test.txt</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><p>可引导echo 命令运行成功。<br>接下来再看一个很实用的昨天，批量修改文件名.下面这个例子给除了black_test和已有后缀以外的文件名后面加上相同的后缀.txt：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; ls <span class="comment"># 查看文件或者文件夹名称</span></span><br><span class="line">test0 test1 test2 test3 test4.txt black_test</span><br><span class="line"></span><br><span class="line">&gt;&gt; ls | grep -vE <span class="string">"txt|black"</span> | awk <span class="string">'&#123;system("mv " $0 " " $0 ".txt")&#125;'</span> <span class="comment"># 方法1</span></span><br><span class="line">&gt;&gt; ls</span><br><span class="line">test0.txt test1.txt test2.txt test3.txt test4.txt black_test</span><br><span class="line"></span><br><span class="line">&gt;&gt; ls | egrep -v <span class="string">"txt|black"</span> | awk <span class="string">'&#123;system("mv " $0 " " $0 ".txt")&#125;'</span> <span class="comment"># 方法2</span></span><br><span class="line">&gt;&gt; ls</span><br><span class="line">test0.txt test1.txt test2.txt test3.txt test4.txt black_test</span><br><span class="line"></span><br><span class="line">&gt;&gt; ls | awk <span class="string">'$0 !~/txt|black/ &#123;system("mv " $0 " " $0 ".txt")&#125;'</span> <span class="comment"># 方法3</span></span><br><span class="line">&gt;&gt; ls</span><br><span class="line">test0.txt test1.txt test2.txt test3.txt test4.txt black_test</span><br></pre></td></tr></table></figure><p>这里涉及到grep的知识，列在下面：</p><ul><li><p>grep -E 用来扩展选项为正则表达式，如果使用了grep 命令的选项-E，则应该使用 | 来分割多个pattern</p></li><li><p>grep -v  实现反向操作。例如方法1中命令 ls | grep -vE “txt|black” 实现了搜索名称中既不包含txt也不包含black的文件。</p></li><li><p>egrep 等同于‘grep -E’</p><p>看了上面的grep的简介，方法1和方法2实际上是一样的。再来看后面的 “mv “ $0 “ “ $0 “.txt” 部分。这里假如$0 = “a”, 那么”mv “ $0 “ “ $0 “.txt”=”mv a a.txt”.这里需要注意的是空格，一定要记得打上。</p><p>再来看方法3，既然awk也有正则匹配， 那就完全可以省去grep，直接用awk的正则去筛选满足条件的文件名。命令 ‘$0 !~/txt|black/‘ 即实现了相同的效果。可以看一下上一节的规则。</p></li></ul><p>到这里，over！有不对的地方，欢迎提出来。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;awk 其名称的由来很简单，就是由其三个创始人Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母组成。很多人都说它是一个效率神器，以前也多多少少用过它一些简单的用法。后来python一上手，pandas一用上，反而忘了它。正所谓技多不压身，这么高大上的程序语言（速度快，常用命令简单， 效率高），一定要学会它。所以这篇文章特将awk的基本语法，常用的命令以及相对应的命令记录下来，以备需要时直接复制使用。
    
    </summary>
    
    
      <category term="Code Tool" scheme="https://jkknotes.com/categories/Code-Tool/"/>
    
      <category term="命令速查" scheme="https://jkknotes.com/categories/Code-Tool/%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/"/>
    
    
      <category term="Tool" scheme="https://jkknotes.com/tags/Tool/"/>
    
      <category term="awk" scheme="https://jkknotes.com/tags/awk/"/>
    
  </entry>
  
  <entry>
    <title>Pandas那些年遇到的坑</title>
    <link href="https://jkknotes.com/pandas_%E9%82%A3%E4%BA%9B%E5%B9%B4%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>https://jkknotes.com/pandas_那些年遇到的坑/</id>
    <published>2019-11-02T09:03:09.000Z</published>
    <updated>2020-01-16T15:31:03.020Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><strong>在进行数据处理和分析时，pandas就像一条高速公路，能够帮助我们快速的进行各种数据处理和分析操作。但是高速公路也可能有各种坑，一不小心就翻车。</strong></p><p>在平时的工作中，也积累了pandas处理的各种坑，记录下来，跟大家分享一下。<a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h1 id="Pandas-IO中的坑"><a href="#Pandas-IO中的坑" class="headerlink" title="Pandas IO中的坑"></a>Pandas IO中的坑</h1><p>先从pandas的读写操作写起。使用pandas读写CSV文件的最常见的操作，即使这个最简单的操作，就很有可能掉入坑里。</p><h2 id="解决读的坑，让pandas读文件内存占用减小-80"><a href="#解决读的坑，让pandas读文件内存占用减小-80" class="headerlink" title="解决读的坑，让pandas读文件内存占用减小 80%"></a>解决读的坑，让pandas读文件内存占用减小 80%</h2><p><strong>资源总是有限的， 僧多肉少是常见的</strong></p><p>而我一次在公司的机器学习平台申请到5G内存，需要打开的csv文件只有900M，当你信心满满的使用 pandas.read_csv 去读取文件，意想不到的是<strong>内存爆了， 内存爆了，内存爆了！！！</strong><br>于是乎，就去学习了一下pandas在内存中存数据的方式，并且找到了解决方式，并很好的填了这个坑。</p><p>一般来说，用pandas处理小于100M的数据，性能不是问题。当用pandas来处理几百兆甚至几个G的数据时，将会比较耗时，同时会导致程序因内存不足而运行失败。那么怎么就解决这个问题呢，我们先来讨论一下pandas的内存使用。</p><p>如下表所示，pandas共有6种大的数据类型，在底层pandas会按照数据类型将列分组形成数据块（blocks）， 相同数据类型的列会合到一起存储。实际上，对于整型和浮点型数据，pandas将它们以 NumPy ndarray 的形式存储。</p><p>从表中可以看到，不同的存储方式所占用的内存不同。其中类型为category的数据在底层使用整型数值来表示该列的值，而不是用原值。当我们把一列转换成category类型时，pandas会用一种最省空间的int子类型去表示这一列中所有的唯一值。当一列只包含有限种值时，这种设计是很不错的。</p><p>了解到这里，<strong><em>我们是不是可以将占用内存多的数据类型转为占用内存低的数据类型，以到达减小内存的占用的目的。</em></strong></p><table><thead><tr><th>memory usage</th><th>float</th><th>int</th><th>unit</th><th>category</th><th>bool</th><th>object</th></tr></thead><tbody><tr><td>1 bytes</td><td></td><td>int8</td><td>unit8</td><td></td><td></td><td></td></tr><tr><td>2 bytes</td><td>float16</td><td>int16</td><td>unit16</td><td></td><td></td><td></td></tr><tr><td>4 bytes</td><td>float32</td><td>int32</td><td>unit32</td><td></td><td></td><td></td></tr><tr><td>8 bytes</td><td>float64</td><td>int64</td><td>unit64</td><td></td><td></td><td></td></tr><tr><td>variable</td><td>Slytherin</td><td></td><td></td><td>category</td><td>bool</td><td>object</td></tr></tbody></table><p>随便找了个数据,实际操作看一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">'game_logs.csv'</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><pre><code>/Users/kk_j/anaconda3/envs/python2_for_project/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (12,13,14,15,19,20,81,83,85,87,93,94,95,96,97,98,99,100,105,106,108,109,111,112,114,115,117,118,120,121,123,124,126,127,129,130,132,133,135,136,138,139,141,142,144,145,147,148,150,151,153,154,156,157,160) have mixed types. Specify dtype option on import or set low_memory=False.  interactivity=interactivity, compiler=compiler, result=result)</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>date</th>      <th>number_of_game</th>      <th>day_of_week</th>      <th>v_name</th>      <th>v_league</th>      <th>v_game_number</th>      <th>h_name</th>      <th>h_league</th>      <th>h_game_number</th>      <th>v_score</th>      <th>...</th>      <th>h_player_7_name</th>      <th>h_player_7_def_pos</th>      <th>h_player_8_id</th>      <th>h_player_8_name</th>      <th>h_player_8_def_pos</th>      <th>h_player_9_id</th>      <th>h_player_9_name</th>      <th>h_player_9_def_pos</th>      <th>additional_info</th>      <th>acquisition_info</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>18710504</td>      <td>0</td>      <td>Thu</td>      <td>CL1</td>      <td>na</td>      <td>1</td>      <td>FW1</td>      <td>na</td>      <td>1</td>      <td>0</td>      <td>...</td>      <td>Ed Mincher</td>      <td>7.0</td>      <td>mcdej101</td>      <td>James McDermott</td>      <td>8.0</td>      <td>kellb105</td>      <td>Bill Kelly</td>      <td>9.0</td>      <td>NaN</td>      <td>Y</td>    </tr>    <tr>      <th>1</th>      <td>18710505</td>      <td>0</td>      <td>Fri</td>      <td>BS1</td>      <td>na</td>      <td>1</td>      <td>WS3</td>      <td>na</td>      <td>1</td>      <td>20</td>      <td>...</td>      <td>Asa Brainard</td>      <td>1.0</td>      <td>burrh101</td>      <td>Henry Burroughs</td>      <td>9.0</td>      <td>berth101</td>      <td>Henry Berthrong</td>      <td>8.0</td>      <td>HTBF</td>      <td>Y</td>    </tr>    <tr>      <th>2</th>      <td>18710506</td>      <td>0</td>      <td>Sat</td>      <td>CL1</td>      <td>na</td>      <td>2</td>      <td>RC1</td>      <td>na</td>      <td>1</td>      <td>12</td>      <td>...</td>      <td>Pony Sager</td>      <td>6.0</td>      <td>birdg101</td>      <td>George Bird</td>      <td>7.0</td>      <td>stirg101</td>      <td>Gat Stires</td>      <td>9.0</td>      <td>NaN</td>      <td>Y</td>    </tr>    <tr>      <th>3</th>      <td>18710508</td>      <td>0</td>      <td>Mon</td>      <td>CL1</td>      <td>na</td>      <td>3</td>      <td>CH1</td>      <td>na</td>      <td>1</td>      <td>12</td>      <td>...</td>      <td>Ed Duffy</td>      <td>6.0</td>      <td>pinke101</td>      <td>Ed Pinkham</td>      <td>5.0</td>      <td>zettg101</td>      <td>George Zettlein</td>      <td>1.0</td>      <td>NaN</td>      <td>Y</td>    </tr>    <tr>      <th>4</th>      <td>18710509</td>      <td>0</td>      <td>Tue</td>      <td>BS1</td>      <td>na</td>      <td>2</td>      <td>TRO</td>      <td>na</td>      <td>1</td>      <td>9</td>      <td>...</td>      <td>Steve Bellan</td>      <td>5.0</td>      <td>pikel101</td>      <td>Lip Pike</td>      <td>3.0</td>      <td>cravb101</td>      <td>Bill Craver</td>      <td>6.0</td>      <td>HTBF</td>      <td>Y</td>    </tr>  </tbody></table><p>5 rows × 161 columns</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.info(memory_usage=<span class="string">'deep'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 171907 entries, 0 to 171906Columns: 161 entries, date to acquisition_infodtypes: float64(77), int64(6), object(78)memory usage: 738.1 MB</code></pre><p><strong><em>可以看到这个数据占用内存738.1M，而文件原来的大小仅仅128M，内存占用是原文件大小的 6 倍！！！</em></strong></p><p>再来尝试一下在打开文件的时候指定列的类型，将数据类型为object的列变成category的数据类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">object_cols = data.select_dtypes(include=[<span class="string">'object'</span>]).columns.tolist()</span><br><span class="line">dtype_list = [<span class="string">'category'</span> <span class="keyword">for</span> x <span class="keyword">in</span> object_cols]</span><br><span class="line">cols_dtype_dict = dict(zip(object_cols, dtype_list))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data1 = pd.read_csv(<span class="string">'game_logs.csv'</span>, dtype=cols_dtype_dict, date_parser=[<span class="string">'date'</span>], infer_datetime_format=<span class="literal">True</span>)</span><br><span class="line">data1.info(memory_usage=<span class="string">'deep'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 171907 entries, 0 to 171906Columns: 161 entries, date to acquisition_infodtypes: category(78), float64(77), int64(6)memory usage: 157.2 MB</code></pre><p>可以看到，<strong><em>内存占用从 738.1M 降到了157.2M，有效降低 78.7%</em></strong>， 而且那一堆Warning 也没了</p><p>很开心对不对，<strong>没有资源，咱自己创造资源</strong><br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly90aW1nc2EuYmFpZHUuY29tL3RpbWc_aW1hZ2UmcXVhbGl0eT04MCZzaXplPWI5OTk5XzEwMDAwJnNlYz0xNTU0NjM2NjI5MjM4JmRpPWI3N2Q3MWU2MzVmOGVkMmU3ZmE0MGM4NDdhYzFiODkzJmltZ3R5cGU9MCZzcmM9aHR0cDovL2Itc3NsLmR1aXRhbmcuY29tL3VwbG9hZHMvaXRlbS8yMDE3MDMvMjkvMjAxNzAzMjkxNjE3MjhfZmRTTUYudGh1bWIuMjI0XzAuZ2lm" alt=""></p><h2 id="解决写的坑，让磁盘空间节约60"><a href="#解决写的坑，让磁盘空间节约60" class="headerlink" title="解决写的坑，让磁盘空间节约60%"></a>解决写的坑，让磁盘空间节约60%</h2><p>经常听见有小伙伴说，XXXX服务器磁盘空间又满了，大家清理一下自己不用的数据，数据很重要，不能删怎么办。</p><p>还是那句话，<strong>没有资源，咱创造资源</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data1.to_csv(<span class="string">'game_logs.gz'</span>, compression=<span class="string">'gzip'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>去磁盘再去看看文件大小，是不是磁盘变大了。错了，是不是文件变小了。</p><p><strong><em>在我的电脑里，这个文件从 128M 减小到18M。我去，磁盘占用减小了86%</em></strong></p><p>那读取的时候怎么办呢，读取方式不变，还是 read_csv </p><h2 id="解决写的坑，避免挖个坑"><a href="#解决写的坑，避免挖个坑" class="headerlink" title="解决写的坑，避免挖个坑"></a>解决写的坑，避免挖个坑</h2><p>这个坑比较简单，但是一不小心就翻车。看个例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(np.random.rand(<span class="number">2</span>,<span class="number">2</span>), columns=[<span class="string">'a'</span>, <span class="string">'b'</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>a</th>      <th>b</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.977292</td>      <td>0.343893</td>    </tr>    <tr>      <th>1</th>      <td>0.478050</td>      <td>0.781146</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_csv(<span class="string">'test_df.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1 = pd.read_csv(<span class="string">'test_df.csv'</span>)</span><br><span class="line">df1</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Unnamed: 0</th>      <th>a</th>      <th>b</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>0.977292</td>      <td>0.343893</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>0.478050</td>      <td>0.781146</td>    </tr>  </tbody></table></div><p>通过以上例子，可以看到，<strong><em>一存一读间，却多了一列。</em></strong><br>这种情况极易给后面的操作埋下一个大坑，而且还蒙在鼓里找不出原因。</p><p>怎么解决呢，只需要在存的时候，指定 index 参数为 False 即可。再来试一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.to_csv(<span class="string">'test_df.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df = pd.read_csv(<span class="string">'test_df.csv'</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>a</th>      <th>b</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.977292</td>      <td>0.343893</td>    </tr>    <tr>      <th>1</th>      <td>0.478050</td>      <td>0.781146</td>    </tr>  </tbody></table></div><h2 id="python2：加上encoding，-读写好习惯"><a href="#python2：加上encoding，-读写好习惯" class="headerlink" title="python2：加上encoding， 读写好习惯"></a>python2：加上encoding， 读写好习惯</h2><p>这个就不举例子讲了。但是讲一下原因。</p><p>在工作中经常处理带中文字符的csv文件，一个好的习惯是，在使用pandas的read_csv（其他的read操作一样）进行文件读取时，<strong><em>加上参数 encoding=‘utf-8’</em></strong>，并且在数据的操作中都始终使用utf-8的编码格式，会减少非常多的坑。另外在使用 .to_csv 存储带有中文字符的DataFram数据时，加上参数 <strong><em>encoding=‘utf-8-sig’</em></strong>，这样存成的csv就可以用excel打开，而不乱码。</p><p>关于编码知识，可以看这里：<a href="https://blog.csdn.net/u010223750/article/details/56684096/" target="_blank" rel="noopener">https://blog.csdn.net/u010223750/article/details/56684096/</a></p><h2 id="乱入：用pandas进行onehot的神坑"><a href="#乱入：用pandas进行onehot的神坑" class="headerlink" title="乱入：用pandas进行onehot的神坑"></a>乱入：用pandas进行onehot的神坑</h2><p>机器学习特征工程中，经常会用到one-hot编码。并且pandas中已经提供了这一函数pandas.get_dummies()。<br><strong><em>但是使用这个函数进行one hot操作后得到的数据类型竟然是是uint8，如果进行数值计算时会溢出。</em></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_df = pd.DataFrame(&#123;<span class="string">'sex'</span>: [<span class="string">'male'</span>, <span class="string">'female'</span>, <span class="string">'female'</span>, <span class="string">'female'</span>, <span class="string">'female'</span>, <span class="string">'male'</span>, <span class="string">'female'</span>],</span><br><span class="line">                        <span class="string">'height'</span>: [<span class="number">182</span>, <span class="number">160</span>, <span class="number">176</span>, <span class="number">172</span>, <span class="number">174</span>, <span class="number">170</span>, <span class="number">155</span>],</span><br><span class="line">                        <span class="string">'weight'</span>: [<span class="number">65</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">48</span>, <span class="number">48</span>, <span class="number">100</span>, <span class="number">80</span>],</span><br><span class="line">                        <span class="string">'is_air_hostesses'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]&#125;)</span><br><span class="line">data_df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>height</th>      <th>is_air_hostesses</th>      <th>sex</th>      <th>weight</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>182</td>      <td>1</td>      <td>male</td>      <td>65</td>    </tr>    <tr>      <th>1</th>      <td>160</td>      <td>1</td>      <td>female</td>      <td>50</td>    </tr>    <tr>      <th>2</th>      <td>176</td>      <td>1</td>      <td>female</td>      <td>55</td>    </tr>    <tr>      <th>3</th>      <td>172</td>      <td>1</td>      <td>female</td>      <td>48</td>    </tr>    <tr>      <th>4</th>      <td>174</td>      <td>1</td>      <td>female</td>      <td>48</td>    </tr>    <tr>      <th>5</th>      <td>170</td>      <td>0</td>      <td>male</td>      <td>100</td>    </tr>    <tr>      <th>6</th>      <td>155</td>      <td>0</td>      <td>female</td>      <td>80</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sex_one_hot_df = pd.get_dummies(data_df[<span class="string">'sex'</span>])</span><br><span class="line">sex_one_hot_df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>female</th>      <th>male</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>5</th>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>6</th>      <td>1</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sex_one_hot_df.dtypes</span><br></pre></td></tr></table></figure><pre><code>female    uint8male      uint8dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-sex_one_hot_df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>female</th>      <th>male</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>255</td>    </tr>    <tr>      <th>1</th>      <td>255</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>255</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>255</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>255</td>      <td>0</td>    </tr>    <tr>      <th>5</th>      <td>0</td>      <td>255</td>    </tr>    <tr>      <th>6</th>      <td>255</td>      <td>0</td>    </tr>  </tbody></table></div><p>这真的是一个神坑，如果特征比较多的话，根本发现不了。如果没有发现，后续如果做其他操的时候，就会出错。这个坑藏得深啊。</p><p>正确的做法是转换一下数据类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sex_one_hot_df = sex_one_hot_df.astype(<span class="string">'float'</span>)</span><br></pre></td></tr></table></figure><h1 id="DataFrame-链式索引的坑"><a href="#DataFrame-链式索引的坑" class="headerlink" title="DataFrame 链式索引的坑"></a>DataFrame 链式索引的坑</h1><h2 id="解决：SettingWithCopyWarning"><a href="#解决：SettingWithCopyWarning" class="headerlink" title="解决：SettingWithCopyWarning:"></a>解决：SettingWithCopyWarning:</h2><p><strong>SettingWithCopyWarning</strong> 可能是人们在学习 Pandas 时遇到的最常见的障碍之一。<br>首先来看看，它出现的情况之一（其他情况大同小异）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sub_df = df.loc[df.a &gt; <span class="number">0.6</span>]</span><br><span class="line">sub_df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>a</th>      <th>b</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.688818</td>      <td>0.510446</td>    </tr>    <tr>      <th>4</th>      <td>0.945565</td>      <td>0.801788</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sub_df[<span class="string">'c'</span>] = [<span class="number">1</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure><pre><code>/Users/kk_j/anaconda3/envs/python2_for_project/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_indexer,col_indexer] = value insteadSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy  &quot;&quot;&quot;Entry point for launching an IPython kernel.</code></pre><p>没有出任何意外，SettingWithCopyWarning 出现。首先要理解的是，SettingWithCopyWarning 是一个警告 Warning，而不是错误 Error，它告诉你，你的操作可能没有按预期运行，需要检查结果以确保没有出错。<strong><em>当你查看结果，发现结果没有错，就是在按预期进行，你极有可能忽略这个Warning, 而当下次它再次出现时，你不会再检查，然后错误就出现了。</em></strong></p><p>直接说他出现的原因，那就是<strong><em>链式索引产生的新的变量并没有在内存中创建副本，当接下来对新的变量进行修改时，有修改原数据的风险。</em></strong></p><p>怎么解决呢。很简单,只需要在链式索引后面加上一个.copy() 即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sub_df = df.loc[df.a &gt; <span class="number">0.6</span>].copy()</span><br><span class="line">sub_df[<span class="string">'c'</span>] = [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">sub_df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>a</th>      <th>b</th>      <th>c</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.688818</td>      <td>0.510446</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>0.945565</td>      <td>0.801788</td>      <td>2</td>    </tr>  </tbody></table></div><p>再试试，可以看到没有再出现问题。</p><p>但是我们也注意到，在Warning的提示里，提到：<strong>Try using .loc[row_indexer,col_indexer] = value instead</strong>。这也是一种解决办法，当你仅仅是想更改原始数据，你可以使用这个操作。</p><p>对这个问题的详细原理讲解，请参考： <a href="https://www.dataquest.io/blog/settingwithcopywarning/" target="_blank" rel="noopener">https://www.dataquest.io/blog/settingwithcopywarning/</a></p><h2 id="DataFrame-里存None：这个坑是真的坑"><a href="#DataFrame-里存None：这个坑是真的坑" class="headerlink" title="DataFrame 里存None：这个坑是真的坑"></a>DataFrame 里存None：这个坑是真的坑</h2><p>真的不好写开场白，直接上例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">v = &#123;<span class="string">'value'</span>: <span class="string">'a'</span>&#125;</span><br><span class="line">d = [&#123;<span class="string">'name'</span>: <span class="string">'class'</span>, <span class="string">'age'</span>: <span class="number">10</span>&#125;, &#123;<span class="string">'name'</span>: <span class="literal">None</span>, <span class="string">'age'</span>: <span class="number">11</span>&#125;, &#123;<span class="string">'name'</span>: <span class="string">'def'</span>, <span class="string">'age'</span>: <span class="number">9</span>&#125;]</span><br><span class="line">df = pd.DataFrame(d)</span><br><span class="line"></span><br><span class="line">new_1 = df[(df[<span class="string">'age'</span>] &gt;= <span class="number">10</span>) | df[<span class="string">'name'</span>].str.contains(v[<span class="string">'value'</span>])]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 颠倒里面条件的顺序</span></span><br><span class="line">new_2 = df[df[<span class="string">'name'</span>].str.contains(v[<span class="string">'value'</span>]) | (df[<span class="string">'age'</span>] &gt;= <span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">40</span>)</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">40</span>)</span><br><span class="line">print(new_1)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">40</span>)</span><br><span class="line">print(new_2)</span><br></pre></td></tr></table></figure><pre><code>----------------------------------------   age   name0   10  class1   11   None2    9    def----------------------------------------   age   name0   10  class1   11   None----------------------------------------   age   name0   10  class</code></pre><p>这。。。。。逻辑操作“或”俩边的条件对调下，结果也能不一样？一脸懵逼。</p><p>但是接下来，我进行了简单的探索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'age'</span>] &gt;= <span class="number">10</span></span><br></pre></td></tr></table></figure><pre><code>0     True1     True2    FalseName: age, dtype: bool</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'name'</span>].str.contains(v[<span class="string">'value'</span>])</span><br></pre></td></tr></table></figure><pre><code>0     True1     None2    FalseName: name, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(df[<span class="string">'age'</span>] &gt;= <span class="number">10</span>) | df[<span class="string">'name'</span>].str.contains(v[<span class="string">'value'</span>])</span><br></pre></td></tr></table></figure><pre><code>0     True1     True2    Falsedtype: bool</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'name'</span>].str.contains(v[<span class="string">'value'</span>]) | (df[<span class="string">'age'</span>] &gt;= <span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>0     True1    False2    Falsedtype: bool</code></pre><p>这。。。。。。还是一脸懵逼。</p><p><strong><em>百度了一圈，还是没有找到答案。但是找到了解决了办法：<br>把 None 改为了 ‘’ 就可以了。</em></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">v = &#123;<span class="string">'value'</span>: <span class="string">'a'</span>&#125;</span><br><span class="line">d = [&#123;<span class="string">'name'</span>: <span class="string">'class'</span>, <span class="string">'age'</span>: <span class="number">10</span>&#125;, &#123;<span class="string">'name'</span>: <span class="string">''</span>, <span class="string">'age'</span>: <span class="number">11</span>&#125;, &#123;<span class="string">'name'</span>: <span class="string">'def'</span>, <span class="string">'age'</span>: <span class="number">9</span>&#125;]</span><br><span class="line">df = pd.DataFrame(d)</span><br><span class="line"></span><br><span class="line">new_1 = df[(df[<span class="string">'age'</span>] &gt;= <span class="number">10</span>) | df[<span class="string">'name'</span>].str.contains(v[<span class="string">'value'</span>])]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 颠倒里面条件的顺序</span></span><br><span class="line">new_2 = df[df[<span class="string">'name'</span>].str.contains(v[<span class="string">'value'</span>]) | (df[<span class="string">'age'</span>] &gt;= <span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">40</span>)</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">40</span>)</span><br><span class="line">print(new_1)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">40</span>)</span><br><span class="line">print(new_2)</span><br></pre></td></tr></table></figure><pre><code>----------------------------------------   age   name0   10  class1   11       2    9    def----------------------------------------   age   name0   10  class1   11       ----------------------------------------   age   name0   10  class1   11       </code></pre><h2 id="这个坑不算坑"><a href="#这个坑不算坑" class="headerlink" title="这个坑不算坑"></a>这个坑不算坑</h2><p>这里就举个例子，自己体会：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> type(df[<span class="string">'age'</span>])</span><br><span class="line">df[<span class="string">'age'</span>]</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.series.Series&apos;&gt;0    101    112     9Name: age, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> type(df[[<span class="string">'age'</span>]])</span><br><span class="line">df[[<span class="string">'age'</span>]]</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>age</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>10</td>    </tr>    <tr>      <th>1</th>      <td>11</td>    </tr>    <tr>      <th>2</th>      <td>9</td>    </tr>  </tbody></table></div><p><strong><em>前面是Series后面是DataFrame，这不知道算不算一个坑</em></strong></p><h1 id="DataFrame-拼接里面的坑与技巧"><a href="#DataFrame-拼接里面的坑与技巧" class="headerlink" title="DataFrame 拼接里面的坑与技巧"></a>DataFrame 拼接里面的坑与技巧</h1><p>pandas 里多个DataFrame的拼接，主要是append， merge，concat，join四个函数。想详细了解的话看一下官方文档。</p><p><strong><em>这里简单说一下concat和merge.</em></strong></p><h2 id="concat：坑虽小，须谨慎"><a href="#concat：坑虽小，须谨慎" class="headerlink" title="concat：坑虽小，须谨慎"></a>concat：坑虽小，须谨慎</h2><p>解释这个坑，也只有靠例子。直接上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">df1 = pd.DataFrame(&#123; <span class="string">'A'</span>: [<span class="string">'A0'</span>, <span class="string">'A1'</span>, <span class="string">'A2'</span>],</span><br><span class="line">                     <span class="string">'B'</span>: [<span class="string">'B0'</span>, <span class="string">'B1'</span>, <span class="string">'B2'</span>],</span><br><span class="line">                     <span class="string">'C'</span>: [<span class="string">'C0'</span>, <span class="string">'C1'</span>, <span class="string">'C2'</span>],</span><br><span class="line">                     <span class="string">'D'</span>: [<span class="string">'D0'</span>, <span class="string">'D1'</span>, <span class="string">'D2'</span>]&#125;)</span><br><span class="line">                     </span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'A'</span>: [<span class="string">'A4'</span>, <span class="string">'A5'</span>, <span class="string">'A6'</span>],</span><br><span class="line">                     <span class="string">'B'</span>: [<span class="string">'B4'</span>, <span class="string">'B5'</span>, <span class="string">'B6'</span>],</span><br><span class="line">                     <span class="string">'C'</span>: [<span class="string">'C4'</span>, <span class="string">'C5'</span>, <span class="string">'C6'</span>],</span><br><span class="line">                     <span class="string">'D'</span>: [<span class="string">'D4'</span>, <span class="string">'D5'</span>, <span class="string">'D6'</span>]&#125;)</span><br><span class="line">                     </span><br><span class="line">df3 = pd.DataFrame(&#123;<span class="string">'A'</span>: [<span class="string">'A8'</span>, <span class="string">'A9'</span>, <span class="string">'A10'</span>],</span><br><span class="line">                     <span class="string">'B'</span>: [<span class="string">'B8'</span>, <span class="string">'B9'</span>, <span class="string">'B10'</span>],</span><br><span class="line">                     <span class="string">'C'</span>: [<span class="string">'C8'</span>, <span class="string">'C9'</span>, <span class="string">'C10'</span>],</span><br><span class="line">                     <span class="string">'D'</span>: [<span class="string">'D8'</span>, <span class="string">'D9'</span>, <span class="string">'D10'</span>]&#125;)</span><br><span class="line"> </span><br><span class="line">frames = [df1, df2, df3]</span><br><span class="line">result = pd.concat(frames)</span><br><span class="line">result</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>A0</td>      <td>B0</td>      <td>C0</td>      <td>D0</td>    </tr>    <tr>      <th>1</th>      <td>A1</td>      <td>B1</td>      <td>C1</td>      <td>D1</td>    </tr>    <tr>      <th>2</th>      <td>A2</td>      <td>B2</td>      <td>C2</td>      <td>D2</td>    </tr>    <tr>      <th>0</th>      <td>A4</td>      <td>B4</td>      <td>C4</td>      <td>D4</td>    </tr>    <tr>      <th>1</th>      <td>A5</td>      <td>B5</td>      <td>C5</td>      <td>D5</td>    </tr>    <tr>      <th>2</th>      <td>A6</td>      <td>B6</td>      <td>C6</td>      <td>D6</td>    </tr>    <tr>      <th>0</th>      <td>A8</td>      <td>B8</td>      <td>C8</td>      <td>D8</td>    </tr>    <tr>      <th>1</th>      <td>A9</td>      <td>B9</td>      <td>C9</td>      <td>D9</td>    </tr>    <tr>      <th>2</th>      <td>A10</td>      <td>B10</td>      <td>C10</td>      <td>D10</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df4 = pd.DataFrame(&#123;<span class="string">'val'</span>:[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],<span class="string">'A'</span>: [<span class="string">'A0'</span>, <span class="string">'A1'</span>, <span class="string">'A2'</span>, <span class="string">'A3'</span>,<span class="string">'A4'</span>, <span class="string">'A5'</span>, <span class="string">'A6'</span>, <span class="string">'A7'</span>,<span class="string">'A8'</span>]&#125;)</span><br><span class="line">result[<span class="string">'val'</span>] = df4[<span class="string">'A'</span>]</span><br><span class="line">result</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>val</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>A0</td>      <td>B0</td>      <td>C0</td>      <td>D0</td>      <td>A0</td>    </tr>    <tr>      <th>1</th>      <td>A1</td>      <td>B1</td>      <td>C1</td>      <td>D1</td>      <td>A1</td>    </tr>    <tr>      <th>2</th>      <td>A2</td>      <td>B2</td>      <td>C2</td>      <td>D2</td>      <td>A2</td>    </tr>    <tr>      <th>0</th>      <td>A4</td>      <td>B4</td>      <td>C4</td>      <td>D4</td>      <td>A0</td>    </tr>    <tr>      <th>1</th>      <td>A5</td>      <td>B5</td>      <td>C5</td>      <td>D5</td>      <td>A1</td>    </tr>    <tr>      <th>2</th>      <td>A6</td>      <td>B6</td>      <td>C6</td>      <td>D6</td>      <td>A2</td>    </tr>    <tr>      <th>0</th>      <td>A8</td>      <td>B8</td>      <td>C8</td>      <td>D8</td>      <td>A0</td>    </tr>    <tr>      <th>1</th>      <td>A9</td>      <td>B9</td>      <td>C9</td>      <td>D9</td>      <td>A1</td>    </tr>    <tr>      <th>2</th>      <td>A10</td>      <td>B10</td>      <td>C10</td>      <td>D10</td>      <td>A2</td>    </tr>  </tbody></table></div><p><strong><em>注意看最后一列 ‘val’ ，和我们预期（预期的是从 A0-A8 ）的真的不一样。原来赋值操作是按照index赋值的，结果就是这么出乎我们的意料。</em></strong></p><p>其实，concat的时候加上参数 <strong><em>ignore_index=True</em></strong> 就好了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = pd.concat(frames, ignore_index=<span class="literal">True</span>)</span><br><span class="line">result</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>A0</td>      <td>B0</td>      <td>C0</td>      <td>D0</td>    </tr>    <tr>      <th>1</th>      <td>A1</td>      <td>B1</td>      <td>C1</td>      <td>D1</td>    </tr>    <tr>      <th>2</th>      <td>A2</td>      <td>B2</td>      <td>C2</td>      <td>D2</td>    </tr>    <tr>      <th>3</th>      <td>A4</td>      <td>B4</td>      <td>C4</td>      <td>D4</td>    </tr>    <tr>      <th>4</th>      <td>A5</td>      <td>B5</td>      <td>C5</td>      <td>D5</td>    </tr>    <tr>      <th>5</th>      <td>A6</td>      <td>B6</td>      <td>C6</td>      <td>D6</td>    </tr>    <tr>      <th>6</th>      <td>A8</td>      <td>B8</td>      <td>C8</td>      <td>D8</td>    </tr>    <tr>      <th>7</th>      <td>A9</td>      <td>B9</td>      <td>C9</td>      <td>D9</td>    </tr>    <tr>      <th>8</th>      <td>A10</td>      <td>B10</td>      <td>C10</td>      <td>D10</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result[<span class="string">'val'</span>] = df4[<span class="string">'A'</span>]</span><br><span class="line">result</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>val</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>A0</td>      <td>B0</td>      <td>C0</td>      <td>D0</td>      <td>A0</td>    </tr>    <tr>      <th>1</th>      <td>A1</td>      <td>B1</td>      <td>C1</td>      <td>D1</td>      <td>A1</td>    </tr>    <tr>      <th>2</th>      <td>A2</td>      <td>B2</td>      <td>C2</td>      <td>D2</td>      <td>A2</td>    </tr>    <tr>      <th>3</th>      <td>A4</td>      <td>B4</td>      <td>C4</td>      <td>D4</td>      <td>A3</td>    </tr>    <tr>      <th>4</th>      <td>A5</td>      <td>B5</td>      <td>C5</td>      <td>D5</td>      <td>A4</td>    </tr>    <tr>      <th>5</th>      <td>A6</td>      <td>B6</td>      <td>C6</td>      <td>D6</td>      <td>A5</td>    </tr>    <tr>      <th>6</th>      <td>A8</td>      <td>B8</td>      <td>C8</td>      <td>D8</td>      <td>A6</td>    </tr>    <tr>      <th>7</th>      <td>A9</td>      <td>B9</td>      <td>C9</td>      <td>D9</td>      <td>A7</td>    </tr>    <tr>      <th>8</th>      <td>A10</td>      <td>B10</td>      <td>C10</td>      <td>D10</td>      <td>A8</td>    </tr>  </tbody></table></div><h2 id="merge：小众的技巧"><a href="#merge：小众的技巧" class="headerlink" title="merge：小众的技巧"></a>merge：小众的技巧</h2><p><a href="https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.merge.html" target="_blank" rel="noopener">panda.merge</a> 这个是pandas最常用的操作之一，具体用法可以看官方文档。这里有个小的tricks, 在做一些统计分析的时候很有用。还是具体看例子吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">left = pd.DataFrame(&#123;<span class="string">'key'</span>: [<span class="string">'key1'</span>, <span class="string">'key2'</span>, <span class="string">'key3'</span>, <span class="string">'key4'</span>], <span class="string">'val_l'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;)</span><br><span class="line">left</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>key</th>      <th>val_l</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>key1</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>key2</td>      <td>2</td>    </tr>    <tr>      <th>2</th>      <td>key3</td>      <td>3</td>    </tr>    <tr>      <th>3</th>      <td>key4</td>      <td>4</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">right = pd.DataFrame(&#123;<span class="string">'key'</span>: [<span class="string">'key3'</span>, <span class="string">'key2'</span>, <span class="string">'key1'</span>, <span class="string">'key6'</span>], <span class="string">'val_r'</span>: [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>]&#125;)</span><br><span class="line">right</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>key</th>      <th>val_r</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>key3</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>key2</td>      <td>2</td>    </tr>    <tr>      <th>2</th>      <td>key1</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>key6</td>      <td>6</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_merge = pd.merge(left, right, on=<span class="string">'key'</span>, how=<span class="string">'left'</span>, indicator=<span class="literal">True</span>)</span><br><span class="line">df_merge</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>key</th>      <th>val_l</th>      <th>val_r</th>      <th>_merge</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>key1</td>      <td>1</td>      <td>1.0</td>      <td>both</td>    </tr>    <tr>      <th>1</th>      <td>key2</td>      <td>2</td>      <td>2.0</td>      <td>both</td>    </tr>    <tr>      <th>2</th>      <td>key3</td>      <td>3</td>      <td>3.0</td>      <td>both</td>    </tr>    <tr>      <th>3</th>      <td>key4</td>      <td>4</td>      <td>NaN</td>      <td>left_only</td>    </tr>  </tbody></table></div><p><strong>_merge 列不仅可以用来检查是否出现数值错误，还可以进行统计分析，比如：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_merge[<span class="string">'_merge'</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>both          3left_only     1right_only    0Name: _merge, dtype: int64</code></pre><h1 id="一些技巧"><a href="#一些技巧" class="headerlink" title="一些技巧"></a>一些技巧</h1><p>技巧总是讲不完，我这里随便再写点。</p><h2 id="pandas-画图"><a href="#pandas-画图" class="headerlink" title="pandas 画图"></a>pandas 画图</h2><p>这个举个例子就好了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">df_merge.val_l.plot(kind=<span class="string">'bar'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1911/output_74_0.png" alt><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data1.plot(kind=<span class="string">'scatter'</span>, x=<span class="string">'v_game_number'</span>, y=<span class="string">'v_score'</span>, alpha=<span class="number">0.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1911/output_75_0.png" alt><p>plot这个命令底层调用的就是matplotlib。必须事先装好matplotlib，不然会报错。</p><p><strong><em>这里的 2 个例子只是抛砖引玉，真正的功能非常强大，有兴趣的小伙伴可以学习一下</em></strong></p><h2 id="简单的相关性分析"><a href="#简单的相关性分析" class="headerlink" title="简单的相关性分析"></a>简单的相关性分析</h2><p>写到这里，写累了。不想去找数据集，还是用前面自己构造的数据集演示一下这个小技巧：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_df = pd.DataFrame(&#123;<span class="string">'sex'</span>: [<span class="string">'male'</span>, <span class="string">'female'</span>, <span class="string">'female'</span>, <span class="string">'female'</span>, <span class="string">'female'</span>, <span class="string">'male'</span>, <span class="string">'female'</span>],</span><br><span class="line">                        <span class="string">'height'</span>: [<span class="number">182</span>, <span class="number">160</span>, <span class="number">176</span>, <span class="number">172</span>, <span class="number">174</span>, <span class="number">170</span>, <span class="number">155</span>],</span><br><span class="line">                        <span class="string">'weight'</span>: [<span class="number">65</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">48</span>, <span class="number">48</span>, <span class="number">100</span>, <span class="number">80</span>],</span><br><span class="line">                        <span class="string">'is_air_hostesses'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]&#125;)</span><br><span class="line">data_df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>height</th>      <th>is_air_hostesses</th>      <th>sex</th>      <th>weight</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>182</td>      <td>1</td>      <td>male</td>      <td>65</td>    </tr>    <tr>      <th>1</th>      <td>160</td>      <td>1</td>      <td>female</td>      <td>50</td>    </tr>    <tr>      <th>2</th>      <td>176</td>      <td>1</td>      <td>female</td>      <td>55</td>    </tr>    <tr>      <th>3</th>      <td>172</td>      <td>1</td>      <td>female</td>      <td>48</td>    </tr>    <tr>      <th>4</th>      <td>174</td>      <td>1</td>      <td>female</td>      <td>48</td>    </tr>    <tr>      <th>5</th>      <td>170</td>      <td>0</td>      <td>male</td>      <td>100</td>    </tr>    <tr>      <th>6</th>      <td>155</td>      <td>0</td>      <td>female</td>      <td>80</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_df[[<span class="string">'sex'</span>, <span class="string">'is_air_hostesses'</span>]].groupby([<span class="string">'sex'</span>], as_index=<span class="literal">False</span>).mean().sort_values(by=<span class="string">'is_air_hostesses'</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>sex</th>      <th>is_air_hostesses</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>female</td>      <td>0.8</td>    </tr>    <tr>      <th>1</th>      <td>male</td>      <td>0.5</td>    </tr>  </tbody></table></div><p><strong>* 可以看到女生做空乘的可能性更大一些 *</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_df[<span class="string">'height_band'</span>] = pd.qcut(data_df[<span class="string">'height'</span>], <span class="number">2</span>)</span><br><span class="line">data_df</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>height</th>      <th>is_air_hostesses</th>      <th>sex</th>      <th>weight</th>      <th>height_band</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>182</td>      <td>1</td>      <td>male</td>      <td>65</td>      <td>(172.0, 182.0]</td>    </tr>    <tr>      <th>1</th>      <td>160</td>      <td>1</td>      <td>female</td>      <td>50</td>      <td>(154.999, 172.0]</td>    </tr>    <tr>      <th>2</th>      <td>176</td>      <td>1</td>      <td>female</td>      <td>55</td>      <td>(172.0, 182.0]</td>    </tr>    <tr>      <th>3</th>      <td>172</td>      <td>1</td>      <td>female</td>      <td>48</td>      <td>(154.999, 172.0]</td>    </tr>    <tr>      <th>4</th>      <td>174</td>      <td>1</td>      <td>female</td>      <td>48</td>      <td>(172.0, 182.0]</td>    </tr>    <tr>      <th>5</th>      <td>170</td>      <td>0</td>      <td>male</td>      <td>100</td>      <td>(154.999, 172.0]</td>    </tr>    <tr>      <th>6</th>      <td>155</td>      <td>0</td>      <td>female</td>      <td>80</td>      <td>(154.999, 172.0]</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">data_df[[<span class="string">'height_band'</span>, <span class="string">'is_air_hostesses'</span>]].groupby([<span class="string">'height_band'</span>], as_index=<span class="literal">False</span>).mean().sort_values(by=<span class="string">'is_air_hostesses'</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }<pre><code>.dataframe thead th {    text-align: left;}.dataframe tbody tr th {    vertical-align: top;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>height_band</th>      <th>is_air_hostesses</th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>(172.0, 182.0]</td>      <td>1.0</td>    </tr>    <tr>      <th>0</th>      <td>(154.999, 172.0]</td>      <td>0.5</td>    </tr>  </tbody></table></div><p><strong>这里可以看到身高大于172的是空乘的可能性更大一些</strong></p><p>同样的也是为了抛砖引玉，不详细介绍了</p><h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>写pandas的这些坑，只是为了更好的提高工作效率，有兴趣的小伙伴可以学一学，相信会很有帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;在进行数据处理和分析时，pandas就像一条高速公路，能够帮助我们快速的进行各种数据处理和分析操作。但是高速公路也可能有各种坑，一不小心就翻车。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在平时的工作中，也积累了pandas处理的各种坑，记录下来，跟大家分享一下。
    
    </summary>
    
    
      <category term="Code Tool" scheme="https://jkknotes.com/categories/Code-Tool/"/>
    
      <category term="Code 总结" scheme="https://jkknotes.com/categories/Code-Tool/Code-%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Pandas" scheme="https://jkknotes.com/tags/Pandas/"/>
    
      <category term="Tool" scheme="https://jkknotes.com/tags/Tool/"/>
    
  </entry>
  
  <entry>
    <title>Git基本命令大全</title>
    <link href="https://jkknotes.com/Git%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/"/>
    <id>https://jkknotes.com/Git基本命令大全/</id>
    <published>2019-06-04T04:29:29.000Z</published>
    <updated>2020-01-05T09:08:02.399Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Git —— 大名鼎鼎的分布式版本控制系统，据说是Linux系统之父Linus花两周时间写出来的，怎一个牛字了得。<br>写过代码的人大都知道它，以写代码为生的人大都离不开它。<br>官方文档实在太难用，这里参考大佬<a href="https://www.liaoxuefeng.com/wiki/896043488029600" target="_blank" rel="noopener"><strong>廖雪峰的教程</strong></a>，把常用命令总结写两下来，以供自己和一起搬砖的朋友备查。<a id="more"></a></p><p>这里先介绍本地仓库的常用命令，再介绍涉及远程仓库和分支管理的常用命令。本文没有实例介绍讲解，只做命令备查。如需详细教程，请参考<a href="https://www.liaoxuefeng.com/wiki/896043488029600" target="_blank" rel="noopener"><strong>廖雪峰的Git教程</strong></a>。</p><p><strong><em>本文同步发在我的<a href="https://blog.csdn.net/iizhuzhu/article/details/90760926" target="_blank" rel="noopener">CSDN Blog</a>，欢迎各位看官大佬关注指教。</em></strong></p><h2 id="1-基本操作"><a href="#1-基本操作" class="headerlink" title="1.基本操作"></a>1.基本操作</h2><h3 id="创建版本库并添加文件"><a href="#创建版本库并添加文件" class="headerlink" title="创建版本库并添加文件"></a>创建版本库并添加文件</h3><p>进入到需要<strong>创建版本库</strong>的文件夹，执行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p>git的本地库包含三部分：工作区、暂存区（stage）、分支（branch，默认为master），git add命令是将文件从工作区添加到暂存区，git commit命令是将文件从暂存区添加到分支，他们的关系如下图所示：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1906/a2.jpeg" alt><br><strong>添加新文件或修改过的文件</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add file1_name file2_name file3_name ....</span><br><span class="line">git commit -m <span class="string">'这里是对添加文件的说明'</span></span><br></pre></td></tr></table></figure><p><strong>查看版本库的状态</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><h3 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h3><p><strong>创建分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch 分支名称</span><br></pre></td></tr></table></figure><p><strong>切换分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout 分支名称</span><br></pre></td></tr></table></figure><p><strong>创建并切换分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b 分支名称</span><br></pre></td></tr></table></figure><p><strong>删除本地分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git branch -d 分支名称</span><br><span class="line">git branch -D 分支名称 <span class="comment"># 强制删除</span></span><br></pre></td></tr></table></figure><p><strong>删除远程分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push &lt;remote_name&gt; --delete 远程分支名称 <span class="comment"># 大部分情况下&lt;remote_name&gt;为origin</span></span><br></pre></td></tr></table></figure><p><strong>查看所有分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git branch    <span class="comment"># 查看本地分支</span></span><br><span class="line">git branch -a <span class="comment"># 查看远程和本地所有分支</span></span><br><span class="line">git branch -r <span class="comment"># 查看远程分支</span></span><br></pre></td></tr></table></figure><p><strong>拉取远程分支并同时创建对应的本地分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b 本地分支名 origin/远程分支名  <span class="comment"># 如失败，可以先git fetch</span></span><br></pre></td></tr></table></figure><p><strong>合并分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git merge 准备与当前分支合并的分支名称</span><br></pre></td></tr></table></figure><p><strong>若merge出现冲突，解决办法请参考</strong><a href="https://www.liaoxuefeng.com/wiki/896043488029600/900004111093344" target="_blank" rel="noopener"><strong>这里</strong></a></p><p><strong>stash功能</strong><br>当在当前分支编辑内容时，没有add和commit就切换到其它分支，发现其它分支中也会出现相应更改，stash功能可以解决这个问题。如果出现这种情况，正在当前分支编辑，需要临时去其他分支处理一些事情，但是当前分支又不能add和commit，这时就需要stash功能，可以它就像一个icebox，能当前工作冻结。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash</span><br></pre></td></tr></table></figure><p>查看冻结列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash list</span><br></pre></td></tr></table></figure><p>想解冻2种方法：<br>第一种：解冻的同时把stash记录也删了，也就是在list中看不到了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash pop</span><br></pre></td></tr></table></figure><p>第二种：解冻不删list中记录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash apply</span><br></pre></td></tr></table></figure><p>想删记录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash drop</span><br></pre></td></tr></table></figure><h2 id="2-时光穿梭"><a href="#2-时光穿梭" class="headerlink" title="2.时光穿梭"></a>2.时光穿梭</h2><h3 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h3><p>当不断对文件进行修改，然后不断提交修改到版本库里，难免会出现失误，把文件改乱或出现其他错误，就需要回到上一次或者之前任一次提交的状态。<br>首先<strong>查看提交历史记录</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></table></figure><p>返回结果中commit 后面的十六进制数即为该次提交的commit id。<br>想返回<strong>结果简化</strong>一点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --pretty=oneline</span><br></pre></td></tr></table></figure><p><strong>想看到merge情况</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --graph</span><br></pre></td></tr></table></figure><p>同样有<strong>简化板</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log --graph --pretty=oneline</span><br></pre></td></tr></table></figure><p><strong>回退到上一版本</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard head^</span><br></pre></td></tr></table></figure><p>同理，回退到上上版本就是head^^，以此类推，当然往上100个版本写100个^比较容易数不过来，所以写成head~100.<br>也可以<strong>根据commit id回到该版本</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard [commit id前几位]</span><br></pre></td></tr></table></figure><p>如果有20个版本，回退到第10个版本，但是后悔了，想回退到第15个版本，这是再去看提交历史记录，第11到20个版本的记录已经不在了，找不到commit id怎么办，可以用以下命令查到commit id前几位：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reflog</span><br></pre></td></tr></table></figure><h3 id="撤销修改或删除"><a href="#撤销修改或删除" class="headerlink" title="撤销修改或删除"></a>撤销修改或删除</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -- file_name</span><br></pre></td></tr></table></figure><p>file_name即为想撤销修改或删除的文件名<br>使用以上命令分三种情况：<br>1.工作区文件自修改后还没有被放到暂存区，现在撤销修改就回到版本库中分支一样的状态；<br>2.工作区文件自修改后已经添加到暂存区后，又作了修改或者删除，现在撤销修改或删除就回到和暂存区一样的状态；<br>3.工作区文件自修改后已经commit到分支，又作了修改或者删除，现在撤销或修删除改就回到和分支一样的状态。</p><p><strong>撤销暂存区的修改或删除：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD file_name</span><br></pre></td></tr></table></figure><p>如果commit到了分支，直接版本回退吧</p><h2 id="3-远程仓库"><a href="#3-远程仓库" class="headerlink" title="3.远程仓库"></a>3.远程仓库</h2><p><strong>从远程仓库克隆：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> 地址</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b 分支名称 地址</span><br></pre></td></tr></table></figure><p><strong>将本地分支推送到远程库</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin 当前分支名称</span><br></pre></td></tr></table></figure><p><strong>将远程库拉取到本地分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin 要拉取到分支名称</span><br></pre></td></tr></table></figure><h2 id="4-标签管理"><a href="#4-标签管理" class="headerlink" title="4.标签管理"></a>4.标签管理</h2><p>给当前分支<strong>打标签</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag 标签内容</span><br></pre></td></tr></table></figure><p><strong>指定commit id打标签</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag 标签内容 commit id前几位</span><br></pre></td></tr></table></figure><p><strong>查看标签</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag</span><br></pre></td></tr></table></figure><p><strong>查看标签详细情况</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag 标签内容</span><br></pre></td></tr></table></figure><p><strong>删除标签</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag -d 标签内容</span><br></pre></td></tr></table></figure><p><strong>推送一个本地标签</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin 标签内容</span><br></pre></td></tr></table></figure><p><strong>推送全部未推送过的本地标签</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin --tags</span><br></pre></td></tr></table></figure><p><strong>删除一个远程标签</strong><br>现在本地删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag -d 标签内容</span><br></pre></td></tr></table></figure><p>再执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin :refs/tags/标签内容</span><br></pre></td></tr></table></figure><p>好了，就这么多，如有错误望指出。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Git —— 大名鼎鼎的分布式版本控制系统，据说是Linux系统之父Linus花两周时间写出来的，怎一个牛字了得。&lt;br&gt;写过代码的人大都知道它，以写代码为生的人大都离不开它。&lt;br&gt;官方文档实在太难用，这里参考大佬&lt;a href=&quot;https://www.liaoxuefeng.com/wiki/896043488029600&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;廖雪峰的教程&lt;/strong&gt;&lt;/a&gt;，把常用命令总结写两下来，以供自己和一起搬砖的朋友备查。
    
    </summary>
    
    
      <category term="Code Tool" scheme="https://jkknotes.com/categories/Code-Tool/"/>
    
      <category term="命令速查" scheme="https://jkknotes.com/categories/Code-Tool/%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/"/>
    
    
      <category term="Tool" scheme="https://jkknotes.com/tags/Tool/"/>
    
      <category term="Git" scheme="https://jkknotes.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>kaggle实战——What Causes Heart Disease?</title>
    <link href="https://jkknotes.com/kaggle%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94What%20Causes%20Heart%20Disease_/"/>
    <id>https://jkknotes.com/kaggle实战——What Causes Heart Disease_/</id>
    <published>2019-04-07T04:29:29.000Z</published>
    <updated>2019-11-03T13:47:42.285Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong>记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。<a id="more"></a>现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。</strong></p></blockquote><p><strong>本文同步发在我的 <a href="https://blog.csdn.net/iizhuzhu/article/details/89067386" target="_blank" rel="noopener">CSDN Blog</a>，2019.4.5 刚搞成功，接下来CSDN和 <a href="http://jkknotes.com/">KK’s Notes</a> 同时更新，各位看官大佬多多指教。</strong></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>这个项目来自于<a href="https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model/notebook" target="_blank" rel="noopener">kaggle</a>。项目主要是利用患者的个人信息和检查数据，利用机器学习方法来诊断该患者收否患疾病，并且尝试对识别结果作出解释。这个项目虽然简单但将机器学习的全流程和常用预处理和分析方法都涉及到了，我做完一遍还是有很多收获。以下操作皆在 <strong>Jubyter notebook</strong> 下以 <strong>Python</strong> 进行的。</p><p>主要使用的技术：</p><ul><li>Random Forest</li><li>Feature Importance Analysis: <strong>Permutation importance</strong></li><li>Feature Importance Analysis: <strong>Partial Dependence Plots</strong></li></ul><h2 id="2-Data"><a href="#2-Data" class="headerlink" title="2. Data"></a>2. Data</h2><p>Data from：<a href="https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/" target="_blank" rel="noopener">https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/</a><br>About Data：下载好数据之后直接打开看一看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.read_csv(<span class="string">'data/heart.csv'</span>)</span><br><span class="line">data.info()</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b1.png" width="342" height="334"><br>可以看到总共有303条数据以及13个特征和1个标签，数据没有缺失项。接下看下前十个数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b2.png" width="621" height="308"><br>这13个特征的含义分别是：</p><blockquote><p>age: 年龄<br>sex：该人的性别（1=男性，0=女性）<br>cp：胸痛经历（值1：典型心绞痛，值2：非典型心绞痛，值3：非心绞痛，值4：无症状）<br>trestbps：该人的静息血压（入院时为mm Hg）<br>chol：人体胆固醇测量单位为mg/dl<br>fbs：该人的空腹血糖（&gt; 120mg/dl，1=true; 0= f=alse）<br>restecg：静息心电图测量（0=正常，1=有ST-T波异常，2=按Estes标准显示可能或明确的左心室肥厚）<br>thalach：达到了该人的最大心率<br>exang：运动诱发心绞痛（1=是; 0=否）<br>oldpeak：运动相对于休息引起的ST段压低（’ST’与ECG图上的位置有关）<br>slope：峰值运动ST段的斜率（值1：上升，值2：平坦，值3：下降）<br>ca：主要血管数量（0-3）<br>thal：称为地中海贫血的血液疾病（1=正常; 2=固定缺陷; 3=可逆缺陷）<br>target：心脏病（0=不，1=是）</p></blockquote><p>为了更好的理解数据，我们应该提前查一下每个特征的含义，以及医学上该特征和心脏病的关系。具体这里不再赘述。</p><h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h2><p>这里为了方便后续做心脏病诊断中影响因素分析即Feature Importance Analysis（还是觉得用英文更能表达意思），将部分数值型特征进行转换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">data.loc[data.sex == <span class="number">1</span>, <span class="string">'sex'</span>] = <span class="string">'male'</span></span><br><span class="line">data.loc[data[<span class="string">'sex'</span>] == <span class="number">0</span>, <span class="string">'sex'</span>] = <span class="string">'female'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">1</span>, <span class="string">'cp'</span>] = <span class="string">'typical'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">2</span>, <span class="string">'cp'</span>] = <span class="string">'atypical'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">3</span>, <span class="string">'cp'</span>] = <span class="string">'no_pain'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">4</span>, <span class="string">'cp'</span>] = <span class="string">'no_feel'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'fbs'</span>] == <span class="number">1</span>, <span class="string">'fbs'</span>] = <span class="string">'higher than 120 mg/dl'</span></span><br><span class="line">data.loc[data[<span class="string">'fbs'</span>] == <span class="number">0</span>, <span class="string">'fbs'</span>] = <span class="string">'lower than 120 mg/dl'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">0</span>, <span class="string">'restecg'</span>] = <span class="string">'normal'</span></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">1</span>, <span class="string">'restecg'</span>] = <span class="string">'ST-T wave abnormality'</span></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">2</span>, <span class="string">'restecg'</span>] = <span class="string">'left ventricular hypertrophy'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'exang'</span>] == <span class="number">1</span>, <span class="string">'exang'</span>] = <span class="string">'true'</span></span><br><span class="line">data.loc[data[<span class="string">'exang'</span>] == <span class="number">0</span>, <span class="string">'exang'</span>] = <span class="string">'false'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">1</span>, <span class="string">'slope'</span>] = <span class="string">'up'</span></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">2</span>, <span class="string">'slope'</span>] = <span class="string">'flat'</span></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">3</span>, <span class="string">'slope'</span>] = <span class="string">'down'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">1</span>, <span class="string">'thal'</span>] = <span class="string">'normal'</span></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">2</span>, <span class="string">'thal'</span>] = <span class="string">'fixed defect'</span></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">3</span>, <span class="string">'thal'</span>] = <span class="string">'reversable defect'</span></span><br></pre></td></tr></table></figure><p>检查下数据情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.describe(include=[np.object])</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b3.png" width="560" height="147"><br>可以看到特征thal有4个值，而我们在转换时只转换了3个。实际上thal存在2个缺失值用0补齐的。为了防止数据类型错误，这里做一下类型转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'thal'</span>] = data[<span class="string">'thal'</span>].astype(<span class="string">'object'</span>)</span><br></pre></td></tr></table></figure><p>再看下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b4.png" width="889" height="174"><br>模型的训练肯定需要数值型特征。这里对特征进行Onehot编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = pd.get_dummies(data, drop_first=<span class="literal">True</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b5.png" width="1011" height="186"><br>（由于我还不知道在用markdown编辑时怎么显示运行结果，这里用的是截图，只能截取一部分，还有特征没有截取出来）<br>数据预处理部分就到此为止，接下来上模型。</p><h2 id="4-Random-Forest"><a href="#4-Random-Forest" class="headerlink" title="4. Random Forest"></a>4. Random Forest</h2><p>对于 Random Forest 的原理这里就不介绍了，网上介绍的文章也很多。废话不多说，直接import package.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>将数据分成 train_data 和 test_data 2个集合，二者比例为8:2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_x, test_x, train_y, test_y = train_test_split(data.drop(columns=<span class="string">'target'</span>),</span><br><span class="line">                                                    data[<span class="string">'target'</span>],</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>简单的画个图调个参。这里 Random Forest 主要的参数有基学习器决策树的最大深度（这里依据经验选5）、基学习器个数 n_estimators。这里基学习器选用CART。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_score = []</span><br><span class="line">test_score = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>):</span><br><span class="line">    model = RandomForestClassifier(max_depth=<span class="number">5</span>,</span><br><span class="line">                                   n_estimators=n，</span><br><span class="line">                                   criterion=<span class="string">'gini'</span>)</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    train_score.append(model.score(train_x, train_y))</span><br><span class="line">    test_score.append(model.score(test_x, test_y))</span><br></pre></td></tr></table></figure><p>训练完，把train和test上的accuracy随基学习器个数的变化画成图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x_axis = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(x_axis, train_score[:<span class="number">99</span>])</span><br><span class="line">ax.plot(x_axis, test_score[:<span class="number">99</span>], c=<span class="string">"r"</span>)</span><br><span class="line">plt.xlim([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.rcParams[<span class="string">'font.size'</span>] = <span class="number">12</span></span><br><span class="line">plt.xlabel(<span class="string">'n_estimators'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'accuracy'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b6.png" alt><br>可以看到大概是n_estimators=14的时候效果最好，train和test上的accuracy分别是0.9463，0.8361。看上去没有那么差。</p><h2 id="5-模型评估"><a href="#5-模型评估" class="headerlink" title="5. 模型评估"></a>5. 模型评估</h2><p>训练完模型，用<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">ROC曲线</a>来评估下模型的效果。ROC曲线事宜FPR和TPR分别为横纵轴作出的曲线，其和坐标轴围成的面积越大，说明模型效果越好。具体评判标准见下文。说一下几个概念：</p><blockquote><ul><li>TPR: 真正例率，表示所有真正为正例的样本被正确预测出来的比例，等同于Recall</li><li>FNR: 假负例率，FNR = 1 - TPR</li><li>FPR: 假正例率，表示所有负例中被预测为正例的比例。</li><li>TNR: 真负例率，TNR = 1 - FPR</li></ul></blockquote><p>好吧，我也快晕了。<br>接下来计算一下正例和负例的recall</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc, roc_curve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line">confusion_m = confusion_matrix(test_y, pred_y) </span><br><span class="line"><span class="keyword">print</span> confusion_m</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[29  6]</span><br><span class="line"> [ 4 22]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total = confusion_m.sum()</span><br><span class="line">tpr = float(confusion_m[<span class="number">0</span>][<span class="number">0</span>]) / (confusion_m[<span class="number">0</span>][<span class="number">0</span>] + confusion_m[<span class="number">1</span>][<span class="number">0</span>])</span><br><span class="line">tnr = float(confusion_m[<span class="number">1</span>][<span class="number">1</span>]) / (confusion_m[<span class="number">1</span>][<span class="number">1</span>] + confusion_m[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> tpr, tnr</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.878787878788 0.785714285714</span><br></pre></td></tr></table></figure><p>Just so so!!</p><p>画ROC曲线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pred_y = model.predict(test_x)  <span class="comment"># 预测结果</span></span><br><span class="line">pred_prob_y = model.predict_proba(test_x)[:, <span class="number">1</span>]  <span class="comment"># 为正例的概率</span></span><br><span class="line">fpr_list, tpr_list, throsholds = roc_curve(test_y, pred_prob_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(fpr_list, tpr_list)</span><br><span class="line">ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], transform=ax.transAxes, ls=<span class="string">"--"</span>, c=<span class="string">"r"</span>)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.rcParams[<span class="string">'font.size'</span>] = <span class="number">12</span></span><br><span class="line">plt.title(<span class="string">'roc curve'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'fpr'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'tpr'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/ROC.png" alt><br>前文说了，ROC曲线和坐标轴围成的面积越大，说明模型效果越好。这个面积就叫 AUC .根据AUC的值，可参考下面的规则评估模型：</p><blockquote><ul><li>0.90 - 1.00 = excellent</li><li>0.80 - 0.90 = good</li><li>0.70 - 0.80 = fair</li><li>0.60 - 0.70 = poor</li><li>0.50 - 0.60 = fail</li></ul></blockquote><p>看看我们训练模型的AUC</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auc(fpr_list, tpr_list)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9032967032967033</span><br></pre></td></tr></table></figure><p>OK， working well！<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly90aW1nc2EuYmFpZHUuY29tL3RpbWc_aW1hZ2UmcXVhbGl0eT04MCZzaXplPWI5OTk5XzEwMDAwJnNlYz0xNTU0NjM2NjI5MjM4JmRpPWI3N2Q3MWU2MzVmOGVkMmU3ZmE0MGM4NDdhYzFiODkzJmltZ3R5cGU9MCZzcmM9aHR0cDovL2Itc3NsLmR1aXRhbmcuY29tL3VwbG9hZHMvaXRlbS8yMDE3MDMvMjkvMjAxNzAzMjkxNjE3MjhfZmRTTUYudGh1bWIuMjI0XzAuZ2lm" alt=""></p><h2 id="6-Feature-Importance-Analysis"><a href="#6-Feature-Importance-Analysis" class="headerlink" title="6. Feature Importance Analysis"></a>6. Feature Importance Analysis</h2><p>训练完模型，我们希望能从模型里得到点什么， 比如说哪些特征对模型结果贡献率比较大，是不是意味着这些影响因素在实际心脏病诊断中也是很重要对参考，或者说还能发现一些现有医学没有发现的发现。所有接下来我们做的是一件很有意思的事。</p><h5 id="6-1-决策树可视化"><a href="#6-1-决策树可视化" class="headerlink" title="6.1 决策树可视化"></a>6.1 决策树可视化</h5><p>如果我没记错的话， 根据决策树的原理，越先分裂的特征越重要。那么下面对决策树进行可视化，看看它到底做了什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 feature_name</span></span><br><span class="line">estimator = model.estimators_[<span class="number">1</span>]</span><br><span class="line">features = [i <span class="keyword">for</span> i <span class="keyword">in</span> train_x.columns]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0 —&gt; no disease，1 —&gt; disease</span></span><br><span class="line">train_y_str = train_y.astype(<span class="string">'str'</span>)</span><br><span class="line">train_y_str[train_y_str == <span class="string">'0'</span>] = <span class="string">'no disease'</span></span><br><span class="line">train_y_str[train_y_str == <span class="string">'1'</span>] = <span class="string">'disease'</span></span><br><span class="line">train_y_str = train_y_str.values</span><br></pre></td></tr></table></figure><p>sklearn 真是个好东西，你能想到对功能他都有。下面用 sklearn 的 export_graphviz 对决策树进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export_graphviz(estimator, out_file=<span class="string">'tree.dot'</span>, </span><br><span class="line">                feature_names = features,</span><br><span class="line">                class_names = train_y_str,</span><br><span class="line">                rounded = <span class="literal">True</span>, proportion = <span class="literal">True</span>, </span><br><span class="line">                label=<span class="string">'root'</span>,</span><br><span class="line">                precision = <span class="number">2</span>, filled = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>生成对这个 tree.dot 文件还不能直接看，网上查了一下，把它输出来看看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line">img = pydotplus.graph_from_dot_file(<span class="string">'tree.dot'</span>)</span><br><span class="line"><span class="comment">#img.write_pdf('tree.pdf') #输出成PDF</span></span><br><span class="line">Image(img.create_png())</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/tree.png" alt><br>实际上这张图就解释来决策树的生成过程。一般我们认为最先分裂的特征越重要，但是从这张图我们并不能很直观的看出特征的重要性。</p><h5 id="6-2-Permutation-importance"><a href="#6-2-Permutation-importance" class="headerlink" title="6.2 Permutation importance"></a>6.2 Permutation importance</h5><p>我们换一个工具—<a href="https://www.kaggle.com/dansbecker/permutation-importance" target="_blank" rel="noopener">Permutation importance</a>. 其原理是依次打乱test_data中其中一个特征数值的顺序，其实就是做shuffle，然后观察模型的效果，下降的多的说明这个特征对模型比较重要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> eli5</span><br><span class="line"><span class="keyword">from</span> eli5.sklearn <span class="keyword">import</span> PermutationImportance</span><br><span class="line"></span><br><span class="line">perm = PermutationImportance(model, random_state=<span class="number">20</span>).fit(test_x, test_y)</span><br><span class="line">eli5.show_weights(perm, feature_names=test_x.columns.tolist())</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/feature_w.png" width="323" height="290"><br>一目了然，一切尽在不言中。还是说俩句吧，绿色越深表示正相关越强，红色越深表示负相关越强。<br>实际上我发现改变 PermutationImportance 的参数 random_state 的值结果变化挺大的，不过还是有几个特征位次变化不大，结果还是具有参考意义。</p><h5 id="6-3-Partial-Dependence-Plots"><a href="#6-3-Partial-Dependence-Plots" class="headerlink" title="6.3 Partial Dependence Plots"></a>6.3 Partial Dependence Plots</h5><p>我们试试另一个工具—<a href="https://www.kaggle.com/dansbecker/partial-plots" target="_blank" rel="noopener">Partial Dependence Plots</a>. 其原理和 Permutation importance 有点类似，当它判断一个特征对模型的影响时，对于所有样本，将该特征依次取该特征的所有取值，观察模型结果的变化。先画图，再根据图解释一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pdpbox <span class="keyword">import</span> pdp, info_plots</span><br><span class="line"></span><br><span class="line">total_features = train_x.columns.values.tolist()</span><br><span class="line">feature_name = <span class="string">'oldpeak'</span></span><br><span class="line">pdp_dist = pdp.pdp_isolate(model=model, dataset=test_x, model_features=total_features, feature=feature_name)</span><br><span class="line"></span><br><span class="line">pdp.pdp_plot(pdp_dist, feature_name)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b7.png" width="670" height="425"><br>上图的纵坐标是模型相对于base model 的变化，横坐标是该特征的所有取值，实线表示相对于base model 的变化的平均值，蓝色阴影表示置信度。oldpeak表示运动相对于休息引起的ST段压低，可以看到其取值越大，患心脏病的可能性越低。不知道这个结果可不可信，我觉得需要医学知识作支撑。</p><p>又试了几个特征：</p><p><strong>Sex：</strong><br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b8.png" width="670" height="425"><br>上图说明男性比女性患心脏病的概率要低些，网上查了一下，还真是这样。</p><p><strong>Age：</strong><br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b9.png" width="670" height="425"><br>上图表示60岁以上老人心脏病高发，这个和现有理论相符。</p><p>接下来看一下 <strong>2D Partial Dependence Plots</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inter = pdp.pdp_interact(model=model, </span><br><span class="line"> dataset=test_x, </span><br><span class="line"> model_features=total_features, </span><br><span class="line"> features=[<span class="string">'oldpeak'</span>, <span class="string">'age'</span>])</span><br><span class="line"></span><br><span class="line">pdp.pdp_interact_plot(pdp_interact_out=inter, </span><br><span class="line">  feature_names=[<span class="string">'oldpeak'</span>, <span class="string">'age'</span>], </span><br><span class="line">  plot_type=<span class="string">'contour'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1904/b10.png" alt><br>这个图一开始没看懂，后来仔细看了<a href="https://www.kaggle.com/dansbecker/partial-plots" target="_blank" rel="noopener">Partial Dependence Plots</a> 的说明文档才搞明白。图中颜色从浅到深表示患心脏病概率降低，以最深的那个紫色为例，oldpeak &gt; 3.0 &amp;&amp; 45 &lt; age &lt; 65 时，患病概率最低，图中黄色部分表示，oldpeak &lt; 0.25 &amp;&amp;  ( age &lt; 45 || age &gt; 65 ) 时，患病概率最高。</p><h2 id="7-后记"><a href="#7-后记" class="headerlink" title="7. 后记"></a>7. 后记</h2><p>实际上本项目的数据是非常小的，其结果的可靠性也是值得怀疑的。但是通过这个项目，去经历机器学习项目的完整过程，却能学到很多东西。重要的是过程，更重要的是举一反三。该项目还引入了2个很有趣的Feature Importance Analysis的方法，对于我来说是新知识，也算是学到了。</p><p>这一篇到这里结束了，期待下一篇。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jkknotes.com/categories/Machine-Learning/"/>
    
      <category term="项目实战" scheme="https://jkknotes.com/categories/Machine-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="AI医疗" scheme="https://jkknotes.com/tags/AI%E5%8C%BB%E7%96%97/"/>
    
      <category term="Machine Learning" scheme="https://jkknotes.com/tags/Machine-Learning/"/>
    
      <category term="Random Forest" scheme="https://jkknotes.com/tags/Random-Forest/"/>
    
      <category term="Feature Engineering" scheme="https://jkknotes.com/tags/Feature-Engineering/"/>
    
  </entry>
  
</feed>
