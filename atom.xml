<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>KK&#39;s Note</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jjkkk.gitee.io/blog/"/>
  <updated>2019-11-03T12:53:43.551Z</updated>
  <id>https://jjkkk.gitee.io/blog/</id>
  
  <author>
    <name>KK.J</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Git基本命令大全</title>
    <link href="https://jjkkk.gitee.io/blog/Code-Tool/%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/Git%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/"/>
    <id>https://jjkkk.gitee.io/blog/Code-Tool/命令速查/Git基本命令大全/</id>
    <published>2019-06-04T04:29:29.000Z</published>
    <updated>2019-11-03T12:53:43.551Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Git —— 大名鼎鼎的分布式版本控制系统，据说是Linux系统之父Linus花两周时间写出来的，怎一个牛字了得。<br>写过代码的人大都知道它，以写代码为生的人大都离不开它。<br>官方文档实在太难用，这里参考大佬<a href="https://www.liaoxuefeng.com/wiki/896043488029600" target="_blank" rel="noopener"><strong>廖雪峰的教程</strong></a>，把常用命令总结写两下来，以供自己和一起搬砖的朋友备查。<a id="more"></a></p><p>这里先介绍本地仓库的常用命令，再介绍涉及远程仓库和分支管理的常用命令。本文没有实例介绍讲解，只做命令备查。如需详细教程，请参考<a href="https://www.liaoxuefeng.com/wiki/896043488029600" target="_blank" rel="noopener"><strong>廖雪峰的Git教程</strong></a>。</p><p><strong><em>本文同步发在我的<a href="https://blog.csdn.net/iizhuzhu/article/details/90760926" target="_blank" rel="noopener">CSDN Blog</a>，欢迎各位看官大佬关注指教。</em></strong></p><h2 id="1-基本操作"><a href="#1-基本操作" class="headerlink" title="1.基本操作"></a>1.基本操作</h2><h3 id="创建版本库并添加文件"><a href="#创建版本库并添加文件" class="headerlink" title="创建版本库并添加文件"></a>创建版本库并添加文件</h3><p>进入到需要<strong>创建版本库</strong>的文件夹，执行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p>git的本地库包含三部分：工作区、暂存区（stage）、分支（branch，默认为master），git add命令是将文件从工作区添加到暂存区，git commit命令是将文件从暂存区添加到分支，他们的关系如下图所示：<br><img src="https://gitee.com/jjkkk/cloud_img/raw/master/1906/a2.jpeg" alt><br><strong>添加新文件或修改过的文件</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add file1_name file2_name file3_name ....</span><br><span class="line">git commit -m <span class="string">'这里是对添加文件的说明'</span></span><br></pre></td></tr></table></figure><p><strong>查看版本库的状态</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><h3 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h3><p><strong>创建分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch 分支名称</span><br></pre></td></tr></table></figure><p><strong>切换分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout 分支名称</span><br></pre></td></tr></table></figure><p><strong>创建并切换分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b 分支名称</span><br></pre></td></tr></table></figure><p><strong>删除分支</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -d 分支名称</span><br></pre></td></tr></table></figure><p><strong>查看所有分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure><p><strong>合并分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git merge 准备与当前分支合并的分支名称</span><br></pre></td></tr></table></figure><p><strong>若merge出现冲突，解决办法请参考</strong><a href="https://www.liaoxuefeng.com/wiki/896043488029600/900004111093344" target="_blank" rel="noopener"><strong>这里</strong></a></p><p><strong>stash功能</strong><br>当在当前分支编辑内容时，没有add和commit就切换到其它分支，发现其它分支中也会出现相应更改，stash功能可以解决这个问题。如果出现这种情况，正在当前分支编辑，需要临时去其他分支处理一些事情，但是当前分支又不能add和commit，这时就需要stash功能，可以它就像一个icebox，能当前工作冻结。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash</span><br></pre></td></tr></table></figure><p>查看冻结列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash list</span><br></pre></td></tr></table></figure><p>想解冻2种方法：<br>第一种：解冻的同时把stash记录也删了，也就是在list中看不到了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash pop</span><br></pre></td></tr></table></figure><p>第二种：解冻不删list中记录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash apply</span><br></pre></td></tr></table></figure><p>想删记录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash drop</span><br></pre></td></tr></table></figure><h2 id="2-时光穿梭"><a href="#2-时光穿梭" class="headerlink" title="2.时光穿梭"></a>2.时光穿梭</h2><h3 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h3><p>当不断对文件进行修改，然后不断提交修改到版本库里，难免会出现失误，把文件改乱或出现其他错误，就需要回到上一次或者之前任一次提交的状态。<br>首先<strong>查看提交历史记录</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></table></figure><p>返回结果中commit 后面的十六进制数即为该次提交的commit id。<br>想返回<strong>结果简化</strong>一点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --pretty=oneline</span><br></pre></td></tr></table></figure><p><strong>想看到merge情况</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --graph</span><br></pre></td></tr></table></figure><p>同样有<strong>简化板</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log --graph --pretty=oneline</span><br></pre></td></tr></table></figure><p><strong>回退到上一版本</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard head^</span><br></pre></td></tr></table></figure><p>同理，回退到上上版本就是head^^，以此类推，当然往上100个版本写100个^比较容易数不过来，所以写成head~100.<br>也可以<strong>根据commit id回到该版本</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard [commit id前几位]</span><br></pre></td></tr></table></figure><p>如果有20个版本，回退到第10个版本，但是后悔了，想回退到第15个版本，这是再去看提交历史记录，第11到20个版本的记录已经不在了，找不到commit id怎么办，可以用以下命令查到commit id前几位：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reflog</span><br></pre></td></tr></table></figure><h3 id="撤销修改或删除"><a href="#撤销修改或删除" class="headerlink" title="撤销修改或删除"></a>撤销修改或删除</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -- file_name</span><br></pre></td></tr></table></figure><p>file_name即为想撤销修改或删除的文件名<br>使用以上命令分三种情况：<br>1.工作区文件自修改后还没有被放到暂存区，现在撤销修改就回到版本库中分支一样的状态；<br>2.工作区文件自修改后已经添加到暂存区后，又作了修改或者删除，现在撤销修改或删除就回到和暂存区一样的状态；<br>3.工作区文件自修改后已经commit到分支，又作了修改或者删除，现在撤销或修删除改就回到和分支一样的状态。</p><p><strong>撤销暂存区的修改或删除：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD file_name</span><br></pre></td></tr></table></figure><p>如果commit到了分支，直接版本回退吧</p><h2 id="3-远程仓库"><a href="#3-远程仓库" class="headerlink" title="3.远程仓库"></a>3.远程仓库</h2><p><strong>从远程仓库克隆：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> 地址</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b 分支名称 地址</span><br></pre></td></tr></table></figure><p><strong>将本地分支推送到远程库</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin 当前分支名称</span><br></pre></td></tr></table></figure><p><strong>将远程库拉取到本地分支</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin 要拉取到分支名称</span><br></pre></td></tr></table></figure><h2 id="4-标签管理"><a href="#4-标签管理" class="headerlink" title="4.标签管理"></a>4.标签管理</h2><p>给当前分支<strong>打标签</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag 标签内容</span><br></pre></td></tr></table></figure><p><strong>指定commit id打标签</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag 标签内容 commit id前几位</span><br></pre></td></tr></table></figure><p><strong>查看标签</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag</span><br></pre></td></tr></table></figure><p><strong>查看标签详细情况</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag 标签内容</span><br></pre></td></tr></table></figure><p><strong>删除标签</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag -d 标签内容</span><br></pre></td></tr></table></figure><p><strong>推送一个本地标签</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin 标签内容</span><br></pre></td></tr></table></figure><p><strong>推送全部未推送过的本地标签</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin --tags</span><br></pre></td></tr></table></figure><p><strong>删除一个远程标签</strong><br>现在本地删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag -d 标签内容</span><br></pre></td></tr></table></figure><p>再执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin :refs/tags/标签内容</span><br></pre></td></tr></table></figure><p>好了，就这么多，如有错误望指出。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Git —— 大名鼎鼎的分布式版本控制系统，据说是Linux系统之父Linus花两周时间写出来的，怎一个牛字了得。&lt;br&gt;写过代码的人大都知道它，以写代码为生的人大都离不开它。&lt;br&gt;官方文档实在太难用，这里参考大佬&lt;a href=&quot;https://www.liaoxuefeng.com/wiki/896043488029600&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;廖雪峰的教程&lt;/strong&gt;&lt;/a&gt;，把常用命令总结写两下来，以供自己和一起搬砖的朋友备查。
    
    </summary>
    
    
      <category term="Code Tool" scheme="https://jjkkk.gitee.io/blog/categories/Code-Tool/"/>
    
      <category term="命令速查" scheme="https://jjkkk.gitee.io/blog/categories/Code-Tool/%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/"/>
    
    
      <category term="Git" scheme="https://jjkkk.gitee.io/blog/tags/Git/"/>
    
      <category term="Tool" scheme="https://jjkkk.gitee.io/blog/tags/Tool/"/>
    
  </entry>
  
  <entry>
    <title>kaggle实战——What Causes Heart Disease?</title>
    <link href="https://jjkkk.gitee.io/blog/Machine-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/kaggle%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94What%20Causes%20Heart%20Disease_/"/>
    <id>https://jjkkk.gitee.io/blog/Machine-Learning/项目实战/kaggle实战——What Causes Heart Disease_/</id>
    <published>2019-04-07T04:29:29.000Z</published>
    <updated>2019-10-25T13:58:32.519Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong>记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。<a id="more"></a>现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。</strong></p></blockquote><p><strong>本文同步发在我的 <a href="https://blog.csdn.net/iizhuzhu/article/details/89067386" target="_blank" rel="noopener">CSDN Blog</a>，2019.4.5 刚搞成功，接下来CSDN和 <a href="http://jkknotes.com/" target="_blank" rel="noopener">KK’s Notes</a> 同时更新，各位看官大佬多多指教。</strong></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>这个项目来自于<a href="https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model/notebook" target="_blank" rel="noopener">kaggle</a>。项目主要是利用患者的个人信息和检查数据，利用机器学习方法来诊断该患者收否患疾病，并且尝试对识别结果作出解释。这个项目虽然简单但将机器学习的全流程和常用预处理和分析方法都涉及到了，我做完一遍还是有很多收获。以下操作皆在 <strong>Jubyter notebook</strong> 下以 <strong>Python</strong> 进行的。</p><p>主要使用的技术：</p><ul><li>Random Forest</li><li>Feature Importance Analysis: <strong>Permutation importance</strong></li><li>Feature Importance Analysis: <strong>Partial Dependence Plots</strong></li></ul><h2 id="2-Data"><a href="#2-Data" class="headerlink" title="2. Data"></a>2. Data</h2><p>Data from：<a href="https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/" target="_blank" rel="noopener">https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/</a><br>About Data：下载好数据之后直接打开看一看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.read_csv(<span class="string">'data/heart.csv'</span>)</span><br><span class="line">data.info()</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b1.png" width="342" height="334"><br>可以看到总共有303条数据以及13个特征和1个标签，数据没有缺失项。接下看下前十个数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b2.png" width="621" height="308"><br>这13个特征的含义分别是：</p><blockquote><p>age: 年龄<br>sex：该人的性别（1=男性，0=女性）<br>cp：胸痛经历（值1：典型心绞痛，值2：非典型心绞痛，值3：非心绞痛，值4：无症状）<br>trestbps：该人的静息血压（入院时为mm Hg）<br>chol：人体胆固醇测量单位为mg/dl<br>fbs：该人的空腹血糖（&gt; 120mg/dl，1=true; 0= f=alse）<br>restecg：静息心电图测量（0=正常，1=有ST-T波异常，2=按Estes标准显示可能或明确的左心室肥厚）<br>thalach：达到了该人的最大心率<br>exang：运动诱发心绞痛（1=是; 0=否）<br>oldpeak：运动相对于休息引起的ST段压低（’ST’与ECG图上的位置有关）<br>slope：峰值运动ST段的斜率（值1：上升，值2：平坦，值3：下降）<br>ca：主要血管数量（0-3）<br>thal：称为地中海贫血的血液疾病（1=正常; 2=固定缺陷; 3=可逆缺陷）<br>target：心脏病（0=不，1=是）</p></blockquote><p>为了更好的理解数据，我们应该提前查一下每个特征的含义，以及医学上该特征和心脏病的关系。具体这里不再赘述。</p><h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h2><p>这里为了方便后续做心脏病诊断中影响因素分析即Feature Importance Analysis（还是觉得用英文更能表达意思），将部分数值型特征进行转换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">data.loc[data.sex == <span class="number">1</span>, <span class="string">'sex'</span>] = <span class="string">'male'</span></span><br><span class="line">data.loc[data[<span class="string">'sex'</span>] == <span class="number">0</span>, <span class="string">'sex'</span>] = <span class="string">'female'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">1</span>, <span class="string">'cp'</span>] = <span class="string">'typical'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">2</span>, <span class="string">'cp'</span>] = <span class="string">'atypical'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">3</span>, <span class="string">'cp'</span>] = <span class="string">'no_pain'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">4</span>, <span class="string">'cp'</span>] = <span class="string">'no_feel'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'fbs'</span>] == <span class="number">1</span>, <span class="string">'fbs'</span>] = <span class="string">'higher than 120 mg/dl'</span></span><br><span class="line">data.loc[data[<span class="string">'fbs'</span>] == <span class="number">0</span>, <span class="string">'fbs'</span>] = <span class="string">'lower than 120 mg/dl'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">0</span>, <span class="string">'restecg'</span>] = <span class="string">'normal'</span></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">1</span>, <span class="string">'restecg'</span>] = <span class="string">'ST-T wave abnormality'</span></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">2</span>, <span class="string">'restecg'</span>] = <span class="string">'left ventricular hypertrophy'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'exang'</span>] == <span class="number">1</span>, <span class="string">'exang'</span>] = <span class="string">'true'</span></span><br><span class="line">data.loc[data[<span class="string">'exang'</span>] == <span class="number">0</span>, <span class="string">'exang'</span>] = <span class="string">'false'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">1</span>, <span class="string">'slope'</span>] = <span class="string">'up'</span></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">2</span>, <span class="string">'slope'</span>] = <span class="string">'flat'</span></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">3</span>, <span class="string">'slope'</span>] = <span class="string">'down'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">1</span>, <span class="string">'thal'</span>] = <span class="string">'normal'</span></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">2</span>, <span class="string">'thal'</span>] = <span class="string">'fixed defect'</span></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">3</span>, <span class="string">'thal'</span>] = <span class="string">'reversable defect'</span></span><br></pre></td></tr></table></figure><p>检查下数据情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.describe(include=[np.object])</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b3.png" width="560" height="147"><br>可以看到特征thal有4个值，而我们在转换时只转换了3个。实际上thal存在2个缺失值用0补齐的。为了防止数据类型错误，这里做一下类型转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'thal'</span>] = data[<span class="string">'thal'</span>].astype(<span class="string">'object'</span>)</span><br></pre></td></tr></table></figure><p>再看下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b4.png" width="889" height="174"><br>模型的训练肯定需要数值型特征。这里对特征进行Onehot编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = pd.get_dummies(data, drop_first=<span class="literal">True</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b5.png" width="1011" height="186"><br>（由于我还不知道在用markdown编辑时怎么显示运行结果，这里用的是截图，只能截取一部分，还有特征没有截取出来）<br>数据预处理部分就到此为止，接下来上模型。</p><h2 id="4-Random-Forest"><a href="#4-Random-Forest" class="headerlink" title="4. Random Forest"></a>4. Random Forest</h2><p>对于 Random Forest 的原理这里就不介绍了，网上介绍的文章也很多。废话不多说，直接import package.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>将数据分成 train_data 和 test_data 2个集合，二者比例为8:2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_x, test_x, train_y, test_y = train_test_split(data.drop(columns=<span class="string">'target'</span>),</span><br><span class="line">                                                    data[<span class="string">'target'</span>],</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>简单的画个图调个参。这里 Random Forest 主要的参数有基学习器决策树的最大深度（这里依据经验选5）、基学习器个数 n_estimators。这里基学习器选用CART。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_score = []</span><br><span class="line">test_score = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>):</span><br><span class="line">    model = RandomForestClassifier(max_depth=<span class="number">5</span>,</span><br><span class="line">                                   n_estimators=n，</span><br><span class="line">                                   criterion=<span class="string">'gini'</span>)</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    train_score.append(model.score(train_x, train_y))</span><br><span class="line">    test_score.append(model.score(test_x, test_y))</span><br></pre></td></tr></table></figure><p>训练完，把train和test上的accuracy随基学习器个数的变化画成图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x_axis = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(x_axis, train_score[:<span class="number">99</span>])</span><br><span class="line">ax.plot(x_axis, test_score[:<span class="number">99</span>], c=<span class="string">"r"</span>)</span><br><span class="line">plt.xlim([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.rcParams[<span class="string">'font.size'</span>] = <span class="number">12</span></span><br><span class="line">plt.xlabel(<span class="string">'n_estimators'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'accuracy'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b6.png" alt><br>可以看到大概是n_estimators=14的时候效果最好，train和test上的accuracy分别是0.9463，0.8361。看上去没有那么差。</p><h2 id="5-模型评估"><a href="#5-模型评估" class="headerlink" title="5. 模型评估"></a>5. 模型评估</h2><p>训练完模型，用<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">ROC曲线</a>来评估下模型的效果。ROC曲线事宜FPR和TPR分别为横纵轴作出的曲线，其和坐标轴围成的面积越大，说明模型效果越好。具体评判标准见下文。说一下几个概念：</p><blockquote><ul><li>TPR: 真正例率，表示所有真正为正例的样本被正确预测出来的比例，等同于Recall</li><li>FNR: 假负例率，FNR = 1 - TPR</li><li>FPR: 假正例率，表示所有负例中被预测为正例的比例。</li><li>TNR: 真负例率，TNR = 1 - FPR</li></ul></blockquote><p>好吧，我也快晕了。<br>接下来计算一下正例和负例的recall</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc, roc_curve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line">confusion_m = confusion_matrix(test_y, pred_y) </span><br><span class="line"><span class="keyword">print</span> confusion_m</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[29  6]</span><br><span class="line"> [ 4 22]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total = confusion_m.sum()</span><br><span class="line">tpr = float(confusion_m[<span class="number">0</span>][<span class="number">0</span>]) / (confusion_m[<span class="number">0</span>][<span class="number">0</span>] + confusion_m[<span class="number">1</span>][<span class="number">0</span>])</span><br><span class="line">tnr = float(confusion_m[<span class="number">1</span>][<span class="number">1</span>]) / (confusion_m[<span class="number">1</span>][<span class="number">1</span>] + confusion_m[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> tpr, tnr</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.878787878788 0.785714285714</span><br></pre></td></tr></table></figure><p>Just so so!!</p><p>画ROC曲线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pred_y = model.predict(test_x)  <span class="comment"># 预测结果</span></span><br><span class="line">pred_prob_y = model.predict_proba(test_x)[:, <span class="number">1</span>]  <span class="comment"># 为正例的概率</span></span><br><span class="line">fpr_list, tpr_list, throsholds = roc_curve(test_y, pred_prob_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(fpr_list, tpr_list)</span><br><span class="line">ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], transform=ax.transAxes, ls=<span class="string">"--"</span>, c=<span class="string">"r"</span>)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.rcParams[<span class="string">'font.size'</span>] = <span class="number">12</span></span><br><span class="line">plt.title(<span class="string">'roc curve'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'fpr'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'tpr'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/ROC.png" alt><br>前文说了，ROC曲线和坐标轴围成的面积越大，说明模型效果越好。这个面积就叫 AUC .根据AUC的值，可参考下面的规则评估模型：</p><blockquote><ul><li>0.90 - 1.00 = excellent</li><li>0.80 - 0.90 = good</li><li>0.70 - 0.80 = fair</li><li>0.60 - 0.70 = poor</li><li>0.50 - 0.60 = fail</li></ul></blockquote><p>看看我们训练模型的AUC</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auc(fpr_list, tpr_list)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9032967032967033</span><br></pre></td></tr></table></figure><p>OK， working well！<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly90aW1nc2EuYmFpZHUuY29tL3RpbWc_aW1hZ2UmcXVhbGl0eT04MCZzaXplPWI5OTk5XzEwMDAwJnNlYz0xNTU0NjM2NjI5MjM4JmRpPWI3N2Q3MWU2MzVmOGVkMmU3ZmE0MGM4NDdhYzFiODkzJmltZ3R5cGU9MCZzcmM9aHR0cDovL2Itc3NsLmR1aXRhbmcuY29tL3VwbG9hZHMvaXRlbS8yMDE3MDMvMjkvMjAxNzAzMjkxNjE3MjhfZmRTTUYudGh1bWIuMjI0XzAuZ2lm" alt=""></p><h2 id="6-Feature-Importance-Analysis"><a href="#6-Feature-Importance-Analysis" class="headerlink" title="6. Feature Importance Analysis"></a>6. Feature Importance Analysis</h2><p>训练完模型，我们希望能从模型里得到点什么， 比如说哪些特征对模型结果贡献率比较大，是不是意味着这些影响因素在实际心脏病诊断中也是很重要对参考，或者说还能发现一些现有医学没有发现的发现。所有接下来我们做的是一件很有意思的事。</p><h5 id="6-1-决策树可视化"><a href="#6-1-决策树可视化" class="headerlink" title="6.1 决策树可视化"></a>6.1 决策树可视化</h5><p>如果我没记错的话， 根据决策树的原理，越先分裂的特征越重要。那么下面对决策树进行可视化，看看它到底做了什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 feature_name</span></span><br><span class="line">estimator = model.estimators_[<span class="number">1</span>]</span><br><span class="line">features = [i <span class="keyword">for</span> i <span class="keyword">in</span> train_x.columns]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0 —&gt; no disease，1 —&gt; disease</span></span><br><span class="line">train_y_str = train_y.astype(<span class="string">'str'</span>)</span><br><span class="line">train_y_str[train_y_str == <span class="string">'0'</span>] = <span class="string">'no disease'</span></span><br><span class="line">train_y_str[train_y_str == <span class="string">'1'</span>] = <span class="string">'disease'</span></span><br><span class="line">train_y_str = train_y_str.values</span><br></pre></td></tr></table></figure><p>sklearn 真是个好东西，你能想到对功能他都有。下面用 sklearn 的 export_graphviz 对决策树进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export_graphviz(estimator, out_file=<span class="string">'tree.dot'</span>, </span><br><span class="line">                feature_names = features,</span><br><span class="line">                class_names = train_y_str,</span><br><span class="line">                rounded = <span class="literal">True</span>, proportion = <span class="literal">True</span>, </span><br><span class="line">                label=<span class="string">'root'</span>,</span><br><span class="line">                precision = <span class="number">2</span>, filled = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>生成对这个 tree.dot 文件还不能直接看，网上查了一下，把它输出来看看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line">img = pydotplus.graph_from_dot_file(<span class="string">'tree.dot'</span>)</span><br><span class="line"><span class="comment">#img.write_pdf('tree.pdf') #输出成PDF</span></span><br><span class="line">Image(img.create_png())</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/tree.png" alt><br>实际上这张图就解释来决策树的生成过程。一般我们认为最先分裂的特征越重要，但是从这张图我们并不能很直观的看出特征的重要性。</p><h5 id="6-2-Permutation-importance"><a href="#6-2-Permutation-importance" class="headerlink" title="6.2 Permutation importance"></a>6.2 Permutation importance</h5><p>我们换一个工具—<a href="https://www.kaggle.com/dansbecker/permutation-importance" target="_blank" rel="noopener">Permutation importance</a>. 其原理是依次打乱test_data中其中一个特征数值的顺序，其实就是做shuffle，然后观察模型的效果，下降的多的说明这个特征对模型比较重要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> eli5</span><br><span class="line"><span class="keyword">from</span> eli5.sklearn <span class="keyword">import</span> PermutationImportance</span><br><span class="line"></span><br><span class="line">perm = PermutationImportance(model, random_state=<span class="number">20</span>).fit(test_x, test_y)</span><br><span class="line">eli5.show_weights(perm, feature_names=test_x.columns.tolist())</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/feature_w.png" width="323" height="290"><br>一目了然，一切尽在不言中。还是说俩句吧，绿色越深表示正相关越强，红色越深表示负相关越强。<br>实际上我发现改变 PermutationImportance 的参数 random_state 的值结果变化挺大的，不过还是有几个特征位次变化不大，结果还是具有参考意义。</p><h5 id="6-3-Partial-Dependence-Plots"><a href="#6-3-Partial-Dependence-Plots" class="headerlink" title="6.3 Partial Dependence Plots"></a>6.3 Partial Dependence Plots</h5><p>我们试试另一个工具—<a href="https://www.kaggle.com/dansbecker/partial-plots" target="_blank" rel="noopener">Partial Dependence Plots</a>. 其原理和 Permutation importance 有点类似，当它判断一个特征对模型的影响时，对于所有样本，将该特征依次取该特征的所有取值，观察模型结果的变化。先画图，再根据图解释一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pdpbox <span class="keyword">import</span> pdp, info_plots</span><br><span class="line"></span><br><span class="line">total_features = train_x.columns.values.tolist()</span><br><span class="line">feature_name = <span class="string">'oldpeak'</span></span><br><span class="line">pdp_dist = pdp.pdp_isolate(model=model, dataset=test_x, model_features=total_features, feature=feature_name)</span><br><span class="line"></span><br><span class="line">pdp.pdp_plot(pdp_dist, feature_name)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b7.png" width="670" height="425"><br>上图的纵坐标是模型相对于base model 的变化，横坐标是该特征的所有取值，实线表示相对于base model 的变化的平均值，蓝色阴影表示置信度。oldpeak表示运动相对于休息引起的ST段压低，可以看到其取值越大，患心脏病的可能性越低。不知道这个结果可不可信，我觉得需要医学知识作支撑。</p><p>又试了几个特征：</p><p><strong>Sex：</strong><br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b8.png" width="670" height="425"><br>上图说明男性比女性患心脏病的概率要低些，网上查了一下，还真是这样。</p><p><strong>Age：</strong><br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b9.png" width="670" height="425"><br>上图表示60岁以上老人心脏病高发，这个和现有理论相符。</p><p>接下来看一下 <strong>2D Partial Dependence Plots</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inter = pdp.pdp_interact(model=model, </span><br><span class="line"> dataset=test_x, </span><br><span class="line"> model_features=total_features, </span><br><span class="line"> features=[<span class="string">'oldpeak'</span>, <span class="string">'age'</span>])</span><br><span class="line"></span><br><span class="line">pdp.pdp_interact_plot(pdp_interact_out=inter, </span><br><span class="line">  feature_names=[<span class="string">'oldpeak'</span>, <span class="string">'age'</span>], </span><br><span class="line">  plot_type=<span class="string">'contour'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://raw.githubusercontent.com/KK-Jiang/cloud_img/master/190407/b10.png" alt><br>这个图一开始没看懂，后来仔细看了<a href="https://www.kaggle.com/dansbecker/partial-plots" target="_blank" rel="noopener">Partial Dependence Plots</a> 的说明文档才搞明白。图中颜色从浅到深表示患心脏病概率降低，以最深的那个紫色为例，oldpeak &gt; 3.0 &amp;&amp; 45 &lt; age &lt; 65 时，患病概率最低，图中黄色部分表示，oldpeak &lt; 0.25 &amp;&amp;  ( age &lt; 45 || age &gt; 65 ) 时，患病概率最高。</p><h2 id="7-后记"><a href="#7-后记" class="headerlink" title="7. 后记"></a>7. 后记</h2><p>实际上本项目的数据是非常小的，其结果的可靠性也是值得怀疑的。但是通过这个项目，去经历机器学习项目的完整过程，却能学到很多东西。重要的是过程，更重要的是举一反三。该项目还引入了2个很有趣的Feature Importance Analysis的方法，对于我来说是新知识，也算是学到了。</p><p>这一篇到这里结束了，期待下一篇。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jjkkk.gitee.io/blog/categories/Machine-Learning/"/>
    
      <category term="项目实战" scheme="https://jjkkk.gitee.io/blog/categories/Machine-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="AI医疗" scheme="https://jjkkk.gitee.io/blog/tags/AI%E5%8C%BB%E7%96%97/"/>
    
      <category term="Machine Learning" scheme="https://jjkkk.gitee.io/blog/tags/Machine-Learning/"/>
    
      <category term="Random Forest" scheme="https://jjkkk.gitee.io/blog/tags/Random-Forest/"/>
    
      <category term="Feature Engineering" scheme="https://jjkkk.gitee.io/blog/tags/Feature-Engineering/"/>
    
  </entry>
  
</feed>
