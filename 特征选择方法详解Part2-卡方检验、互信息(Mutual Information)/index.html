<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>特征选择方法详解Part2-卡方检验、互信息(Mutual Information) | KK&#39;s Note</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="特征工程特征选择卡方检验互信息(Mutual Information)" />
  
  
  
    <meta name="baidu-site-verification" content="true" />
  
  
  <meta name="description" content="在前文《特征选择方法详解Part1-方差分析、Pearson、Spearman》中，详细总结了特征选择的基本方法专家推荐、方差分析和单变量相关性分析方法Pearson、Spearman方法。在本文中将延续Part1的行文结构，介绍单变量相关性分析方法中的卡方检验和互信息方法。">
<meta name="keywords" content="特征工程,特征选择,卡方检验,互信息(Mutual Information)">
<meta property="og:type" content="article">
<meta property="og:title" content="特征选择方法详解Part2-卡方检验、互信息(Mutual Information)">
<meta property="og:url" content="https:&#x2F;&#x2F;jkknotes.com&#x2F;%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3Part2-%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C%E3%80%81%E4%BA%92%E4%BF%A1%E6%81%AF(Mutual%20Information)&#x2F;index.html">
<meta property="og:site_name" content="KK&#39;s Note">
<meta property="og:description" content="在前文《特征选择方法详解Part1-方差分析、Pearson、Spearman》中，详细总结了特征选择的基本方法专家推荐、方差分析和单变量相关性分析方法Pearson、Spearman方法。在本文中将延续Part1的行文结构，介绍单变量相关性分析方法中的卡方检验和互信息方法。">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;jjkkk&#x2F;cloud_img&#x2F;raw&#x2F;master&#x2F;200317&#x2F;p2.jpeg">
<meta property="og:updated_time" content="2020-03-23T12:33:40.145Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;jjkkk&#x2F;cloud_img&#x2F;raw&#x2F;master&#x2F;200317&#x2F;p2.jpeg">
  
    <link rel="alternate" href="/atom.xml" title="KK&#39;s Note" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">

  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Oswald%3A300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/fashion.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >

</head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  


<header id="allheader" class="site-header" role="banner" 
   >
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="KK&#39;s Note" rel="home"> KK&#39;s Note </a>
            
          </h1>
          
          
            <div class="site-description">WRITE AND DO BETTER</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>

            <div class="clearfix sf-menu">
              <ul id="main-nav" class="menu sf-js-enabled sf-arrows"  style="touch-action: pan-y;">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">Tags</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">About</a> </li>
                    
              </ul>
            </div>
          </nav>

      </div>
  </div>
</header>


  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-特征选择方法详解Part2-卡方检验、互信息(Mutual Information)" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      <a class="article-gallery-img fancybox" href="https://gitee.com/jjkkk/cloud_img/raw/master/200317/p2.jpeg" target="_blank" rel="gallery_ck8binbvj0009rwvsfzr13y3m noopener">
        <img src="https://gitee.com/jjkkk/cloud_img/raw/master/200317/p2.jpeg" itemprop="image">
      </a>
    
  </div>
</div>

    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      特征选择方法详解Part2-卡方检验、互信息(Mutual Information)
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3Part2-%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C%E3%80%81%E4%BA%92%E4%BF%A1%E6%81%AF(Mutual%20Information)/" class="article-date">
	  <time datetime="2020-03-22T09:36:00.000Z" itemprop="datePublished">March 22, 2020</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="article-category-link" href="/categories/Machine-Learning/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a>
 
      
	<span id="busuanzi_container_page_pv">
	  VISITORS<span id="busuanzi_value_page_pv"></span>
	</span>


    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>在前文《特征选择方法详解Part1-方差分析、Pearson、Spearman》中，详细总结了特征选择的基本方法专家推荐、方差分析和单变量相关性分析方法Pearson、Spearman方法。在本文中将延续Part1的行文结构，介绍单变量相关性分析方法中的卡方检验和互信息方法。<a id="more"></a></p>
<h1 id="单变量分析"><a href="#单变量分析" class="headerlink" title="单变量分析"></a>单变量分析</h1><h2 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>卡方检验，又称 $\chi^2$ 检验，其用来衡量样本实际观测值与理论推断值之间的偏离程度。在特征工程计算相关性时，理论推断值可以理解为：假设此特征与目标变量（即label）无关，此特征的取值应该服从的分布。（不理解的话直接看下文中例子就明白了）。</p>
<p>先上公式（同样，不想看公式的话直接看下文中例子就明白了）：</p>
<p>$$\chi^2 = \sum_{i=1}^n\frac{(A-T)^2}{T}$$</p>
<p>其中A为实际观测值，T为理论推断值。从上式可以很清晰地看到，计算得到的 $\chi^2$ 值越大，相关性越高。</p>
<p>举个例子就很好明白了：假如现在有一批肿瘤内科患者的数据，想判断患者的收入水平（简单以穷人和富人代替）是否与患者患肠癌有关。简单制作下表表示真实观测值的分布：</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">肠癌患者</th>
<th align="left">非肠癌患者</th>
<th align="left">总计</th>
</tr>
</thead>
<tbody><tr>
<td align="left">穷人</td>
<td align="left">67</td>
<td align="left">83</td>
<td align="left">150</td>
</tr>
<tr>
<td align="left">富人</td>
<td align="left">102</td>
<td align="left">64</td>
<td align="left">166</td>
</tr>
<tr>
<td align="left">总计</td>
<td align="left">169</td>
<td align="left">147</td>
<td align="left">316</td>
</tr>
</tbody></table>
<p>在上表中四个频数就是卡方检验公式里的A，即A=[67, 83, 102, 64]。</p>
<p>在这个数据中，肠癌患者约占总患者人数的53.48%。如果我们假设患者的收入水平与其患肠癌无关，那理论上，表格应该如下：</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">肠癌患者</th>
<th align="left">非肠癌患者</th>
<th align="left">总计</th>
</tr>
</thead>
<tbody><tr>
<td align="left">穷人</td>
<td align="left">150x53.48% $\approx$ 80</td>
<td align="left">150x46.52% $\approx$ 70</td>
<td align="left">150</td>
</tr>
<tr>
<td align="left">富人</td>
<td align="left">166x53.48% $\approx$ 89</td>
<td align="left">166x46.52% $\approx$ 77</td>
<td align="left">166</td>
</tr>
<tr>
<td align="left">总计</td>
<td align="left">169</td>
<td align="left">147</td>
<td align="left">316</td>
</tr>
</tbody></table>
<p>那么这时候，理论推断值T=[80, 70, 89, 77]。</p>
<p>这下就很明白了，带入卡方检验公式得：</p>
<p>$$\chi^2 = \frac{(67-80)^2}{80} + \frac{(83-70)^2}{70} + \frac{(102-89)^2}{89} + \frac{(64-77)^2}{77} \approx 8.62$$</p>
<p>计算得到了 $\chi^2$ 值，如何判断“患者的收入水平与其患肠癌无关”的假设是否成立呢？聪明的人早已整理了卡方分布临界值表，根据 $\chi^2$ 值和其对应的自由度（自由度=(行数 - 1) * (列数 - 1)，如上表，行数和列数不计入“总计”列和行。）去表里查一下就可以了（见下图），这个例子中，8.62 &gt; 6.64, 就认为有1%的概率认为“患者的收入水平与其患肠癌无关”的假设成立，即有99%的置信度认为“患者的收入水平与其患肠癌有关”。</p>
<p><img src="https://gitee.com/jjkkk/cloud_img/raw/master/200317/p3.png" alt=""></p>
<p>上面例子中，使用的变量如收入水平，使用的是离散值，而且从公式可以看到，需要统计频数信息，显然连续变量不适合统计频数。那么问题来了，<strong>连续值如何使用卡方检验方法呢？</strong> 其实很简单，只需要将连续变量离散化就可以了，想想决策树算法在连续变量上怎么使用的，就是将连续变量的取值范围划分区间，这样每个连续变量的值都有一个区间，就变成离散变量了。使用卡方检验也是一样的道理。</p>
<h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>sklearn.feature_selection.chi2(X, y）可以计算 $\chi^2$ 值和对应的P-value，但是进行特征选择时，当然不是一个一个计算，而是批量计算。</p>
<p>sklearn.feature_selection提供了 SelectKBest和SelectPercentile方法，看名字就知道，SelectKBest选择前K个最好的特征，SelectPercentile则是按百分比选择特征。这2个方法可选择计算相关性的方法，具体可查看官方文档<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" target="_blank" rel="noopener">SelectKBest</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile" target="_blank" rel="noopener">SelectPercentile</a></p>
<p>这里给一个官方示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.shape</span><br><span class="line">(<span class="number">150</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_new = SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_new.shape</span><br><span class="line">(<span class="number">150</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="互信息（Mutual-Information）"><a href="#互信息（Mutual-Information）" class="headerlink" title="互信息（Mutual Information）"></a>互信息（Mutual Information）</h2><p>互信息这个方法，真正总结起来，远远比我想象的要复杂。</p>
<p>我仔细阅读了sklearn.feature_select中方法mutual_info_regression、mutual_info_classif的<a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_selection/_mutual_info.py" target="_blank" rel="noopener">源码</a>、sklearn.metrics中方法mutual_info_score、normalized_mutual_info_score、adjusted_mutual_info_score的<a href="https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/metrics/cluster/_supervised.py" target="_blank" rel="noopener">源码</a>和互信息的经典论文【A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E69, 2004.】，然后才弄清楚以下几点：</p>
<ul>
<li>mutual_info_regression、mutual_info_classif两种方法实现时的区别</li>
<li>sklearn.feature_select中互信息方法和sklearn.metrics中互信息方法的区别</li>
<li>当变量是连续变量的时候，如何计算的（我原先以为是将连续变量划分区间从而实现离散化，实际和我想象的远不一样）</li>
</ul>
<p>这一小节，我会对上面三个点进行总结性的介绍，如果有时间，我想我会专门写一篇文章详细介绍。</p>
<h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><h4 id="互信息（Mutual-Information）-1"><a href="#互信息（Mutual-Information）-1" class="headerlink" title="互信息（Mutual Information）"></a>互信息（Mutual Information）</h4><h5 id="定义一"><a href="#定义一" class="headerlink" title="定义一"></a>定义一</h5><p>了解决策树的生成过程的话，互信息的理解其实很简单。首先介绍一个概念：信息熵（information entropy）,按西瓜书的说法，其是度量样本集合纯度最常用的一种指标。看到“熵”（其物理含义是度量体系混乱程度的指标，值越大越混乱），再看到修饰词“信息”，我想它的含义已经很明确了。这里同样给出其在西瓜书上的公式：</p>
<p>$$Ent(D) = -\sum_{k=1}^{|Y|}p_k\log_2p_k$$</p>
<p>这里的D可以理解为一个变量（机器学习中可认为其是label，feature都可以定义为D）， $p_k$ 表示向量D中第k个取值所占的比例。Ent(D)值越小，则D纯度越高。</p>
<p>西瓜书中的信息增益（information gain）即是互信息。这里给出互信息的第一个公式(来自西瓜书，变量名修改了下)：</p>
<p>$$Gain(Y, X) = Ent(Y) - \sum_{v-1}^V\frac{|Y^v|}{|Y|}Ent(Y^v)$$</p>
<p>式中定义了变量Y和X之间的互信息，这里的 $v$ 表示X所有可能取值的编号， $Y_v$ 表示当 $x=x_v$ 时，其所对应的Y的值的集合， $|Y^v|$ 即表示此集合中的样本数量。举个例子吧，假如 $Y=[1,1,0,1], X=[1,2,1,1]$ ， 这时X的取值有2个：1和2，即 $v=[0, 1]$ , 当v=0, 即 $x=x_0=1$ 时， $|Y^v|=[1,0,1]$ , $|Y^v|=3$ .Gain(Y, X) 值越大，说明X和Y相关性越高。</p>
<p>从上面的公式和举例来看，明显X和Y是离散值。如果是连续值的话，可以将X和Y通过划分区间的方式离散化，具体可见西瓜书4.4节。</p>
<h5 id="定义二"><a href="#定义二" class="headerlink" title="定义二"></a>定义二</h5><p>直接上公式, 如果X和Y是离散变量：</p>
<p>$$MI(X,Y)=\sum_{i=1}^{|X|} \sum_{j=1}^{|Y|} \hat P(i,j)<br>        \log\frac{\hat P(i,j)}{P(i)P(j)}=\sum_{i=1}^{|X|} \sum_{j=1}^{|Y|} \frac{|X_i\cap Y_j|}{N}<br>        \log\frac{|X_i \cap Y_j|/N}{|X_i|/N \cdot |Y_j|/N}$$</p>
<p>如果X和Y是连续变量：</p>
<p>$$MI(X,Y)=\iint f(x,y) \log\frac{f(x,y)}{f(x)f(y)}$$</p>
<p>上式中 $f(x,y)$ 表示变量X和Y的联合概率密度函数， $f(x)$ 和 $f(y)$ 同理。话说这个式子，完全不知道怎么使用。</p>
<h5 id="定义三"><a href="#定义三" class="headerlink" title="定义三"></a>定义三</h5><p>直接上公式（来自于我看网上没人写过这个公式，因为很少有人懂吧）：</p>
<p>$$MI(X,Y)=\Psi(k)-&lt;\Psi(n_x+1)+\Psi(n_y+1)&gt;+\Psi(N)$$</p>
<p>上式 $\Psi$ 函数是 Gamma函数的导数，scipy.special.digamma可实现其计算；另外 $\lt X\gt=N^{-1}\sum_{i=1}^Nx_i$ ，上式中k是k近邻算法的参数， $n_x$ 表示使用X拟合一个k近邻算法后，给定中心点和半径，所有在半径内对数据的个数（我看sklearn上源码是这样算的，论文中也说了，但是没说实现细节）， $n_x$ 同理，N表示样本数。</p>
<p>这个公式引用自互信息的经典论文【A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E69, 2004.】。其从公式二推导而来（推导挺复杂，没看懂）。</p>
<p>为什么要写这个公式呢，sklearn中mutual_info_regression、mutual_info_classif方法在实现时，如果X和Y其中出现连续变量，则使用上式计算的互信息。</p>
<h4 id="Normalized-Mutual-Information"><a href="#Normalized-Mutual-Information" class="headerlink" title="Normalized Mutual Information"></a>Normalized Mutual Information</h4><p>Normalized Mutual Information将互信息归一化到0和1之间，0表示不相关，1表示很完美的相关，这样就更方便比较不同的feature和label的相关程度大小。如下为其公式：</p>
<p>$$NMI(X,Y)=\frac{MI(X,Y)}{F(H(X),H(Y))}, H(X)=-Ent(X)$$</p>
<p>上式中 $F(H(X),H(Y))$ F表示函数，其可以是均值(mean)、最大值(max)、最小值(min)、均方根函数(sqrt)</p>
<h4 id="Adjusted-Mutual-Information"><a href="#Adjusted-Mutual-Information" class="headerlink" title="Adjusted Mutual Information"></a>Adjusted Mutual Information</h4><p><strong>不管是NMutual Information还是Normalized Mutual Information都有一个缺点：对于离散变量X和Y，如果X和Y的取值越多，那么MI（X,Y）和NMI（X,Y）都倾向于变得更大，而这并不代表X和Y的相关性变得更强</strong>。于是，Adjusted Mutual Information应运而生，其公式如下：</p>
<p>$$AMI(X, Y)=\frac{MI(X, Y) - E(MI(X, Y))}{F(H(X), H(Y)) - E(MI(X, Y))}$$</p>
<p>至于上式中的 $E(MI(X, Y))$ 的定义，由于太长且很复杂，这里就不写了（主要自己没理解[捂脸]），给个维基百科的链接吧，有兴趣的朋友可以fan&amp;@¥墙看一下<a href="https://en.wikipedia.org/wiki/Adjusted_mutual_information" target="_blank" rel="noopener">Adjusted Mutual Information</a>）。</p>
<h3 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h3><ul>
<li><p>不管是NMutual Information还是Normalized Mutual Information都有一个缺点：对于离散变量X和Y，如果X和Y的取值越多，那么MI（X,Y）和NMI（X,Y）都倾向于变得更大，而这并不代表X和Y的相关性变得更强。所以现在使用更多的是Adjusted Mutual Information；</p>
</li>
<li><p>sklearn中mutual_info_regression方法在计算互信息时使用的是定义三；</p>
</li>
<li><p>sklearn中mutual_info_classif方法在计算互信息时，如果X和Y其中出现连续变量，使用定义三计算互信息，如果都是离散变量，则直接调用sklearn.metrics.mutual_info_score方法，即定义二中的第一个公式；</p>
</li>
<li><p>sklearn.metrics中方法mutual_info_score、normalized_mutual_info_score、adjusted_mutual_info_score本意是评价聚类算法效果的，所以这三种算法只支持离散变量。即使输入连续变量也会按照离散变量去计算，导致不正确的结果。</p>
</li>
</ul>
<h3 id="使用示例-1"><a href="#使用示例-1" class="headerlink" title="使用示例"></a>使用示例</h3><p>sklearn.feature_select中方法mutual_info_regression、mutual_info_classif的使用可见上节卡方检验的使用示例（只需要将方法chi2改成mutual_info_regressionu或mutual_info_classif即可）。</p>
<p>这里给一下sklearn.metrics中方法mutual_info_score、normalized_mutual_info_score、adjusted_mutual_info_score的示例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mutual_info_score, normalized_mutual_info_score, adjusted_mutual_info_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rng = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y1 = rng.randint(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y2 = rng.randint(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y2[:<span class="number">20</span>] = y1[:<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"MI"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(mutual_info_score(y1, y2), mutual_info_score(y1 % <span class="number">3</span>, y2 % <span class="number">3</span>))</span><br><span class="line"><span class="number">0.5305136007637552</span>, <span class="number">0.026551556693229707</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"NMI"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(normalized_mutual_info_score(y1, y2), normalized_mutual_info_score(y1 % <span class="number">3</span>, y2 % <span class="number">3</span>))</span><br><span class="line"><span class="number">0.2381306291604139</span>, <span class="number">0.0252301499426718</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"AMI"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(adjusted_mutual_info_score(y1, y2), adjusted_mutual_info_score(y1 % <span class="number">3</span>, y2 % <span class="number">3</span>))</span><br><span class="line"><span class="number">0.03389853740628566</span>, <span class="number">0.0054476031181075295</span></span><br></pre></td></tr></table></figure>

<h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>一下午码了这么多，肯定有一些错字或者遗漏，以后发现再改吧。</p>
<p>写到这里，单变量相关性分析方法就介绍完了。基于模型的特征选择方法可继续看Part3。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="article-category-link" href="/categories/Machine-Learning/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%BA%92%E4%BF%A1%E6%81%AF-Mutual-Information/" rel="tag">互信息(Mutual Information)</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/" rel="tag">卡方检验</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" rel="tag">特征选择</a></li></ul>

    </footer>
    <hr class="entry-footer-hr">
  </div>
      
        
        <div id="vcomments"></div>
		<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    		<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
        <script>
            new Valine({
	    	av: AV,
                el: '#vcomments',
                appId: '2D30CEQ8V1SWbAhEuhAVDLff-gzGzoHsz',
                appKey: 'OoybxmzBIBcKIlDStoAh6dmV',
		placeholder: 'email可选，填上昵称评论吧',
		avatar: 'monsterid',
		meta: ['nick', 'mail']
            })
	    //增加以下六行代码去除 power by valine
    	var infoEle = document.querySelector('#vcomments .info');
    	if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      		infoEle.childNodes.forEach(function(item) {
        		item.parentNode.removeChild(item);
      		});
    	}	
        </script>


      
  
    
<nav id="article-nav">
  
    <a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3Part3-SelectFromModel-%20RFE%E3%80%81L1%E3%80%81Tree%E3%80%81Permutation%20importance/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          特征选择方法详解Part3-SelectFromModel- RFE、L1、Tree、Permutation importance
        
      </div>
    </a>
  
  
    <a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3Part1-%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90%E3%80%81Pearson%E3%80%81Spearman/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">特征选择方法详解Part1-方差分析、Pearson、Spearman</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#单变量分析"><span class="nav-number">1.</span> <span class="nav-text">单变量分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#卡方检验"><span class="nav-number">1.1.</span> <span class="nav-text">卡方检验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原理"><span class="nav-number">1.1.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用示例"><span class="nav-number">1.1.2.</span> <span class="nav-text">使用示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#互信息（Mutual-Information）"><span class="nav-number">1.2.</span> <span class="nav-text">互信息（Mutual Information）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原理-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#互信息（Mutual-Information）-1"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">互信息（Mutual Information）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#定义一"><span class="nav-number">1.2.1.1.1.</span> <span class="nav-text">定义一</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#定义二"><span class="nav-number">1.2.1.1.2.</span> <span class="nav-text">定义二</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#定义三"><span class="nav-number">1.2.1.1.3.</span> <span class="nav-text">定义三</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Normalized-Mutual-Information"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Normalized Mutual Information</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adjusted-Mutual-Information"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Adjusted Mutual Information</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Notice"><span class="nav-number">1.2.2.</span> <span class="nav-text">Notice</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用示例-1"><span class="nav-number">1.2.3.</span> <span class="nav-text">使用示例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#后续"><span class="nav-number">2.</span> <span class="nav-text">后续</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2020 KK&#39;s Note All Rights Reserved.&nbsp;&nbsp;
        
            <span id="busuanzi_container_site_uv">
              Unique Visitor: <span id="busuanzi_value_site_uv"></span>&nbsp;&nbsp;  
              Page View: <span id="busuanzi_value_site_pv"></span>
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hipaper" target="_blank">hipaper</a>
      </div>
  </div>
  <script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");

    wrapdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";


    <!-- headerblur min height -->
    
    
</script>

    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
