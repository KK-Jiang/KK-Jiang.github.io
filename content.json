{"meta":{"title":"KK's Note","subtitle":null,"description":"WRITE AND DO BETTER","author":"KK.J","url":"https://jkknotes.com","root":"/"},"pages":[{"title":"共勉","date":"2019-10-25T14:13:21.000Z","updated":"2019-10-25T14:54:58.246Z","comments":true,"path":"about/index.html","permalink":"https://jkknotes.com/about/index.html","excerpt":"","text":"我々が岩壁の花を美しく思うのは我々が岩壁に足を止めてしまうからだ恐れ悚れ无き その花のように空へと踏み出せずにいるからだ我们之所以觉得悬崖上的花朵美丽那是因为我们会在悬崖停下脚步而不是像那些毫不畏惧的花朵般能向天空踏出一步。——BLEACH.12"},{"title":"categories","date":"2019-10-25T14:13:21.000Z","updated":"2019-10-25T14:39:14.508Z","comments":true,"path":"categories/index.html","permalink":"https://jkknotes.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-10-25T14:13:06.000Z","updated":"2019-10-25T14:38:09.163Z","comments":true,"path":"tags/index.html","permalink":"https://jkknotes.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kaggle《SIIM-ACR Pneumothorax Segmentation》第一名方案详解","slug":"kaggle《SIIM-ACR Pneumothorax Segmentation》第一名方案详解","date":"2020-01-16T11:30:29.000Z","updated":"2020-01-16T15:20:54.876Z","comments":true,"path":"kaggle《SIIM-ACR Pneumothorax Segmentation》第一名方案详解/","link":"","permalink":"https://jkknotes.com/kaggle%E3%80%8ASIIM-ACR%20Pneumothorax%20Segmentation%E3%80%8B%E7%AC%AC%E4%B8%80%E5%90%8D%E6%96%B9%E6%A1%88%E8%AF%A6%E8%A7%A3/","excerpt":"当初刷微博无意中看到这个比赛，内容和自己所在团队的工作内容很契合，就花了点时间学习了下第一名的方案。在这里把自己学习的成果记录下来，也希望和大家一起讨论学习。","text":"当初刷微博无意中看到这个比赛，内容和自己所在团队的工作内容很契合，就花了点时间学习了下第一名的方案。在这里把自己学习的成果记录下来，也希望和大家一起讨论学习。 1 Introduction引用一段比赛页面对这个比赛的介绍： “Pneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying—it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event. Pneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more confident diagnosis for non-radiologists.” 比赛地址：SIIM-ACR Pneumothorax Segmentation Data: 10679份dicom文件（站立位胸片），有气胸：无气胸 = 2379:8300 有气胸的胸片皆有mask, run-length-encoded (RLE)格式 一个胸片如果有多处气胸，会有多个单独的mask Evaluation: Dice coefficient: $$\\frac {2*|X \\bigcap Y|}{|X|+|Y|}$$ 2 OverView用一张图来表示作者的思路： 作者在训练模型时，采用5折交叉验证，并将模型训练分成了4个阶段（下文中分别表示成part0，part1，part2，part3），每个阶段都是上图中这样一个完整的过程，后一阶段直接在前一阶段的最优模型上fine-tuning.每一个阶段有不同的数据构成和模型参数。 3 Input Data3.1 Data Augmentation这里模型的输入是1024X1024X3的胸片和1024X1024X1的mask。作者直接使用了图像增强库 albumentations 对数据进行增强，作者使用了一个较为复杂的方案：以固定顺序对图像进行不同的变换，并且给予每种方法一定的概率，使增强方法的运用随机化。具体如下: HorizontalFlip：0.5 OneOf：0.3 RandomContrast：0.5 RandomGamma：0.5 RandomBrightness：0.5 OneOf：0.3 ElasticTransform：0.5 GridDistortion：0.5 OpticalDistortion：0.5 ShiftScaleRotate：0.5 上面小数表示此增强方法运用的概率，OneOf表示在其子方法中选择一个。 需要注意的是，这里有2个OneOf，第一个OneOf下面的增强方法主要对图像的亮度、对比度等进行调整，而第二个OneOf下面的增强方法主要对图像的形状进行调整。可以看几个例子。 因为训练模型分成四个阶段，每个阶段使用的数据是一样的，使用一定概率给图像做增强，实际上保证了每个阶段实际参加训练的数据都不完全一样。 3.2 Sliding Sample对无气胸的样本随机下采样，使有气胸的样本占0.8（part0），0.6（part1），0.4（part2），0.5（part3） 4 Model4.1 Model Zoo Albunet: UNet (论文链接) with Resnet34(论文链接) encoder proposed by Alexander Buslaev UNet简直是图像分割的神器，特别是在医疗图像分割的上，目前其各种变体网络仍是各种比赛的主力。Unet主要有2个特点：1.U型结构；2.skip connection. – U型结构encoder的下采样和decoder的上采样的次数相同，这就保证了模型的输出恢复到原图片的分辨率，实现端到端的预测。 – skip connection的结构使模型结合不同level的feature map上进行学习，相比于FCN分割边缘更清晰。 同时Albunet的网络的encoder使用的是ResNet，其由何凯明大佬于2015年提出（Unet也是这一年提出，回过头看，这一年真的是丰收的一年），同样风靡至今。从理论上讲，越高级的特征，应该有越强的表征能力，而VGG网络证明，网络的深度对特征的表达能力至关重要。 理想情况下，当我们直接对网络进行简单的堆叠到特别长，网络内部的特征在其中某一层已经达到了最佳的情况，这时候剩下层应该不对改特征做任何改变，自动学成恒等映射的形式。也就是说，对一个特别深的深度网络而言，该网络的浅层形式的解空间应该是这个深度网络解空间的子集，但实际上，如果使用简单是网络堆叠，由于网络性能衰减，网络的效果反而越差。为了深度网络后面的层至少实现恒等映射的作用，作者提出了residual模块。 除了Albunet，作者还尝试了如下2种网络进行实验： Resnet50: GitHub链接 SCSEUnet：也就是SENet((论文：squeeze-and-excitation network)) 4.2 Loss Function作者使用BCE，Dice和Focal loss加权的方式作为最终的损失函数： $$Loss = W_1 * StableBCELoss + W_2 * DiceLoss + W_3 * FocalLoss2d()$$ 这里需要注意的是，损失函数StableBCELoss的输入是没有进行sigmoid计算的模型输出，而其他2个损失函数的输入是经过sigmoid计算的模型输出。作者使用这三个损失函数，是想从三个维度驱动网络学习。 – Focal loss主要解决正负样本失衡的问题，在医学图像分割的这种像素级分类的任务中，往往都是正样本较少，负样本较多，气胸的分割同样不例外。其公式如下： 从如上公式上可以看到，某个样本输出的概率越高，其产生的loss越小，实际上达到了促使网络学习困难样本的目的（在分割任务中，目标像素就是困难样本，因为其数量较少）。 – Dice loss在很多关于医学图像分割的竞赛、论文和项目中出现的频率很高，此比赛的评价指标也是它。从前文它的公式可以看到，其表示的是预测的轮廓与真实的mask的相似程度。其公式如下： $$DiceLoss = 1 - \\frac {2*|X \\bigcap Y|}{|X|+|Y|}$$ – StableBCELoss, 这里非常奇怪的是，其输入是没有进行sigmoid计算的模型输出，至今没想通（to do：等搞明白了补上）。这里附上pytorch文档中其公式： 最终作者寻优，得到各个损失函数的权重如下： (3,1,4) for albunet_valid and seunet; (1,1,1) for albunet_public; (2,1,2) for resnet50. 4.3 Learn Rate Scheduler作者在不同的训练阶段，采用了不同的学习率调整策略，具体如下： Phase Start Lr Scheduler part0 1e-4 ReduceLROnPlateau part1 1e-5 ReduceLROnPlateau or CosineAnnealingWarmRestarts part2 1e-5 ReduceLROnPlateau or CosineAnnealingWarmRestarts part3 1e-6 ReduceLROnPlateauor CosineAnnealingWarmRestarts 4.3.1 torch.optim.lr_scheduler.ReduceLROnPlateau其基本思想是：当参考的评价指标停止变优时,降低学习率，挺实用的方法。 Code Example: 1234567optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)scheduler = ReduceLROnPlateau(optimizer, 'min')for epoch in range(10): train(...) val_loss = validate(...) # Note that step should be called after validate() scheduler.step(val_loss) 4.3.2 torch.optim.lr_scheduler.CosineAnnealingLR可以翻译为余弦退火调整学习率：根据论文SGDR: Stochastic Gradient Descent with Warm Restarts而来，只包含论文里余弦退火部分，并不包含restart部分。 当使用SGD时，模型的Loss应越来约接近全局最小值。当它逐渐接近这个最小值时，学习率应该变得更小来使得模型尽可能接近这一点。 从上图可以看出，随着x的增加，余弦值首先缓慢下降，然后加速下降，再次缓慢下降。这种下降模式能和学习率配合，比较有效的训练模型。 4.3.3 torch.optim.lr_scheduler.CosineAnnealingWarmRestartsSGDR：这个方法就是论文SGDR: Stochastic Gradient Descent with Warm Restarts的完整实现。 在训练时，梯度下降算法可能陷入局部最小值，而不是全局最小值。做随机梯度下降时可以通过突然提高学习率，来“跳出”局部最小值并找到通向全局最小值的路径。具体可以看下图： 5 Post Proprecess我们训练的分割模型输出每个像素的0-1概率，然后卡一下阈值，我们可以称这样的mask为basic sigmoid mask, 实际上医学图像中我们的分割目标也许并不存在，所以常用双重阈值（top_score_threshold, min_contour_area）的方法计算出mask并同时判断是否有分割目标（在本次比赛中我们分割目标是气胸），这种方法且称为doublet。其具体逻辑为：当大于概率阈值top_score_threshold的pixel数少于min_contour_area，就将mask像素值全部置0，也就是认为此胸片没有气胸。简单画个示意图如下： 而作者在此基础上作了改进，使用了三重阈值（top_score_threshold, min_contour_area, bottom_score_threshold）的方法来达到相同的目标，且称改进后的方法为Triplet.其具体逻辑为：当大于概率阈值top_score_threshold的pixel数少于min_contour_area，就将此mask pixel值全部置0，也就是认为此胸片没有气胸，然后再使用阈值bottom_score_threshold产生真正的mask。简单画个示意图如下： 最终作者通过搜索，分别获得了在validation和在Public Leaderboard上的最优参数： Best triplet on validation: (0.75, 2000, 0.3). Best triplet on Public Leaderboard: (0.7, 600, 0.3) 最后再附上作者的代码： 12345classification_mask = predicted &gt; top_score_thresholdmask = predicted.copy()mask[classification_mask.sum(axis=(1,2,3)) &lt; min_contour_area, :,:,:] = np.zeros_like(predicted[0])mask = mask &gt; bot_score_thresholdreturn mask 6 Train and Inference6.1 Train前面就已经提到过，作者将训练过程分为了4个阶段，这里在最前面新加一个预训练阶段。梳理如下： Phase Sample rate Start lr Scheduler Epochs Note part pre The model be pretrained on our dataset with lower resolution (512x512) part0 0.8 1e-4 ReduceLROnPlateau 10-12 The goal of this part: quickly get a good enough model with validation score about 0.835 part1 0.6 1e-5 ReduceLROnPlateau or CosineAnnealingWarmRestarts Repeat until best convergence uptrain the best model from the previous step part2 0.4 1e-5 ReduceLROnPlateau or CosineAnnealingWarmRestarts Repeat until best convergence uptrain the best model from the previous step part3 0.5 1e-6 ReduceLROnPlateauor CosineAnnealingWarmRestarts Repeat until best convergence uptrain the best model from the previous step 6.2 Inference作者采用五折交叉验证训练模型，并选择每一个fold的top3个模型的结果求平均输出最终的mask。这里的求平均每个像素点的概率求平均。模型的最终效果如下表： NOTE： albunet_public - best model for Public Leaderboard albunet_valid - best resnet34 model on validation seunet - best seresnext50 model on validation resnet50 - best resnet50 model on validation","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://jkknotes.com/categories/Deep-Learning/"},{"name":"项目实战","slug":"Deep-Learning/项目实战","permalink":"https://jkknotes.com/categories/Deep-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"AI医疗","slug":"AI医疗","permalink":"https://jkknotes.com/tags/AI%E5%8C%BB%E7%96%97/"},{"name":"Pneumothorax Segmentation","slug":"Pneumothorax-Segmentation","permalink":"https://jkknotes.com/tags/Pneumothorax-Segmentation/"}]},{"title":"awk命令详解","slug":"awk命令详解","date":"2019-11-13T13:42:36.000Z","updated":"2019-11-15T12:29:35.309Z","comments":true,"path":"awk命令详解/","link":"","permalink":"https://jkknotes.com/awk%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/","excerpt":"awk 其名称的由来很简单，就是由其三个创始人Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母组成。很多人都说它是一个效率神器，以前也多多少少用过它一些简单的用法。后来python一上手，pandas一用上，反而忘了它。正所谓技多不压身，这么高大上的程序语言（速度快，常用命令简单， 效率高），一定要学会它。所以这篇文章特将awk的基本语法，常用的命令以及相对应的命令记录下来，以备需要时直接复制使用。","text":"awk 其名称的由来很简单，就是由其三个创始人Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母组成。很多人都说它是一个效率神器，以前也多多少少用过它一些简单的用法。后来python一上手，pandas一用上，反而忘了它。正所谓技多不压身，这么高大上的程序语言（速度快，常用命令简单， 效率高），一定要学会它。所以这篇文章特将awk的基本语法，常用的命令以及相对应的命令记录下来，以备需要时直接复制使用。 awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。awk脚本通常用来格式化文本文件中的信息。awk是以文件的每一行为处理单位的，即其对所接收文件或者其他文本内容的一行执行相应的命令，来处理文本。 1.基本格式1awk '&#123;命令&#125;' file1, file2, ... 或者 1其他命令的输出 ｜ awk '&#123;命令&#125;' 举个例子： 1awk '&#123;print $1&#125;' demo.txt awk的命令一般写在花括号里，但是花括号并不是必须的，某些情况下它可以省略，或者压根不需要。下面看具体讲解和实例。 2.基本用法2.1 awk内置变量前面说了，awk一般以行为单位进行处理，下表中的记录行即表示文件的一行，这是默认情况下以换行符作为判断为一行的标志，当然也可以手动修改以其他字符作为换行标志。下表中的字段可以理解为使用分隔符分隔记录行后的每一个元素。以下是awk内置变量及其含义： 变量名 含义 $0 当前记录行 $1 ~ $n 当对当前记录行进行分隔时，表示分隔后第几个字段 FS 字段分隔符，默认是空格 RS 记录行分隔符，默认为换行符 NF 对记录行进行分隔后，字段的个数 NR 已经读出对记录数，从1开始，多个文件时继续累加计数 FNR 已经读出对记录数，从1开始，多个文件时每个文件单独计数 ORS 输出时的记录行分隔符， 默认是换行符 OFS 输出时的字段分隔符， 默认是空格 2.2 $n、-F、FS、RS首先看 $0 ~ $n 和 -F 的例子： 12345678910&gt;&gt; cat awk_test.txta,b,c,de,f,g,h&gt;&gt; awk '&#123;print $0&#125;' awk_test.txt # 实例1a,b,c,de,f,g,h&gt;&gt; awk -F ',' '&#123;print $1,$2,$3,$4&#125;' awk_test.txt # 实例2a b c de f g h 这里的 ‘-F’ 命令用于定义按什么进行分隔，功能同python里的split。‘$0’ 表示整个记录行（实例1），‘$1,$2,$3,$4’表示分隔后的每个元素。 再继续来看FS和RS： 123456789&gt;&gt; awk -F ',' '&#123;print $1 FS $2,$3,$4&#125;' awk_test.txt # 实例3a,b c de,f g h&gt;&gt; awk -F ',' '&#123;print $1 RS $2,$3,$4&#125;' awk_test.txt # 实例4ab c def g h 实例3中，FS的值已经由‘-F’命令赋值成‘，’了；实例4中，RS默认为换行符，所以输出结果进行了换行。 如果想用多个分隔符分隔记录行，只需要将多个分隔符放在 [分隔符1 隔符2 ..] 中，如果想用一个或者多个相同分隔符进行分隔，可以写成 [分隔符1 隔符2 ..]+ 。继续看例子： 12345&gt;&gt; echo 'I am Poe,my qq is 0000001' | awk -F '[\" \",]' '&#123;print $3 \" \" $7&#125;'Poe 0000001&gt;&gt; echo 'I am Poe,,my qq is 0000001' | awk -F '[\" \",]+' '&#123;print $3 \" \" $7&#125;'Poe 0000001 不解释了，已经很清楚了。 2.3 NF、NR、FNR先看 NF和NR： 1234567&gt;&gt; cat awk_test.txta,b,c,de,f,g,h&gt;&gt; awk -F ',' '&#123;print NR\") \"$0\"\\t size is \"NF&#125;' awk_test.txt1) a,b,c,d size is 42) e,f,g,h size is 4 可以看到，结果中有行号，即为NR， 输出结果中的 4 即为字段数。再看以下FNR及其和NR的区别： 12345678910111213141516171819&gt;&gt; cat file.txtaaaaaabbbbbb&gt;&gt; awk -F ',' '&#123;print NR\") \"$0&#125;' awk_test.txt file.txt1) a,b,c,d2) e,f,g,h3) aaaaaa4) bbbbbb&gt;&gt; awk -F ',' '&#123;print FNR\") \"$0&#125;' awk_test.txt file.txt1) a,b,c,d2) e,f,g,h1) aaaaaa2) bbbbbb&gt;&gt; awk -F ',' '&#123;if (NR==FNR) print $0&#125;' awk_test.txt file.txta,b,c,de,f,g,h 很清楚了吧 2.4 OFS、ORS同样的看个例子： 123456789&gt;&gt; echo '1 2 3' | awk 'BEGIN &#123;OFS=\"|\"&#125; &#123;print $1,$2,$3&#125;' # 实例51|2|3&gt;&gt; cat awk_test.txt | awk -F ',' '&#123;print $1$2&#125;'abef&gt;&gt; cat awk_test.txt | awk -F ',' 'BEGIN &#123;ORS=\"---\"&#125; &#123;print $1$2&#125;' # 实例6ab---ef--- 这里出现了一个新的语法，BEGIN，其作用是定义在命令执行之前需要执行的操作，在实例5中，在命令执行之前将输出时的字段分隔符赋值成了‘｜’。实例6同理。 2.5 BEGIN、END 命令、运算符2.5.1 BEGIN 和 END 命令在许多编程情况中，需要在 awk 开始处理输入文件中的文本之前执行初始化操作。这个时候 可以定义一个 BEGIN 块。 因为 awk 在开始处理输入文件之前会执行 BEGIN 块，因此它是初始化 各种变量（包括内置变量，全局变量）的极佳位置。相对应的，awk 还提供了另一个特殊块，叫作 END 块。 awk 在处理了输入文件中的所有行之后执行这个块。通常， END 块用于执行最终计算或打印应该出现在输出流结尾的摘要信息。 下面这个例子的作用是统计某个文件夹下的文件占用的字节数： 12&gt;&gt; ll |awk 'BEGIN &#123;size=0&#125; &#123;size=size+$5&#125; END&#123;print \"[end]size is \",size/2014/1024, \"M\"&#125;'[end]size is 3.3 M 这样一个简单的例子信息量还是挺丰富的。首先可以看到所有的命令都在 ‘ ’ 里，BEGIN and END 以及记录行正常的操作部分，都有单独的代码块，用 {} 区分。另外我们可以看到，awk支持自定义变量，还可以进行各种运算。实际上awk也是一个轻量级的编程语言。这里代码“BEGIN {size=0}”可以省略，awk默认size=0. 2.5.2 awk运算符既然awk也算是一个编程语言，那自然支持各种运算。如下表所示： 运算符 解释 =、+、-、+=、-=、=、/=、%=、*=、++、–等 赋值及数学运算 ||、&amp;&amp; 逻辑或、逻辑与 &lt;、&lt;=、&gt;、&gt;=、!=、== 关系运算符 ?: 三目运算符 in 数组中是否存在某键值 随便举个例子，其余的看看就好： 1234&gt;&gt; awk 'BEGIN&#123;a=\"b\";print a==\"b\"?\"ok\":\"err\"&#125;'ok&gt;&gt; awk 'BEGIN&#123;a=\"b\";print a==\"c\"?\"ok\":\"err\"&#125;'err 2.5.3 awk 数组和循环awk的循环包含while、do…while、for循环。这里不介绍了。具体可看链接：https://www.cnblogs.com/ginvip/p/6352157.html 说说数组。awk的数组像极了python里的dict。有数组的地方，常常也有循环，当然这不是绝对的。这次的实例使用一个真实的log文件，先看看log前2行长什么样（涉及到实际业务，做了一些处理）： 123&gt;&gt; cat awk_log.log | head -n 22019-10-22 18:11:42,726 feature_name_unify.py[line:99] ERROR: d_type, origin_name, sample_type, exam_name: exam,awk,linux,awk命令简介2019-10-22 18:11:42,727 feature_name_unify.py[line:99] ERROR: d_type, origin_name, sample_type, exam_name: exam,grep,linux,awk命令简介1 整个log文件有0.18亿行。现在想统计按逗号分隔后倒数第三个位置会出现哪几种字符，及其频数。为了简单起见，用前20行做统计： 123456&gt;&gt; cat awk_log.log | head -n 20 | awk -F ',' '&#123;a[$6]++&#125; END &#123;for (x in a) print x \"\\t\" a[x]&#125;'awk 7grep 6seed 3ls 3pwd 1 这个例子先将这个log文件的前20行输出，以便awk处理；在使用awk处理时，先使用逗号进行分隔；后面对记录行的每一个操作都写在单引号 ‘’ 里，每一个{}是一个代码块；对于代码块 {a[$6]++} ，a表示一个数组，这个数组可以理解为python里的dict，$6是其索引；初始时，a[$6] = 0，后面随着处理的记录行增加，每来一个相同的索引，就加1；END {for (x in a) print x “\\t” a[x]}在处理完所有记录行后执行，此时用一个循环，打印出数组a的索引及其值。就如同打印出python字典里的key和value。 再来一个更复杂的例子，统计一个公司每个部门的人数及具体姓名： 1234567891011&gt;&gt; cat company.csv | head -n 3部门,姓名,性别技术研发部,张三,男财务部,韩梅梅,女&gt;&gt; cat company.csv | head -n 16 ｜ awk -F ',' '&#123;if (NR==1) next&#125;&#123;a[$1]++;b[$1]=b[$1]\",\"$2&#125; END &#123;for (x in a) print x \"\\t\" a[x] \"\\t\" b[x&#125;'技术研发部 5 ,张三,李四,王五,李雷,tom财务部 4 ,张一,李二,王三,Bob商务部 3 ,张二,李三,王四战略部门 2 ,张五,王六总裁办 1 ,三一 这里新出现了一个语法 ‘next’， 它的作用等同于 continue。这个命令出现，就不会执行后面的命令了。这里它的作用是跳过第一行，因为第一行是csv文件的header。这个例子看起来毫无意义，但是在实际处理中，特别是一些log文件，数据量几千万到亿级，用python处理代码写起来也很简单，但是论速度，肯定远远不及awk。awk处理起来很快，比常用的编程语言都要快。 3. awk的正则表达式正则表达式，不管什么语言，语法都是基本一样的。在实际工作中也用过不少，却始终不够熟练，甚至有点蒙。awk我也是如此。这一部分 不详细讲解了（觉得自己讲不明白），但是写2个简单例子，也算对得起这一节。下图是awk正则表达式：awk的正则表达式写在2个 / 之间： 1&gt;&gt; ls -l | awk '/^d/&#123;print $9&#125;' 这个例子是输出当前目录下所有的文件夹的名称（不写结果了，可自行尝试看结果）。 1&gt;&gt; awk -F ':' '$5~/root/&#123;print $0&#125;' /etc/passwd 这个例子是以分号作为分隔符，匹配第五个字段包含‘root’的行 4. 多文件操作及awk运行shell命令4.1 awk 多文件操作文章开始的时候就说过，awk的基本像是如下： 1awk '&#123;命令&#125;' file1, file2, ... 实际上awk是按文件循序逐行读取的，读完file1， 接着读取file2，以此类推.多文件处理的一个常用操作就是多文件特定内容的merge。下面用2个例子演示一下，横向合并2个文件和根据某一列或者多列合并2个文件： 1234567891011121314151617&gt;&gt; cat awk_test.txta,b,c,de,f,g,h&gt;&gt; cat awk_test1.txt2,3,4,a1,2,3,e# 横向合并2个文件&gt;&gt; awk -F ',' 'NR==FNR&#123;a[NR]=$0;next&#125;&#123;if (a[FNR]) print a[FNR] FS $0&#125;' awk_test.txt awk_test1.txt # 实例7a,b,c,d,2,3,4,ae,f,g,h,1,2,3,e# 根据某一列合并2个文件&gt;&gt; awk -F ',' 'NR==FNR&#123;a[$1]=$0;next&#125;&#123;if (a[$4]) print a[$4] FS $1 FS $2 FS $3&#125;' awk_test.txt awk_test1.txt # 实例8a,b,c,d,2,3,4e,f,g,h,1,2,3 这2个例子中的简单命令直接实现了Pandas的merge、concat和join操作，关键是你可以不用安装任何包，配置任何环境。更可贵的是，速度非常快。通常，使用 NR==FNR 条件判断是否正在读取的在第一个文件。在实例7中，记录行数为索引，将awk_test.txt文件的数据存在数组里，当读到第二个文件，对于每一个记录行，先打印数组里存的第一个文件的数据，再打印第二个文件的内容，就实现了横向拼接。实例8原理基本相同。 4.2 awk 运行shell命令使用awk运行shell命令的基本格式是： 1awk '&#123;system(\"shell 命令\")&#125;' 需要注意一点的是system里的shell命令需要写成字符串，即写在双引号里。来一个最简单的例子： 123&gt;&gt; awk '&#123;system(\"echo hello world\")&#125;' awk_test.txthello worldhello world 可引导echo 命令运行成功。接下来再看一个很实用的昨天，批量修改文件名.下面这个例子给除了black_test和已有后缀以外的文件名后面加上相同的后缀.txt： 1234567891011121314&gt;&gt; ls # 查看文件或者文件夹名称test0 test1 test2 test3 test4.txt black_test&gt;&gt; ls | grep -vE \"txt|black\" | awk '&#123;system(\"mv \" $0 \" \" $0 \".txt\")&#125;' # 方法1&gt;&gt; lstest0.txt test1.txt test2.txt test3.txt test4.txt black_test&gt;&gt; ls | egrep -v \"txt|black\" | awk '&#123;system(\"mv \" $0 \" \" $0 \".txt\")&#125;' # 方法2&gt;&gt; lstest0.txt test1.txt test2.txt test3.txt test4.txt black_test&gt;&gt; ls | awk '$0 !~/txt|black/ &#123;system(\"mv \" $0 \" \" $0 \".txt\")&#125;' # 方法3&gt;&gt; lstest0.txt test1.txt test2.txt test3.txt test4.txt black_test 这里涉及到grep的知识，列在下面： grep -E 用来扩展选项为正则表达式，如果使用了grep 命令的选项-E，则应该使用 | 来分割多个pattern grep -v 实现反向操作。例如方法1中命令 ls | grep -vE “txt|black” 实现了搜索名称中既不包含txt也不包含black的文件。 egrep 等同于‘grep -E’ 看了上面的grep的简介，方法1和方法2实际上是一样的。再来看后面的 “mv “ $0 “ “ $0 “.txt” 部分。这里假如$0 = “a”, 那么”mv “ $0 “ “ $0 “.txt”=”mv a a.txt”.这里需要注意的是空格，一定要记得打上。 再来看方法3，既然awk也有正则匹配， 那就完全可以省去grep，直接用awk的正则去筛选满足条件的文件名。命令 ‘$0 !~/txt|black/‘ 即实现了相同的效果。可以看一下上一节的规则。 到这里，over！有不对的地方，欢迎提出来。","categories":[{"name":"Code Tool","slug":"Code-Tool","permalink":"https://jkknotes.com/categories/Code-Tool/"},{"name":"命令速查","slug":"Code-Tool/命令速查","permalink":"https://jkknotes.com/categories/Code-Tool/%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/"}],"tags":[{"name":"Tool","slug":"Tool","permalink":"https://jkknotes.com/tags/Tool/"},{"name":"awk","slug":"awk","permalink":"https://jkknotes.com/tags/awk/"}]},{"title":"Pandas那些年遇到的坑","slug":"pandas_那些年遇到的坑","date":"2019-11-02T09:03:09.000Z","updated":"2019-11-03T15:13:35.868Z","comments":true,"path":"pandas_那些年遇到的坑/","link":"","permalink":"https://jkknotes.com/pandas_%E9%82%A3%E4%BA%9B%E5%B9%B4%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","excerpt":"在进行数据处理和分析时，pandas就像一条高速公路，能够帮助我们快速的进行各种数据处理和分析操作。但是高速公路也可能有各种坑，一不小心就翻车。 在平时的工作中，也积累了pandas处理的各种坑，记录下来，跟大家分享一下。","text":"在进行数据处理和分析时，pandas就像一条高速公路，能够帮助我们快速的进行各种数据处理和分析操作。但是高速公路也可能有各种坑，一不小心就翻车。 在平时的工作中，也积累了pandas处理的各种坑，记录下来，跟大家分享一下。 12import pandas as pdimport numpy as np 1. Pandas IO中的坑先从pandas的读写操作写起。使用pandas读写CSV文件的最常见的操作，即使这个最简单的操作，就很有可能掉入坑里。 1.1 解决读的坑，让pandas读文件内存占用减小 80%资源总是有限的， 僧多肉少是常见的 而我一次在公司的机器学习平台申请到5G内存，需要打开的csv文件只有900M，当你信心满满的使用 pandas.read_csv 去读取文件，意想不到的是内存爆了， 内存爆了，内存爆了！！！于是乎，就去学习了一下pandas在内存中存数据的方式，并且找到了解决方式，并很好的填了这个坑。 一般来说，用pandas处理小于100M的数据，性能不是问题。当用pandas来处理几百兆甚至几个G的数据时，将会比较耗时，同时会导致程序因内存不足而运行失败。那么怎么就解决这个问题呢，我们先来讨论一下pandas的内存使用。 如下表所示，pandas共有6种大的数据类型，在底层pandas会按照数据类型将列分组形成数据块（blocks）， 相同数据类型的列会合到一起存储。实际上，对于整型和浮点型数据，pandas将它们以 NumPy ndarray 的形式存储。 从表中可以看到，不同的存储方式所占用的内存不同。其中类型为category的数据在底层使用整型数值来表示该列的值，而不是用原值。当我们把一列转换成category类型时，pandas会用一种最省空间的int子类型去表示这一列中所有的唯一值。当一列只包含有限种值时，这种设计是很不错的。 了解到这里，我们是不是可以将占用内存多的数据类型转为占用内存低的数据类型，以到达减小内存的占用的目的。 memory usage float int unit category bool object 1 bytes int8 unit8 2 bytes float16 int16 unit16 4 bytes float32 int32 unit32 8 bytes float64 int64 unit64 variable Slytherin category bool object 随便找了个数据,实际操作看一下： 12data = pd.read_csv('game_logs.csv')data.head() /Users/kk_j/anaconda3/envs/python2_for_project/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (12,13,14,15,19,20,81,83,85,87,93,94,95,96,97,98,99,100,105,106,108,109,111,112,114,115,117,118,120,121,123,124,126,127,129,130,132,133,135,136,138,139,141,142,144,145,147,148,150,151,153,154,156,157,160) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } date number_of_game day_of_week v_name v_league v_game_number h_name h_league h_game_number v_score ... h_player_7_name h_player_7_def_pos h_player_8_id h_player_8_name h_player_8_def_pos h_player_9_id h_player_9_name h_player_9_def_pos additional_info acquisition_info 0 18710504 0 Thu CL1 na 1 FW1 na 1 0 ... Ed Mincher 7.0 mcdej101 James McDermott 8.0 kellb105 Bill Kelly 9.0 NaN Y 1 18710505 0 Fri BS1 na 1 WS3 na 1 20 ... Asa Brainard 1.0 burrh101 Henry Burroughs 9.0 berth101 Henry Berthrong 8.0 HTBF Y 2 18710506 0 Sat CL1 na 2 RC1 na 1 12 ... Pony Sager 6.0 birdg101 George Bird 7.0 stirg101 Gat Stires 9.0 NaN Y 3 18710508 0 Mon CL1 na 3 CH1 na 1 12 ... Ed Duffy 6.0 pinke101 Ed Pinkham 5.0 zettg101 George Zettlein 1.0 NaN Y 4 18710509 0 Tue BS1 na 2 TRO na 1 9 ... Steve Bellan 5.0 pikel101 Lip Pike 3.0 cravb101 Bill Craver 6.0 HTBF Y 5 rows × 161 columns 1data.info(memory_usage='deep') &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 171907 entries, 0 to 171906 Columns: 161 entries, date to acquisition_info dtypes: float64(77), int64(6), object(78) memory usage: 738.1 MB可以看到这个数据占用内存738.1M，而文件原来的大小仅仅128M，内存占用是原文件大小的 6 倍！！！ 再来尝试一下在打开文件的时候指定列的类型，将数据类型为object的列变成category的数据类型。 123object_cols = data.select_dtypes(include=['object']).columns.tolist()dtype_list = ['category' for x in object_cols]cols_dtype_dict = dict(zip(object_cols, dtype_list)) 12data1 = pd.read_csv('game_logs.csv', dtype=cols_dtype_dict, date_parser=['date'], infer_datetime_format=True)data1.info(memory_usage='deep') &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 171907 entries, 0 to 171906 Columns: 161 entries, date to acquisition_info dtypes: category(78), float64(77), int64(6) memory usage: 157.2 MB可以看到，内存占用从 738.1M 降到了157.2M，有效降低 78.7%， 而且那一堆Warning 也没了 很开心对不对，没有资源，咱自己创造资源 1.2 解决写的坑，让磁盘空间节约60%经常听见有小伙伴说，XXXX服务器磁盘空间又满了，大家清理一下自己不用的数据，数据很重要，不能删怎么办。 还是那句话，没有资源，咱创造资源 1data1.to_csv('game_logs.gz', compression='gzip', index=False) 去磁盘再去看看文件大小，是不是磁盘变大了。错了，是不是文件变小了。 在我的电脑里，这个文件从 128M 减小到18M。我去，磁盘占用减小了86% 那读取的时候怎么办呢，读取方式不变，还是 read_csv 1.3 解决写的坑，避免挖个坑这个坑比较简单，但是一不小心就翻车。看个例子 12df = pd.DataFrame(np.random.rand(2,2), columns=['a', 'b'])df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } a b 0 0.977292 0.343893 1 0.478050 0.781146 1df.to_csv('test_df.csv') 12df1 = pd.read_csv('test_df.csv')df1 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Unnamed: 0 a b 0 0 0.977292 0.343893 1 1 0.478050 0.781146 通过以上例子，可以看到，一存一读间，却多了一列。这种情况极易给后面的操作埋下一个大坑，而且还蒙在鼓里找不出原因。 怎么解决呢，只需要在存的时候，指定 index 参数为 False 即可。再来试一下： 123df.to_csv('test_df.csv', index=False)df = pd.read_csv('test_df.csv')df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } a b 0 0.977292 0.343893 1 0.478050 0.781146 1.4 python2：加上encoding， 读写好习惯这个就不举例子讲了。但是讲一下原因。 在工作中经常处理带中文字符的csv文件，一个好的习惯是，在使用pandas的read_csv（其他的read操作一样）进行文件读取时，加上参数 encoding=‘utf-8’，并且在数据的操作中都始终使用utf-8的编码格式，会减少非常多的坑。另外在使用 .to_csv 存储带有中文字符的DataFram数据时，加上参数 encoding=‘utf-8-sig’，这样存成的csv就可以用excel打开，而不乱码。 关于编码知识，可以看这里：https://blog.csdn.net/u010223750/article/details/56684096/ 1.5 乱入：用pandas进行onehot的神坑机器学习特征工程中，经常会用到one-hot编码。并且pandas中已经提供了这一函数pandas.get_dummies()。但是使用这个函数进行one hot操作后得到的数据类型竟然是是uint8，如果进行数值计算时会溢出。 12345data_df = pd.DataFrame(&#123;'sex': ['male', 'female', 'female', 'female', 'female', 'male', 'female'], 'height': [182, 160, 176, 172, 174, 170, 155], 'weight': [65, 50, 55, 48, 48, 100, 80], 'is_air_hostesses': [1, 1, 1, 1, 1, 0, 0]&#125;)data_df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } height is_air_hostesses sex weight 0 182 1 male 65 1 160 1 female 50 2 176 1 female 55 3 172 1 female 48 4 174 1 female 48 5 170 0 male 100 6 155 0 female 80 12sex_one_hot_df = pd.get_dummies(data_df['sex'])sex_one_hot_df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } female male 0 0 1 1 1 0 2 1 0 3 1 0 4 1 0 5 0 1 6 1 0 1sex_one_hot_df.dtypes female uint8 male uint8 dtype: object1-sex_one_hot_df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } female male 0 0 255 1 255 0 2 255 0 3 255 0 4 255 0 5 0 255 6 255 0 这真的是一个神坑，如果特征比较多的话，根本发现不了。如果没有发现，后续如果做其他操的时候，就会出错。这个坑藏得深啊。 正确的做法是转换一下数据类型： 1sex_one_hot_df = sex_one_hot_df.astype('float') 2. DataFrame 链式索引的坑2.1 解决：SettingWithCopyWarning:SettingWithCopyWarning 可能是人们在学习 Pandas 时遇到的最常见的障碍之一。首先来看看，它出现的情况之一（其他情况大同小异）： 12sub_df = df.loc[df.a &gt; 0.6]sub_df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } a b 0 0.688818 0.510446 4 0.945565 0.801788 1sub_df['c'] = [1,2] /Users/kk_j/anaconda3/envs/python2_for_project/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy &quot;&quot;&quot;Entry point for launching an IPython kernel.没有出任何意外，SettingWithCopyWarning 出现。首先要理解的是，SettingWithCopyWarning 是一个警告 Warning，而不是错误 Error，它告诉你，你的操作可能没有按预期运行，需要检查结果以确保没有出错。当你查看结果，发现结果没有错，就是在按预期进行，你极有可能忽略这个Warning, 而当下次它再次出现时，你不会再检查，然后错误就出现了。 直接说他出现的原因，那就是链式索引产生的新的变量并没有在内存中创建副本，当接下来对新的变量进行修改时，有修改原数据的风险。 怎么解决呢。很简单,只需要在链式索引后面加上一个.copy() 即可： 123sub_df = df.loc[df.a &gt; 0.6].copy()sub_df['c'] = [1,2]sub_df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } a b c 0 0.688818 0.510446 1 4 0.945565 0.801788 2 再试试，可以看到没有再出现问题。 但是我们也注意到，在Warning的提示里，提到：Try using .loc[row_indexer,col_indexer] = value instead。这也是一种解决办法，当你仅仅是想更改原始数据，你可以使用这个操作。 对这个问题的详细原理讲解，请参考： https://www.dataquest.io/blog/settingwithcopywarning/ 2.2 DataFrame 里存None：这个坑是真的坑真的不好写开场白，直接上例子： 1234567891011121314151617v = &#123;'value': 'a'&#125;d = [&#123;'name': 'class', 'age': 10&#125;, &#123;'name': None, 'age': 11&#125;, &#123;'name': 'def', 'age': 9&#125;]df = pd.DataFrame(d)new_1 = df[(df['age'] &gt;= 10) | df['name'].str.contains(v['value'])]# 颠倒里面条件的顺序new_2 = df[df['name'].str.contains(v['value']) | (df['age'] &gt;= 10)]print('-'*40)print(df)print('-'*40)print(new_1)print('-'*40)print(new_2) ---------------------------------------- age name 0 10 class 1 11 None 2 9 def ---------------------------------------- age name 0 10 class 1 11 None ---------------------------------------- age name 0 10 class这。。。。。逻辑操作“或”俩边的条件对调下，结果也能不一样？一脸懵逼。 但是接下来，我进行了简单的探索。 1df['age'] &gt;= 10 0 True 1 True 2 False Name: age, dtype: bool1df['name'].str.contains(v['value']) 0 True 1 None 2 False Name: name, dtype: object1(df['age'] &gt;= 10) | df['name'].str.contains(v['value']) 0 True 1 True 2 False dtype: bool1df['name'].str.contains(v['value']) | (df['age'] &gt;= 10) 0 True 1 False 2 False dtype: bool这。。。。。。还是一脸懵逼。 百度了一圈，还是没有找到答案。但是找到了解决了办法：把 None 改为了 ‘’ 就可以了。 1234567891011121314151617v = &#123;'value': 'a'&#125;d = [&#123;'name': 'class', 'age': 10&#125;, &#123;'name': '', 'age': 11&#125;, &#123;'name': 'def', 'age': 9&#125;]df = pd.DataFrame(d)new_1 = df[(df['age'] &gt;= 10) | df['name'].str.contains(v['value'])]# 颠倒里面条件的顺序new_2 = df[df['name'].str.contains(v['value']) | (df['age'] &gt;= 10)]print('-'*40)print(df)print('-'*40)print(new_1)print('-'*40)print(new_2) ---------------------------------------- age name 0 10 class 1 11 2 9 def ---------------------------------------- age name 0 10 class 1 11 ---------------------------------------- age name 0 10 class 1 11 2.3 这个坑不算坑这里就举个例子，自己体会： 12print type(df['age'])df['age'] &lt;class &apos;pandas.core.series.Series&apos;&gt; 0 10 1 11 2 9 Name: age, dtype: int6412print type(df[['age']])df[['age']] &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } age 0 10 1 11 2 9 前面是Series后面是DataFrame，这不知道算不算一个坑 3. DataFrame 拼接里面的坑与技巧pandas 里多个DataFrame的拼接，主要是append， merge，concat，join四个函数。想详细了解的话看一下官方文档。 这里简单说一下concat和merge. 3.1 concat：坑虽小，须谨慎解释这个坑，也只有靠例子。直接上代码： 12345678910111213141516171819df1 = pd.DataFrame(&#123; 'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2'], 'C': ['C0', 'C1', 'C2'], 'D': ['D0', 'D1', 'D2']&#125;) df2 = pd.DataFrame(&#123;'A': ['A4', 'A5', 'A6'], 'B': ['B4', 'B5', 'B6'], 'C': ['C4', 'C5', 'C6'], 'D': ['D4', 'D5', 'D6']&#125;) df3 = pd.DataFrame(&#123;'A': ['A8', 'A9', 'A10'], 'B': ['B8', 'B9', 'B10'], 'C': ['C8', 'C9', 'C10'], 'D': ['D8', 'D9', 'D10']&#125;) frames = [df1, df2, df3]result = pd.concat(frames)result .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 0 A4 B4 C4 D4 1 A5 B5 C5 D5 2 A6 B6 C6 D6 0 A8 B8 C8 D8 1 A9 B9 C9 D9 2 A10 B10 C10 D10 123df4 = pd.DataFrame(&#123;'val':[0,1,2,3,4,5,6,7,8],'A': ['A0', 'A1', 'A2', 'A3','A4', 'A5', 'A6', 'A7','A8']&#125;)result['val'] = df4['A']result .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } A B C D val 0 A0 B0 C0 D0 A0 1 A1 B1 C1 D1 A1 2 A2 B2 C2 D2 A2 0 A4 B4 C4 D4 A0 1 A5 B5 C5 D5 A1 2 A6 B6 C6 D6 A2 0 A8 B8 C8 D8 A0 1 A9 B9 C9 D9 A1 2 A10 B10 C10 D10 A2 注意看最后一列 ‘val’ ，和我们预期（预期的是从 A0-A8 ）的真的不一样。原来赋值操作是按照index赋值的，结果就是这么出乎我们的意料。 其实，concat的时候加上参数 ignore_index=True 就好了： 12result = pd.concat(frames, ignore_index=True)result .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A4 B4 C4 D4 4 A5 B5 C5 D5 5 A6 B6 C6 D6 6 A8 B8 C8 D8 7 A9 B9 C9 D9 8 A10 B10 C10 D10 12result['val'] = df4['A']result .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } A B C D val 0 A0 B0 C0 D0 A0 1 A1 B1 C1 D1 A1 2 A2 B2 C2 D2 A2 3 A4 B4 C4 D4 A3 4 A5 B5 C5 D5 A4 5 A6 B6 C6 D6 A5 6 A8 B8 C8 D8 A6 7 A9 B9 C9 D9 A7 8 A10 B10 C10 D10 A8 3.2 merge：小众的技巧panda.merge 这个是pandas最常用的操作之一，具体用法可以看官方文档。这里有个小的tricks, 在做一些统计分析的时候很有用。还是具体看例子吧。 12left = pd.DataFrame(&#123;'key': ['key1', 'key2', 'key3', 'key4'], 'val_l': [1, 2, 3, 4]&#125;)left .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } key val_l 0 key1 1 1 key2 2 2 key3 3 3 key4 4 12right = pd.DataFrame(&#123;'key': ['key3', 'key2', 'key1', 'key6'], 'val_r': [3, 2, 1, 6]&#125;)right .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } key val_r 0 key3 3 1 key2 2 2 key1 1 3 key6 6 12df_merge = pd.merge(left, right, on='key', how='left', indicator=True)df_merge .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } key val_l val_r _merge 0 key1 1 1.0 both 1 key2 2 2.0 both 2 key3 3 3.0 both 3 key4 4 NaN left_only _merge 列不仅可以用来检查是否出现数值错误，还可以进行统计分析，比如： 1df_merge['_merge'].value_counts() both 3 left_only 1 right_only 0 Name: _merge, dtype: int644. 一些技巧技巧总是讲不完，我这里随便再写点。 4.1 pandas 画图这个举个例子就好了 123from matplotlib import pyplot as pltdf_merge.val_l.plot(kind='bar')plt.show() 12data1.plot(kind='scatter', x='v_game_number', y='v_score', alpha=0.1)plt.show() plot这个命令底层调用的就是matplotlib。必须事先装好matplotlib，不然会报错。 这里的 2 个例子只是抛砖引玉，真正的功能非常强大，有兴趣的小伙伴可以学习一下 4.2 简单的相关性分析写到这里，写累了。不想去找数据集，还是用前面自己构造的数据集演示一下这个小技巧： 12345data_df = pd.DataFrame(&#123;'sex': ['male', 'female', 'female', 'female', 'female', 'male', 'female'], 'height': [182, 160, 176, 172, 174, 170, 155], 'weight': [65, 50, 55, 48, 48, 100, 80], 'is_air_hostesses': [1, 1, 1, 1, 1, 0, 0]&#125;)data_df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } height is_air_hostesses sex weight 0 182 1 male 65 1 160 1 female 50 2 176 1 female 55 3 172 1 female 48 4 174 1 female 48 5 170 0 male 100 6 155 0 female 80 1data_df[['sex', 'is_air_hostesses']].groupby(['sex'], as_index=False).mean().sort_values(by='is_air_hostesses', ascending=False) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sex is_air_hostesses 0 female 0.8 1 male 0.5 * 可以看到女生做空乘的可能性更大一些 * 12data_df['height_band'] = pd.qcut(data_df['height'], 2)data_df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } height is_air_hostesses sex weight height_band 0 182 1 male 65 (172.0, 182.0] 1 160 1 female 50 (154.999, 172.0] 2 176 1 female 55 (172.0, 182.0] 3 172 1 female 48 (154.999, 172.0] 4 174 1 female 48 (172.0, 182.0] 5 170 0 male 100 (154.999, 172.0] 6 155 0 female 80 (154.999, 172.0] 12data_df[['height_band', 'is_air_hostesses']].groupby(['height_band'], as_index=False).mean().sort_values(by='is_air_hostesses', ascending=False) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } height_band is_air_hostesses 1 (172.0, 182.0] 1.0 0 (154.999, 172.0] 0.5 这里可以看到身高大于172的是空乘的可能性更大一些 同样的也是为了抛砖引玉，不详细介绍了 5. 结束语写pandas的这些坑，只是为了更好的提高工作效率，有兴趣的小伙伴可以学一学，相信会很有帮助。","categories":[{"name":"Code Tool","slug":"Code-Tool","permalink":"https://jkknotes.com/categories/Code-Tool/"},{"name":"Code 总结","slug":"Code-Tool/Code-总结","permalink":"https://jkknotes.com/categories/Code-Tool/Code-%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"Tool","slug":"Tool","permalink":"https://jkknotes.com/tags/Tool/"},{"name":"Pandas","slug":"Pandas","permalink":"https://jkknotes.com/tags/Pandas/"}]},{"title":"Git基本命令大全","slug":"Git基本命令大全","date":"2019-06-04T04:29:29.000Z","updated":"2020-01-05T09:08:02.399Z","comments":true,"path":"Git基本命令大全/","link":"","permalink":"https://jkknotes.com/Git%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/","excerpt":"Git —— 大名鼎鼎的分布式版本控制系统，据说是Linux系统之父Linus花两周时间写出来的，怎一个牛字了得。写过代码的人大都知道它，以写代码为生的人大都离不开它。官方文档实在太难用，这里参考大佬廖雪峰的教程，把常用命令总结写两下来，以供自己和一起搬砖的朋友备查。","text":"Git —— 大名鼎鼎的分布式版本控制系统，据说是Linux系统之父Linus花两周时间写出来的，怎一个牛字了得。写过代码的人大都知道它，以写代码为生的人大都离不开它。官方文档实在太难用，这里参考大佬廖雪峰的教程，把常用命令总结写两下来，以供自己和一起搬砖的朋友备查。 这里先介绍本地仓库的常用命令，再介绍涉及远程仓库和分支管理的常用命令。本文没有实例介绍讲解，只做命令备查。如需详细教程，请参考廖雪峰的Git教程。 本文同步发在我的CSDN Blog，欢迎各位看官大佬关注指教。 1.基本操作创建版本库并添加文件进入到需要创建版本库的文件夹，执行以下命令 1git init git的本地库包含三部分：工作区、暂存区（stage）、分支（branch，默认为master），git add命令是将文件从工作区添加到暂存区，git commit命令是将文件从暂存区添加到分支，他们的关系如下图所示：添加新文件或修改过的文件： 12git add file1_name file2_name file3_name ....git commit -m '这里是对添加文件的说明' 查看版本库的状态： 1git status 分支管理创建分支 1git branch 分支名称 切换分支 1git checkout 分支名称 创建并切换分支 1git checkout -b 分支名称 删除本地分支 12git branch -d 分支名称git branch -D 分支名称 # 强制删除 删除远程分支 1git push &lt;remote_name&gt; --delete 远程分支名称 # 大部分情况下&lt;remote_name&gt;为origin 查看所有分支 123git branch # 查看本地分支git branch -a # 查看远程和本地所有分支git branch -r # 查看远程分支 拉取远程分支并同时创建对应的本地分支 1git checkout -b 本地分支名 origin/远程分支名 # 如失败，可以先git fetch 合并分支 1git merge 准备与当前分支合并的分支名称 若merge出现冲突，解决办法请参考这里 stash功能当在当前分支编辑内容时，没有add和commit就切换到其它分支，发现其它分支中也会出现相应更改，stash功能可以解决这个问题。如果出现这种情况，正在当前分支编辑，需要临时去其他分支处理一些事情，但是当前分支又不能add和commit，这时就需要stash功能，可以它就像一个icebox，能当前工作冻结。 1git stash 查看冻结列表： 1git stash list 想解冻2种方法：第一种：解冻的同时把stash记录也删了，也就是在list中看不到了 1git stash pop 第二种：解冻不删list中记录 1git stash apply 想删记录： 1git stash drop 2.时光穿梭版本回退当不断对文件进行修改，然后不断提交修改到版本库里，难免会出现失误，把文件改乱或出现其他错误，就需要回到上一次或者之前任一次提交的状态。首先查看提交历史记录： 1git log 返回结果中commit 后面的十六进制数即为该次提交的commit id。想返回结果简化一点： 1git log --pretty=oneline 想看到merge情况： 1git log --graph 同样有简化板 1git log --graph --pretty=oneline 回退到上一版本： 1git reset --hard head^ 同理，回退到上上版本就是head^^，以此类推，当然往上100个版本写100个^比较容易数不过来，所以写成head~100.也可以根据commit id回到该版本： 1git reset --hard [commit id前几位] 如果有20个版本，回退到第10个版本，但是后悔了，想回退到第15个版本，这是再去看提交历史记录，第11到20个版本的记录已经不在了，找不到commit id怎么办，可以用以下命令查到commit id前几位： 1git reflog 撤销修改或删除1git checkout -- file_name file_name即为想撤销修改或删除的文件名使用以上命令分三种情况：1.工作区文件自修改后还没有被放到暂存区，现在撤销修改就回到版本库中分支一样的状态；2.工作区文件自修改后已经添加到暂存区后，又作了修改或者删除，现在撤销修改或删除就回到和暂存区一样的状态；3.工作区文件自修改后已经commit到分支，又作了修改或者删除，现在撤销或修删除改就回到和分支一样的状态。 撤销暂存区的修改或删除： 1git reset HEAD file_name 如果commit到了分支，直接版本回退吧 3.远程仓库从远程仓库克隆： 1git clone 地址 或者 1git clone -b 分支名称 地址 将本地分支推送到远程库 1git push origin 当前分支名称 将远程库拉取到本地分支 1git pull origin 要拉取到分支名称 4.标签管理给当前分支打标签 1git tag 标签内容 指定commit id打标签 1git tag 标签内容 commit id前几位 查看标签 1git tag 查看标签详细情况 1git tag 标签内容 删除标签 1git tag -d 标签内容 推送一个本地标签 1git push origin 标签内容 推送全部未推送过的本地标签 1git push origin --tags 删除一个远程标签现在本地删除： 1git tag -d 标签内容 再执行： 1git push origin :refs/tags/标签内容 好了，就这么多，如有错误望指出。","categories":[{"name":"Code Tool","slug":"Code-Tool","permalink":"https://jkknotes.com/categories/Code-Tool/"},{"name":"命令速查","slug":"Code-Tool/命令速查","permalink":"https://jkknotes.com/categories/Code-Tool/%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://jkknotes.com/tags/Git/"},{"name":"Tool","slug":"Tool","permalink":"https://jkknotes.com/tags/Tool/"}]},{"title":"kaggle实战——What Causes Heart Disease?","slug":"kaggle实战——What Causes Heart Disease_","date":"2019-04-07T04:29:29.000Z","updated":"2019-11-03T13:47:42.285Z","comments":true,"path":"kaggle实战——What Causes Heart Disease_/","link":"","permalink":"https://jkknotes.com/kaggle%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94What%20Causes%20Heart%20Disease_/","excerpt":"记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。","text":"记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。 本文同步发在我的 CSDN Blog，2019.4.5 刚搞成功，接下来CSDN和 KK’s Notes 同时更新，各位看官大佬多多指教。 1. Introduction这个项目来自于kaggle。项目主要是利用患者的个人信息和检查数据，利用机器学习方法来诊断该患者收否患疾病，并且尝试对识别结果作出解释。这个项目虽然简单但将机器学习的全流程和常用预处理和分析方法都涉及到了，我做完一遍还是有很多收获。以下操作皆在 Jubyter notebook 下以 Python 进行的。 主要使用的技术： Random Forest Feature Importance Analysis: Permutation importance Feature Importance Analysis: Partial Dependence Plots 2. DataData from：https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/About Data：下载好数据之后直接打开看一看。 1234import pandas as pdimport numpy as npdata = pd.read_csv('data/heart.csv')data.info() Output:可以看到总共有303条数据以及13个特征和1个标签，数据没有缺失项。接下看下前十个数据。 1data.head(10) Output:这13个特征的含义分别是： age: 年龄sex：该人的性别（1=男性，0=女性）cp：胸痛经历（值1：典型心绞痛，值2：非典型心绞痛，值3：非心绞痛，值4：无症状）trestbps：该人的静息血压（入院时为mm Hg）chol：人体胆固醇测量单位为mg/dlfbs：该人的空腹血糖（&gt; 120mg/dl，1=true; 0= f=alse）restecg：静息心电图测量（0=正常，1=有ST-T波异常，2=按Estes标准显示可能或明确的左心室肥厚）thalach：达到了该人的最大心率exang：运动诱发心绞痛（1=是; 0=否）oldpeak：运动相对于休息引起的ST段压低（’ST’与ECG图上的位置有关）slope：峰值运动ST段的斜率（值1：上升，值2：平坦，值3：下降）ca：主要血管数量（0-3）thal：称为地中海贫血的血液疾病（1=正常; 2=固定缺陷; 3=可逆缺陷）target：心脏病（0=不，1=是） 为了更好的理解数据，我们应该提前查一下每个特征的含义，以及医学上该特征和心脏病的关系。具体这里不再赘述。 3. 数据预处理这里为了方便后续做心脏病诊断中影响因素分析即Feature Importance Analysis（还是觉得用英文更能表达意思），将部分数值型特征进行转换： 12345678910111213141516171819202122232425data.loc[data.sex == 1, 'sex'] = 'male'data.loc[data['sex'] == 0, 'sex'] = 'female'data.loc[data['cp'] == 1, 'cp'] = 'typical'data.loc[data['cp'] == 2, 'cp'] = 'atypical'data.loc[data['cp'] == 3, 'cp'] = 'no_pain'data.loc[data['cp'] == 4, 'cp'] = 'no_feel'data.loc[data['fbs'] == 1, 'fbs'] = 'higher than 120 mg/dl'data.loc[data['fbs'] == 0, 'fbs'] = 'lower than 120 mg/dl'data.loc[data['restecg'] == 0, 'restecg'] = 'normal'data.loc[data['restecg'] == 1, 'restecg'] = 'ST-T wave abnormality'data.loc[data['restecg'] == 2, 'restecg'] = 'left ventricular hypertrophy'data.loc[data['exang'] == 1, 'exang'] = 'true'data.loc[data['exang'] == 0, 'exang'] = 'false'data.loc[data['slope'] == 1, 'slope'] = 'up'data.loc[data['slope'] == 2, 'slope'] = 'flat'data.loc[data['slope'] == 3, 'slope'] = 'down'data.loc[data['thal'] == 1, 'thal'] = 'normal'data.loc[data['thal'] == 2, 'thal'] = 'fixed defect'data.loc[data['thal'] == 3, 'thal'] = 'reversable defect' 检查下数据情况： 1data.describe(include=[np.object]) Output:可以看到特征thal有4个值，而我们在转换时只转换了3个。实际上thal存在2个缺失值用0补齐的。为了防止数据类型错误，这里做一下类型转换。 1data['thal'] = data['thal'].astype('object') 再看下数据： 1data.head() Output:模型的训练肯定需要数值型特征。这里对特征进行Onehot编码。 12data = pd.get_dummies(data, drop_first=True)data.head() Output：（由于我还不知道在用markdown编辑时怎么显示运行结果，这里用的是截图，只能截取一部分，还有特征没有截取出来）数据预处理部分就到此为止，接下来上模型。 4. Random Forest对于 Random Forest 的原理这里就不介绍了，网上介绍的文章也很多。废话不多说，直接import package. 1234from sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierimport matplotlib.pyplot as plt 将数据分成 train_data 和 test_data 2个集合，二者比例为8:2。 1234train_x, test_x, train_y, test_y = train_test_split(data.drop(columns='target'), data['target'], test_size=0.2, random_state=10) 简单的画个图调个参。这里 Random Forest 主要的参数有基学习器决策树的最大深度（这里依据经验选5）、基学习器个数 n_estimators。这里基学习器选用CART。 12345678910train_score = []test_score = []for n in range(1, 100): model = RandomForestClassifier(max_depth=5, n_estimators=n， criterion='gini') model.fit(train_x, train_y) train_score.append(model.score(train_x, train_y)) test_score.append(model.score(test_x, test_y)) 训练完，把train和test上的accuracy随基学习器个数的变化画成图。 1234567891011x_axis = [i for i in range(1, 100)]fig, ax = plt.subplots()ax.plot(x_axis, train_score[:99])ax.plot(x_axis, test_score[:99], c=\"r\")plt.xlim([0, 100])plt.ylim([0.0, 1.0])plt.rcParams['font.size'] = 12plt.xlabel('n_estimators')plt.ylabel('accuracy')plt.grid(True) Output：可以看到大概是n_estimators=14的时候效果最好，train和test上的accuracy分别是0.9463，0.8361。看上去没有那么差。 5. 模型评估训练完模型，用ROC曲线来评估下模型的效果。ROC曲线事宜FPR和TPR分别为横纵轴作出的曲线，其和坐标轴围成的面积越大，说明模型效果越好。具体评判标准见下文。说一下几个概念： TPR: 真正例率，表示所有真正为正例的样本被正确预测出来的比例，等同于Recall FNR: 假负例率，FNR = 1 - TPR FPR: 假正例率，表示所有负例中被预测为正例的比例。 TNR: 真负例率，TNR = 1 - FPR 好吧，我也快晕了。接下来计算一下正例和负例的recall 123456from sklearn.metrics import confusion_matrixfrom sklearn.metrics import auc, roc_curve# 混淆矩阵confusion_m = confusion_matrix(test_y, pred_y) print confusion_m Output: 12[[29 6] [ 4 22]] 1234total = confusion_m.sum()tpr = float(confusion_m[0][0]) / (confusion_m[0][0] + confusion_m[1][0])tnr = float(confusion_m[1][1]) / (confusion_m[1][1] + confusion_m[0][1])print tpr, tnr Output: 10.878787878788 0.785714285714 Just so so!! 画ROC曲线图： 123456789101112131415pred_y = model.predict(test_x) # 预测结果pred_prob_y = model.predict_proba(test_x)[:, 1] # 为正例的概率fpr_list, tpr_list, throsholds = roc_curve(test_y, pred_prob_y)# 画图fig, ax = plt.subplots()ax.plot(fpr_list, tpr_list)ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"r\")plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.0])plt.rcParams['font.size'] = 12plt.title('roc curve')plt.xlabel('fpr')plt.ylabel('tpr')plt.grid(True) Output:前文说了，ROC曲线和坐标轴围成的面积越大，说明模型效果越好。这个面积就叫 AUC .根据AUC的值，可参考下面的规则评估模型： 0.90 - 1.00 = excellent 0.80 - 0.90 = good 0.70 - 0.80 = fair 0.60 - 0.70 = poor 0.50 - 0.60 = fail 看看我们训练模型的AUC 1auc(fpr_list, tpr_list) Output: 10.9032967032967033 OK， working well！ 6. Feature Importance Analysis训练完模型，我们希望能从模型里得到点什么， 比如说哪些特征对模型结果贡献率比较大，是不是意味着这些影响因素在实际心脏病诊断中也是很重要对参考，或者说还能发现一些现有医学没有发现的发现。所有接下来我们做的是一件很有意思的事。 6.1 决策树可视化如果我没记错的话， 根据决策树的原理，越先分裂的特征越重要。那么下面对决策树进行可视化，看看它到底做了什么。 1234567891011from sklearn.tree import export_graphviz# 输出 feature_nameestimator = model.estimators_[1]features = [i for i in train_x.columns]# 0 —&gt; no disease，1 —&gt; diseasetrain_y_str = train_y.astype('str')train_y_str[train_y_str == '0'] = 'no disease'train_y_str[train_y_str == '1'] = 'disease'train_y_str = train_y_str.values sklearn 真是个好东西，你能想到对功能他都有。下面用 sklearn 的 export_graphviz 对决策树进行可视化。 123456export_graphviz(estimator, out_file='tree.dot', feature_names = features, class_names = train_y_str, rounded = True, proportion = True, label='root', precision = 2, filled = True) 生成对这个 tree.dot 文件还不能直接看，网上查了一下，把它输出来看看。 12345import pydotplusfrom IPython.display import Imageimg = pydotplus.graph_from_dot_file('tree.dot')#img.write_pdf('tree.pdf') #输出成PDFImage(img.create_png()) Output：实际上这张图就解释来决策树的生成过程。一般我们认为最先分裂的特征越重要，但是从这张图我们并不能很直观的看出特征的重要性。 6.2 Permutation importance我们换一个工具—Permutation importance. 其原理是依次打乱test_data中其中一个特征数值的顺序，其实就是做shuffle，然后观察模型的效果，下降的多的说明这个特征对模型比较重要。 12345import eli5from eli5.sklearn import PermutationImportanceperm = PermutationImportance(model, random_state=20).fit(test_x, test_y)eli5.show_weights(perm, feature_names=test_x.columns.tolist()) Output：一目了然，一切尽在不言中。还是说俩句吧，绿色越深表示正相关越强，红色越深表示负相关越强。实际上我发现改变 PermutationImportance 的参数 random_state 的值结果变化挺大的，不过还是有几个特征位次变化不大，结果还是具有参考意义。 6.3 Partial Dependence Plots我们试试另一个工具—Partial Dependence Plots. 其原理和 Permutation importance 有点类似，当它判断一个特征对模型的影响时，对于所有样本，将该特征依次取该特征的所有取值，观察模型结果的变化。先画图，再根据图解释一下。 12345678from pdpbox import pdp, info_plotstotal_features = train_x.columns.values.tolist()feature_name = 'oldpeak'pdp_dist = pdp.pdp_isolate(model=model, dataset=test_x, model_features=total_features, feature=feature_name)pdp.pdp_plot(pdp_dist, feature_name)plt.show() Output：上图的纵坐标是模型相对于base model 的变化，横坐标是该特征的所有取值，实线表示相对于base model 的变化的平均值，蓝色阴影表示置信度。oldpeak表示运动相对于休息引起的ST段压低，可以看到其取值越大，患心脏病的可能性越低。不知道这个结果可不可信，我觉得需要医学知识作支撑。 又试了几个特征： Sex：上图说明男性比女性患心脏病的概率要低些，网上查了一下，还真是这样。 Age：上图表示60岁以上老人心脏病高发，这个和现有理论相符。 接下来看一下 2D Partial Dependence Plots. 123456789inter = pdp.pdp_interact(model=model, dataset=test_x, model_features=total_features, features=['oldpeak', 'age'])pdp.pdp_interact_plot(pdp_interact_out=inter, feature_names=['oldpeak', 'age'], plot_type='contour')plt.show() Output：这个图一开始没看懂，后来仔细看了Partial Dependence Plots 的说明文档才搞明白。图中颜色从浅到深表示患心脏病概率降低，以最深的那个紫色为例，oldpeak &gt; 3.0 &amp;&amp; 45 &lt; age &lt; 65 时，患病概率最低，图中黄色部分表示，oldpeak &lt; 0.25 &amp;&amp; ( age &lt; 45 || age &gt; 65 ) 时，患病概率最高。 7. 后记实际上本项目的数据是非常小的，其结果的可靠性也是值得怀疑的。但是通过这个项目，去经历机器学习项目的完整过程，却能学到很多东西。重要的是过程，更重要的是举一反三。该项目还引入了2个很有趣的Feature Importance Analysis的方法，对于我来说是新知识，也算是学到了。 这一篇到这里结束了，期待下一篇。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jkknotes.com/categories/Machine-Learning/"},{"name":"项目实战","slug":"Machine-Learning/项目实战","permalink":"https://jkknotes.com/categories/Machine-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"AI医疗","slug":"AI医疗","permalink":"https://jkknotes.com/tags/AI%E5%8C%BB%E7%96%97/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jkknotes.com/tags/Machine-Learning/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"https://jkknotes.com/tags/Random-Forest/"},{"name":"Feature Engineering","slug":"Feature-Engineering","permalink":"https://jkknotes.com/tags/Feature-Engineering/"}]}]}